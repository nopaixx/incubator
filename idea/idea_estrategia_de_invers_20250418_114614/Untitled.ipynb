{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03a1d54b-97d3-4d75-bfed-2976beda4598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando datos para 509 activos desde 2023-01-01 hasta 2025-01-01...\n",
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando activos: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 503/503 [00:01<00:00, 442.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos descargados y procesados para 500 activos válidos.\n",
      "Identificando regímenes de mercado...\n",
      "Ejecutando backtest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando fechas de rebalanceo: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [04:34<00:00,  5.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RESUMEN DE RESULTADOS - ESTRATEGIA AMAR (OPTIMIZADA)\n",
      "==================================================\n",
      "\n",
      "Período de Backtest: 2024-01-01 a 2025-01-01\n",
      "Retorno Total: 1.45%\n",
      "Retorno Anualizado: 7.38%\n",
      "Volatilidad Anualizada: 11.43%\n",
      "Sharpe Ratio: 0.47\n",
      "Máximo Drawdown: -3.12%\n",
      "Calmar Ratio: 2.37\n",
      "Alpha Anualizado: 5.44%\n",
      "Beta: 0.06\n",
      "\n",
      "Benchmark (Mercado):\n",
      "Retorno Total: 8.07%\n",
      "Retorno Anualizado: 46.73%\n",
      "Volatilidad Anualizada: 12.14%\n",
      "Sharpe Ratio: 3.68\n",
      "Máximo Drawdown: -2.67%\n",
      "\n",
      "Resultados guardados en: ./artifacts/results/\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import norm\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from typing import Dict, List, Tuple, Union, Optional\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy import stats\n",
    "\n",
    "# Configuración general (sin cambios)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Crear directorios para resultados\n",
    "os.makedirs('./artifacts/results', exist_ok=True)\n",
    "os.makedirs('./artifacts/results/figures', exist_ok=True)\n",
    "os.makedirs('./artifacts/results/data', exist_ok=True)\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    filename='./artifacts/errors.txt',\n",
    "    level=logging.ERROR,\n",
    "    format='[%(asctime)s] %(levelname)s: %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "class AMAR:\n",
    "    \"\"\"\n",
    "    Implementación mejorada de la estrategia AMAR (Adaptive Multi-factor Allocation with Reinforcement learning)\n",
    "    que combina identificación de regímenes de mercado con técnicas de aprendizaje por refuerzo\n",
    "    para optimizar la selección de factores y construcción de portafolios.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 start_date: str = '2010-01-01',\n",
    "                 end_date: str = None,\n",
    "                 lookback_years: int = 3,\n",
    "                 regime_update_frequency: str = 'M',\n",
    "                 portfolio_rebalance_frequency: str = 'W-FRI',\n",
    "                 n_regimes: int = 3,\n",
    "                 market_index: str = 'SPY',\n",
    "                 target_beta: float = 0.0,  # Cambiado a 0.0 para enfatizar neutralidad al mercado\n",
    "                 beta_range: float = 0.1,\n",
    "                 max_stock_weight: float = 0.05,  # Aumentado para permitir más concentración en mejores señales\n",
    "                 max_sector_deviation: float = 0.05,\n",
    "                 max_turnover: float = 0.2,  # Reducido para limitar costos de transacción\n",
    "                 volatility_target: float = 0.08,  # Reducido para mejor control de riesgo\n",
    "                 min_liquidity: float = 5e6,  # Aumentado a $5M para mejor liquidez\n",
    "                 transaction_cost: float = 0.0015,  # Más realista (0.15%)\n",
    "                 random_state: int = 42):\n",
    "        \"\"\"\n",
    "        Inicializa la estrategia AMAR con parámetros optimizados.\n",
    "        \"\"\"\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date if end_date else datetime.now().strftime('%Y-%m-%d')\n",
    "        self.lookback_years = lookback_years\n",
    "        self.regime_update_frequency = regime_update_frequency\n",
    "        self.portfolio_rebalance_frequency = portfolio_rebalance_frequency\n",
    "        self.n_regimes = n_regimes\n",
    "        self.market_index = market_index\n",
    "        self.target_beta = target_beta\n",
    "        self.beta_range = beta_range\n",
    "        self.max_stock_weight = max_stock_weight\n",
    "        self.max_sector_deviation = max_sector_deviation\n",
    "        self.max_turnover = max_turnover\n",
    "        self.volatility_target = volatility_target\n",
    "        self.min_liquidity = min_liquidity\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Atributos internos (sin cambios)\n",
    "        self.data = None\n",
    "        self.sp500_stocks = None\n",
    "        self.market_data = None\n",
    "        self.regimes = None\n",
    "        self.factor_performance = {}\n",
    "        self.portfolio_weights = pd.DataFrame()\n",
    "        self.portfolio_returns = pd.Series(dtype='float64')\n",
    "        self.current_regime = None\n",
    "        \n",
    "        # MEJORA 1: Añadir factor de Reversal a corto plazo\n",
    "        self.factor_categories = ['Value', 'Momentum', 'Quality', 'Volatility', 'Liquidity', 'Reversal']\n",
    "        \n",
    "        # Inicializar distribuciones de creencia para Thompson Sampling\n",
    "        self.initialize_belief_distributions()\n",
    "        \n",
    "        # Métricas de rendimiento\n",
    "        self.metrics = {}\n",
    "        \n",
    "        # MEJORA 2: Añadir descarga de VIX para mejor identificación de regímenes\n",
    "        self.vix_data = None\n",
    "        \n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "    def initialize_belief_distributions(self):\n",
    "        \"\"\"\n",
    "        Inicializa las distribuciones de creencia para el algoritmo Thompson Sampling.\n",
    "        Usado prior informativo más balanceado basado en literatura de factores.\n",
    "        \"\"\"\n",
    "        # Para cada categoría y cada régimen, mantenemos alpha y beta para distribución Beta\n",
    "        self.belief_distributions = {}\n",
    "        \n",
    "        # MEJORA 3: Prior informativo mejorado basado en literatura académica\n",
    "        priors = {\n",
    "            'Value': {'success': 5, 'failure': 5},     # Valor funciona bien en regímenes de recuperación\n",
    "            'Momentum': {'success': 6, 'failure': 4},  # Momentum tiende a funcionar en tendencias claras\n",
    "            'Quality': {'success': 7, 'failure': 3},   # Quality tiende a ser más estable en general\n",
    "            'Volatility': {'success': 5, 'failure': 5}, # Neutral para empezar\n",
    "            'Liquidity': {'success': 4, 'failure': 6},  # Liquidez es importante en regímenes de crisis\n",
    "            'Reversal': {'success': 4, 'failure': 6}    # Reversal funciona mejor en mercados volátiles\n",
    "        }\n",
    "        \n",
    "        for category in self.factor_categories:\n",
    "            self.belief_distributions[category] = {}\n",
    "            for regime in range(self.n_regimes):\n",
    "                # Inicializamos con priors informativas\n",
    "                self.belief_distributions[category][regime] = {\n",
    "                    'alpha': priors[category]['success'],\n",
    "                    'beta': priors[category]['failure']\n",
    "                }\n",
    "    \n",
    "    def get_sp500_tickers(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Obtiene los tickers actuales del S&P 500 desde Wikipedia.\n",
    "        \n",
    "        Returns:\n",
    "            Lista de tickers del S&P 500\n",
    "        \"\"\"\n",
    "        try:\n",
    "            table = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "            df = table[0]\n",
    "            tickers = df['Symbol'].str.replace('.', '-').tolist()\n",
    "            return tickers\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al obtener tickers del S&P 500: {str(e)}\")\n",
    "            # Fallback a lista reducida en caso de error\n",
    "            return ['AAPL', 'MSFT', 'AMZN', 'GOOGL', 'META', 'TSLA', 'BRK-B', 'JPM', 'JNJ', 'V', 'PG', 'UNH', 'HD', 'BAC', 'MA']\n",
    "    \n",
    "    def download_data(self):\n",
    "        \"\"\"\n",
    "        Descarga datos históricos para todos los activos del S&P 500, índice de referencia y VIX.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Calcular fecha de inicio para incluir el período de lookback\n",
    "            extended_start = (datetime.strptime(self.start_date, '%Y-%m-%d') - \n",
    "                             timedelta(days=int(365.25 * self.lookback_years))).strftime('%Y-%m-%d')\n",
    "            \n",
    "            # Obtener tickers del S&P 500\n",
    "            self.sp500_stocks = self.get_sp500_tickers()\n",
    "            \n",
    "            # MEJORA 4: Añadir VIX y otros ETFs importantes para identificación de regímenes\n",
    "            additional_tickers = ['^VIX', 'TLT', 'IEF', 'HYG', 'LQD']  # VIX, bonos largos, bonos intermedios, HY, IG\n",
    "            \n",
    "            # Incluir índice de mercado y tickers adicionales\n",
    "            tickers = self.sp500_stocks + [self.market_index] + additional_tickers\n",
    "            \n",
    "            print(f\"Descargando datos para {len(tickers)} activos desde {extended_start} hasta {self.end_date}...\")\n",
    "            \n",
    "            # Descargar datos de precio y volumen\n",
    "            self.data = yf.download(\n",
    "                tickers, \n",
    "                start=extended_start, \n",
    "                end=self.end_date,\n",
    "                group_by='ticker',\n",
    "                progress=False\n",
    "            )\n",
    "            \n",
    "            # Separar datos del mercado\n",
    "            self.market_data = pd.DataFrame({\n",
    "                'close': self.data[self.market_index]['Close'],\n",
    "                'high': self.data[self.market_index]['High'],\n",
    "                'low': self.data[self.market_index]['Low'],\n",
    "                'volume': self.data[self.market_index]['Volume']\n",
    "            })\n",
    "            \n",
    "            # Calcular retornos diarios del mercado\n",
    "            self.market_data['returns'] = self.market_data['close'].pct_change()\n",
    "            \n",
    "            # MEJORA 5: Guardar datos del VIX para identificación de regímenes\n",
    "            if '^VIX' in self.data.columns.levels[0]:\n",
    "                self.vix_data = pd.DataFrame({\n",
    "                    'close': self.data['^VIX']['Close'],\n",
    "                    'high': self.data['^VIX']['High'],\n",
    "                    'low': self.data['^VIX']['Low']\n",
    "                })\n",
    "            \n",
    "            # Limpiar datos de precio y volumen para acciones individuales\n",
    "            stocks_data = {}\n",
    "            \n",
    "            for ticker in tqdm(self.sp500_stocks, desc=\"Procesando activos\"):\n",
    "                try:\n",
    "                    if ticker in self.data.columns.levels[0]:\n",
    "                        stock_data = pd.DataFrame({\n",
    "                            'close': self.data[ticker]['Close'],\n",
    "                            'open': self.data[ticker]['Open'],\n",
    "                            'high': self.data[ticker]['High'],\n",
    "                            'low': self.data[ticker]['Low'],\n",
    "                            'volume': self.data[ticker]['Volume'],\n",
    "                        })\n",
    "                        \n",
    "                        # Calcular retornos diarios\n",
    "                        stock_data['returns'] = stock_data['close'].pct_change()\n",
    "                        \n",
    "                        # Solo mantener acciones con suficientes datos\n",
    "                        if stock_data['close'].dropna().shape[0] > 252:  # Al menos un año de datos\n",
    "                            stocks_data[ticker] = stock_data\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error procesando {ticker}: {str(e)}\")\n",
    "            \n",
    "            # Guardar datos procesados\n",
    "            self.stock_data = stocks_data\n",
    "            \n",
    "            print(f\"Datos descargados y procesados para {len(stocks_data)} activos válidos.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error descargando datos: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def calculate_market_regime_features(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calcula características mejoradas para la identificación de regímenes de mercado.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame con características para identificación de regímenes\n",
    "        \"\"\"\n",
    "        market = self.market_data.copy()\n",
    "        \n",
    "        # MEJORA 6: Características más relevantes para identificación de regímenes\n",
    "        features = pd.DataFrame(index=market.index)\n",
    "        \n",
    "        # 1. Tendencia de precios (corto, medio y largo plazo)\n",
    "        market['sma20'] = market['close'].rolling(20).mean()\n",
    "        market['sma50'] = market['close'].rolling(50).mean()\n",
    "        market['sma200'] = market['close'].rolling(200).mean()\n",
    "        \n",
    "        features['trend_st'] = market['close'] / market['sma20'] - 1  # Tendencia corto plazo\n",
    "        features['trend_mt'] = market['close'] / market['sma50'] - 1  # Tendencia medio plazo\n",
    "        features['trend_lt'] = market['close'] / market['sma200'] - 1  # Tendencia largo plazo\n",
    "        \n",
    "        # 2. Volatilidad en diferentes períodos\n",
    "        features['volatility_st'] = market['returns'].rolling(21).std() * np.sqrt(252)  # Volatilidad 1 mes\n",
    "        features['volatility_mt'] = market['returns'].rolling(63).std() * np.sqrt(252)  # Volatilidad 3 meses\n",
    "        \n",
    "        # 3. Usar VIX si está disponible\n",
    "        if self.vix_data is not None:\n",
    "            vix = self.vix_data.copy()\n",
    "            vix_index = vix.index.intersection(features.index)\n",
    "            features.loc[vix_index, 'vix'] = vix.loc[vix_index, 'close']\n",
    "            features['vix_ma20'] = features['vix'].rolling(20).mean()\n",
    "            features['vix_ratio'] = features['vix'] / features['vix_ma20']\n",
    "        \n",
    "        # 4. Momentum de mercado (rendimiento de diferentes períodos)\n",
    "        features['momentum_1m'] = market['close'].pct_change(21)  # 1 mes\n",
    "        features['momentum_3m'] = market['close'].pct_change(63)  # 3 meses\n",
    "        features['momentum_6m'] = market['close'].pct_change(126)  # 6 meses\n",
    "        \n",
    "        # 5. Amplitud de mercado (usamos volatilidad como proxy)\n",
    "        features['market_breadth'] = features['volatility_mt'] / features['volatility_st']\n",
    "        \n",
    "        # Imputar valores faltantes con la media\n",
    "        features = features.fillna(features.mean())\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def identify_market_regimes(self, training_data: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Identifica regímenes de mercado utilizando GMM con selección de características.\n",
    "        \n",
    "        Args:\n",
    "            training_data: DataFrame con características para entrenamiento\n",
    "        \n",
    "        Returns:\n",
    "            Array con etiquetas de régimen\n",
    "        \"\"\"\n",
    "        # MEJORA 7: Selección de variables más importantes para evitar ruido\n",
    "        # Usamos un conjunto más pequeño de variables para obtener regímenes más estables\n",
    "        key_features = ['volatility_st', 'trend_mt', 'momentum_3m']\n",
    "        \n",
    "        if 'vix' in training_data.columns:\n",
    "            key_features.append('vix_ratio')\n",
    "            \n",
    "        # Extraer características clave\n",
    "        regime_data = training_data[key_features].copy()\n",
    "        \n",
    "        # Normalizar características\n",
    "        scaler = StandardScaler()\n",
    "        normalized_data = scaler.fit_transform(regime_data)\n",
    "        \n",
    "        # MEJORA 8: Inicialización más robusta para GMM\n",
    "        # Ajustar GMM con múltiples inicializaciones\n",
    "        gmm = GaussianMixture(\n",
    "            n_components=self.n_regimes,\n",
    "            covariance_type='full',\n",
    "            random_state=self.random_state,\n",
    "            n_init=20,  # Aumentado para mayor estabilidad\n",
    "            max_iter=200\n",
    "        )\n",
    "        \n",
    "        gmm.fit(normalized_data)\n",
    "        labels = gmm.predict(normalized_data)\n",
    "        \n",
    "        # MEJORA 9: Ordenar regímenes por volatilidad para mejor interpretación\n",
    "        # Calcular volatilidad media en cada régimen\n",
    "        regime_volatility = {}\n",
    "        for i in range(self.n_regimes):\n",
    "            mask = (labels == i)\n",
    "            if mask.sum() > 0:\n",
    "                regime_volatility[i] = training_data.loc[mask, 'volatility_st'].mean()\n",
    "        \n",
    "        # Ordenar regímenes por volatilidad (0: baja, 1: media, 2: alta)\n",
    "        sorted_regimes = sorted(regime_volatility.items(), key=lambda x: x[1])\n",
    "        regime_map = {old: new for new, (old, _) in enumerate(sorted_regimes)}\n",
    "        \n",
    "        # Reasignar etiquetas\n",
    "        new_labels = np.array([regime_map[label] for label in labels])\n",
    "        \n",
    "        return new_labels\n",
    "    \n",
    "    def identify_regimes(self):\n",
    "        \"\"\"\n",
    "        Identifica regímenes de mercado para todo el período.\n",
    "        Actualiza el modelo en intervalos regulares para evitar look-ahead bias.\n",
    "        \"\"\"\n",
    "        print(\"Identificando regímenes de mercado...\")\n",
    "        \n",
    "        # Calcular características para todo el período\n",
    "        all_features = self.calculate_market_regime_features()\n",
    "        \n",
    "        # Obtener fechas para actualizaciones del modelo\n",
    "        if self.regime_update_frequency == 'M':\n",
    "            update_dates = pd.date_range(start=self.start_date, end=self.end_date, freq='M')\n",
    "        else:\n",
    "            update_dates = pd.date_range(start=self.start_date, end=self.end_date, freq=self.regime_update_frequency)\n",
    "        \n",
    "        # Añadir fecha inicial si no está incluida\n",
    "        if update_dates[0] > pd.Timestamp(self.start_date):\n",
    "            update_dates = pd.DatetimeIndex([pd.Timestamp(self.start_date)]).append(update_dates)\n",
    "        \n",
    "        # DataFrame para almacenar regímenes\n",
    "        self.regimes = pd.DataFrame(index=all_features.index, columns=['regime'])\n",
    "        \n",
    "        # Para cada fecha de actualización, entrenar modelo con datos disponibles hasta ese momento\n",
    "        for i in range(len(update_dates) - 1):\n",
    "            current_date = update_dates[i]\n",
    "            next_date = update_dates[i + 1]\n",
    "            \n",
    "            # Obtener datos de entrenamiento (solo datos hasta current_date)\n",
    "            lookback_start = current_date - pd.Timedelta(days=int(365.25 * self.lookback_years))\n",
    "            train_features = all_features.loc[\n",
    "                (all_features.index >= lookback_start) & \n",
    "                (all_features.index <= current_date)\n",
    "            ]\n",
    "            \n",
    "            if train_features.shape[0] > 30:  # Asegurar suficientes datos\n",
    "                # Identificar regímenes\n",
    "                labels = self.identify_market_regimes(train_features)\n",
    "                \n",
    "                # Aplicar el modelo a datos entre current_date y next_date\n",
    "                predict_features = all_features.loc[\n",
    "                    (all_features.index > current_date) & \n",
    "                    (all_features.index <= next_date)\n",
    "                ]\n",
    "                \n",
    "                if predict_features.shape[0] > 0:\n",
    "                    # MEJORA 10: Selección de características consistente\n",
    "                    key_features = ['volatility_st', 'trend_mt', 'momentum_3m']\n",
    "                    if 'vix' in train_features.columns:\n",
    "                        key_features.append('vix_ratio')\n",
    "                    \n",
    "                    # Normalizar con el mismo scaler\n",
    "                    scaler = StandardScaler()\n",
    "                    train_key_features = train_features[key_features]\n",
    "                    scaler.fit(train_key_features)\n",
    "                    predict_key_features = predict_features[key_features]\n",
    "                    predict_normalized = scaler.transform(predict_key_features)\n",
    "                    \n",
    "                    # Ajustar GMM nuevamente con datos de entrenamiento\n",
    "                    gmm = GaussianMixture(\n",
    "                        n_components=self.n_regimes,\n",
    "                        covariance_type='full',\n",
    "                        random_state=self.random_state,\n",
    "                        n_init=20,\n",
    "                        max_iter=200\n",
    "                    )\n",
    "                    gmm.fit(scaler.transform(train_key_features))\n",
    "                    \n",
    "                    # Predecir regímenes\n",
    "                    predict_labels = gmm.predict(predict_normalized)\n",
    "                    \n",
    "                    # MEJORA 11: Aplicar la misma reordenación por volatilidad\n",
    "                    regime_volatility = {}\n",
    "                    for j in range(self.n_regimes):\n",
    "                        mask = (labels == j)\n",
    "                        if mask.sum() > 0:\n",
    "                            regime_volatility[j] = train_features.loc[mask, 'volatility_st'].mean()\n",
    "                    \n",
    "                    sorted_regimes = sorted(regime_volatility.items(), key=lambda x: x[1])\n",
    "                    regime_map = {old: new for new, (old, _) in enumerate(sorted_regimes)}\n",
    "                    \n",
    "                    # Guardar predicciones con etiquetas mapeadas\n",
    "                    predict_dates = predict_features.index\n",
    "                    for j, date in enumerate(predict_dates):\n",
    "                        old_label = predict_labels[j]\n",
    "                        if old_label in regime_map:\n",
    "                            self.regimes.loc[date, 'regime'] = regime_map[old_label]\n",
    "                        else:\n",
    "                            self.regimes.loc[date, 'regime'] = old_label\n",
    "        \n",
    "        # Llenar valores faltantes forward fill\n",
    "        self.regimes = self.regimes.fillna(method='ffill')\n",
    "        \n",
    "        # Si hay valores faltantes al inicio, usar backfill\n",
    "        self.regimes = self.regimes.fillna(method='bfill')\n",
    "        \n",
    "        # Convertir a enteros\n",
    "        self.regimes['regime'] = self.regimes['regime'].astype(int)\n",
    "        \n",
    "        # Guardar regímenes identificados\n",
    "        self.regimes.to_csv('./artifacts/results/data/market_regimes.csv')\n",
    "        \n",
    "        # Visualizar regímenes\n",
    "        self.plot_market_regimes()\n",
    "    \n",
    "    def plot_market_regimes(self):\n",
    "        \"\"\"\n",
    "        Visualiza los regímenes de mercado identificados junto con el precio del índice.\n",
    "        \"\"\"\n",
    "        if self.regimes is None:\n",
    "            print(\"No hay regímenes identificados para visualizar.\")\n",
    "            return\n",
    "        \n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 12), sharex=True)\n",
    "        \n",
    "        # Gráfico de precios\n",
    "        ax1.plot(self.market_data.loc[self.regimes.index, 'close'], 'k-', label='Índice de Mercado')\n",
    "        ax1.set_ylabel('Precio')\n",
    "        ax1.set_title('Regímenes de Mercado Identificados')\n",
    "        ax1.legend(loc='upper left')\n",
    "        \n",
    "        # MEJORA 12: Añadir gráfico de VIX si disponible\n",
    "        if self.vix_data is not None:\n",
    "            common_dates = self.vix_data.index.intersection(self.regimes.index)\n",
    "            ax2.plot(common_dates, self.vix_data.loc[common_dates, 'close'], 'r-', label='VIX')\n",
    "            ax2.set_ylabel('VIX')\n",
    "            ax2.legend(loc='upper left')\n",
    "        else:\n",
    "            # Si no hay VIX, mostrar volatilidad\n",
    "            vol_data = self.market_data['returns'].rolling(21).std() * np.sqrt(252)\n",
    "            ax2.plot(vol_data.loc[self.regimes.index], 'r-', label='Volatilidad (21d)')\n",
    "            ax2.set_ylabel('Volatilidad Anualizada')\n",
    "            ax2.legend(loc='upper left')\n",
    "        \n",
    "        # Gráfico de regímenes\n",
    "        regime_colors = ['green', 'orange', 'red']  # Bajo, medio, alto riesgo\n",
    "        \n",
    "        for regime in range(self.n_regimes):\n",
    "            regime_periods = self.regimes[self.regimes['regime'] == regime].index\n",
    "            \n",
    "            if len(regime_periods) > 0:\n",
    "                # Agrupar períodos consecutivos\n",
    "                groups = []\n",
    "                current_group = [regime_periods[0]]\n",
    "                \n",
    "                for i in range(1, len(regime_periods)):\n",
    "                    if (regime_periods[i] - regime_periods[i-1]).days <= 2:  # Considerar días consecutivos\n",
    "                        current_group.append(regime_periods[i])\n",
    "                    else:\n",
    "                        groups.append(current_group)\n",
    "                        current_group = [regime_periods[i]]\n",
    "                \n",
    "                groups.append(current_group)\n",
    "                \n",
    "                # Dibujar rectángulos para cada grupo\n",
    "                for group in groups:\n",
    "                    if len(group) > 1:\n",
    "                        start_date = group[0]\n",
    "                        end_date = group[-1]\n",
    "                        ax3.axvspan(start_date, end_date, alpha=0.3, color=regime_colors[regime])\n",
    "        \n",
    "        # Etiquetar regímenes\n",
    "        labels = ['Baja Volatilidad/Alcista', 'Transición/Neutral', 'Alta Volatilidad/Bajista']\n",
    "        handles = [plt.Rectangle((0,0),1,1, color=regime_colors[i], alpha=0.3) for i in range(self.n_regimes)]\n",
    "        ax3.legend(handles, labels, loc='upper left')\n",
    "        \n",
    "        ax3.plot(self.regimes.index, self.regimes['regime'], 'k-', alpha=0.7)\n",
    "        ax3.set_ylabel('Régimen')\n",
    "        ax3.set_xlabel('Fecha')\n",
    "        \n",
    "        # Ajustar formato de fechas\n",
    "        fig.autofmt_xdate()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('./artifacts/results/figures/market_regimes.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def calculate_factors(self, date: pd.Timestamp) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Calcula todos los factores para una fecha específica.\n",
    "        \n",
    "        Args:\n",
    "            date: Fecha para la que se calculan los factores\n",
    "        \n",
    "        Returns:\n",
    "            Diccionario con Series de factores calculados\n",
    "        \"\"\"\n",
    "        # MEJORA 13: Ventana de datos variable según factor\n",
    "        # Usar más datos históricos para factores de value y quality\n",
    "        lookback_long = date - pd.Timedelta(days=365*2)  # 2 años para factores de más largo plazo\n",
    "        lookback_medium = date - pd.Timedelta(days=365)  # 1 año para factores generales\n",
    "        lookback_short = date - pd.Timedelta(days=90)    # 3 meses para momentum corto plazo y reversal\n",
    "        \n",
    "        # Filtrar acciones con datos completos\n",
    "        valid_stocks = []\n",
    "        for ticker, data in self.stock_data.items():\n",
    "            slice_data = data.loc[lookback_medium:date]\n",
    "            if slice_data.shape[0] > 200:  # Al menos 200 días de datos\n",
    "                valid_stocks.append(ticker)\n",
    "        \n",
    "        factors = {}\n",
    "        \n",
    "        # 1. VALOR\n",
    "        # Para factores fundamentales, en una implementación real usaríamos datos trimestrales\n",
    "        # Aquí usamos proxies basados en precio y volumen\n",
    "        \n",
    "        # MEJORA 14: Mejorar proxy de valor usando datos de precio históricos\n",
    "        pb_proxy = {}\n",
    "        for ticker in valid_stocks:\n",
    "            data = self.stock_data[ticker].loc[lookback_long:date]\n",
    "            if data.shape[0] > 0:\n",
    "                # Usar ratio precio actual / precio máximo histórico como proxy de valor\n",
    "                current_price = data['close'].iloc[-1]\n",
    "                historical_high = data['close'].max()\n",
    "                if historical_high > 0:\n",
    "                    pb_proxy[ticker] = current_price / historical_high\n",
    "        factors['PB_proxy'] = pd.Series(pb_proxy)\n",
    "        \n",
    "        # MEJORA 15: Proxy de valor basado en volumen-precio\n",
    "        eveb_proxy = {}\n",
    "        for ticker in valid_stocks:\n",
    "            data = self.stock_data[ticker].loc[lookback_long:date]\n",
    "            if data.shape[0] > 0:\n",
    "                # Precio / (volumen * volatilidad) como proxy de valor\n",
    "                avg_volume = data['volume'].mean()\n",
    "                volatility = data['returns'].std() * np.sqrt(252)\n",
    "                current_price = data['close'].iloc[-1]\n",
    "                if avg_volume > 0 and volatility > 0:\n",
    "                    eveb_proxy[ticker] = current_price / (avg_volume * volatility)\n",
    "        factors['EVEB_proxy'] = pd.Series(eveb_proxy)\n",
    "        \n",
    "        # 2. MOMENTUM\n",
    "        \n",
    "        # MEJORA 16: Múltiples períodos de momentum con ponderación\n",
    "        momentum_periods = [\n",
    "            (21, 0.2),    # 1 mes (20%)\n",
    "            (63, 0.3),    # 3 meses (30%)\n",
    "            (126, 0.3),   # 6 meses (30%)\n",
    "            (252, 0.2)    # 12 meses (20%)\n",
    "        ]\n",
    "        \n",
    "        weighted_momentum = {}\n",
    "        for ticker in valid_stocks:\n",
    "            data = self.stock_data[ticker].loc[lookback_long:date]\n",
    "            if data.shape[0] > 252:  # Al menos 1 año de datos\n",
    "                momentum_sum = 0.0\n",
    "                weight_sum = 0.0\n",
    "                \n",
    "                for days, weight in momentum_periods:\n",
    "                    if data.shape[0] >= days:\n",
    "                        # Precio actual vs hace \"days\" días\n",
    "                        ret = data['close'].iloc[-1] / data['close'].iloc[-days] - 1\n",
    "                        momentum_sum += ret * weight\n",
    "                        weight_sum += weight\n",
    "                \n",
    "                if weight_sum > 0:\n",
    "                    weighted_momentum[ticker] = momentum_sum / weight_sum\n",
    "                    \n",
    "        factors['Momentum_weighted'] = pd.Series(weighted_momentum)\n",
    "        \n",
    "        # MEJORA 17: Momentum ajustado por volatilidad (Sharpe de momentum)\n",
    "        momentum_sharpe = {}\n",
    "        for ticker in valid_stocks:\n",
    "            data = self.stock_data[ticker].loc[lookback_medium:date]\n",
    "            if data.shape[0] > 125:  # Al menos 6 meses de datos\n",
    "                # Rendimiento de 6 meses\n",
    "                ret_6m = data['close'].iloc[-1] / data['close'].iloc[-min(125, data.shape[0])] - 1\n",
    "                \n",
    "                # Volatilidad de 6 meses\n",
    "                vol_6m = data['returns'].tail(min(125, data.shape[0])).std() * np.sqrt(252)\n",
    "                \n",
    "                if vol_6m > 0:\n",
    "                    momentum_sharpe[ticker] = ret_6m / vol_6m\n",
    "                    \n",
    "        factors['Momentum_sharpe'] = pd.Series(momentum_sharpe)\n",
    "        \n",
    "        # 3. CALIDAD\n",
    "        \n",
    "        # MEJORA 18: Mejor proxy para estabilidad de ganancias\n",
    "        price_stability = {}\n",
    "        for ticker in valid_stocks:\n",
    "            data = self.stock_data[ticker].loc[lookback_long:date]\n",
    "            if data.shape[0] > 252:\n",
    "                # Calcular downside deviation (solo rendimientos negativos)\n",
    "                neg_returns = data['returns'][data['returns'] < 0]\n",
    "                if len(neg_returns) > 0:\n",
    "                    downside_vol = neg_returns.std() * np.sqrt(252)\n",
    "                    if downside_vol > 0:\n",
    "                        price_stability[ticker] = 1 / downside_vol\n",
    "                        \n",
    "        factors['Downside_stability'] = pd.Series(price_stability)\n",
    "        \n",
    "        # MEJORA 19: Proxy de calidad basado en consistencia de rendimientos\n",
    "        consistency = {}\n",
    "        for ticker in valid_stocks:\n",
    "            data = self.stock_data[ticker].loc[lookback_long:date]\n",
    "            if data.shape[0] > 252:\n",
    "                # Calcular % de días con rendimiento positivo\n",
    "                positive_days = (data['returns'] > 0).sum() / len(data['returns'])\n",
    "                consistency[ticker] = positive_days\n",
    "                \n",
    "        factors['Return_consistency'] = pd.Series(consistency)\n",
    "        \n",
    "        # 4. VOLATILIDAD\n",
    "        \n",
    "        # MEJORA 20: Múltiples medidas de volatilidad y riesgo\n",
    "        \n",
    "        # Volatilidad realizada (más reciente tiene mayor peso)\n",
    "        vol_weights = [0.6, 0.3, 0.1]  # 60% 1 mes, 30% 3 meses, 10% 6 meses\n",
    "        periods = [21, 63, 126]\n",
    "        \n",
    "        weighted_vol = {}\n",
    "        for ticker in valid_stocks:\n",
    "            data = self.stock_data[ticker].loc[lookback_medium:date]\n",
    "            if data.shape[0] > max(periods):\n",
    "                vol_sum = 0.0\n",
    "                weight_sum = 0.0\n",
    "                \n",
    "                for days, weight in zip(periods, vol_weights):\n",
    "                    period_vol = data['returns'].tail(days).std() * np.sqrt(252)\n",
    "                    vol_sum += period_vol * weight\n",
    "                    weight_sum += weight\n",
    "                \n",
    "                if weight_sum > 0:\n",
    "                    weighted_vol[ticker] = vol_sum / weight_sum\n",
    "                    \n",
    "        factors['Volatility_weighted'] = pd.Series(weighted_vol)\n",
    "        \n",
    "        # Beta mejorado (más estable)\n",
    "        beta_improved = {}\n",
    "        for ticker in valid_stocks:\n",
    "            stock_data = self.stock_data[ticker].loc[lookback_medium:date]\n",
    "            market_slice = self.market_data.loc[lookback_medium:date]\n",
    "            \n",
    "            # Alinear datos\n",
    "            common_dates = stock_data.index.intersection(market_slice.index)\n",
    "            if len(common_dates) > 120:  # Al menos 6 meses de datos\n",
    "                stock_returns = stock_data.loc[common_dates, 'returns']\n",
    "                market_returns = market_slice.loc[common_dates, 'returns']\n",
    "                \n",
    "                # Remover NaN\n",
    "                valid_mask = ~(np.isnan(stock_returns) | np.isnan(market_returns))\n",
    "                stock_returns = stock_returns[valid_mask]\n",
    "                market_returns = market_returns[valid_mask]\n",
    "                \n",
    "                if len(stock_returns) > 120:\n",
    "                    # MEJORA 21: Beta con ponderación exponencial (mayor peso a datos recientes)\n",
    "                    # Crear pesos exponenciales (más reciente = más peso)\n",
    "                    n = len(stock_returns)\n",
    "                    weights_recent = np.exp(np.linspace(0, 1, n)) - 1  # Pesos exponenciales\n",
    "                    weights_recent = weights_recent / weights_recent.sum()  # Normalizar\n",
    "                    \n",
    "                    # Calcular beta ponderado\n",
    "                    # CORRECCIÓN: Usar implementación correcta de covarianza y varianza ponderadas\n",
    "                    # Calcular media ponderada\n",
    "                    weighted_mean_market = np.sum(market_returns * weights_recent)\n",
    "                    weighted_mean_stock = np.sum(stock_returns * weights_recent)\n",
    "                    \n",
    "                    # Calcular covarianza ponderada\n",
    "                    cov = np.sum(weights_recent * (stock_returns - weighted_mean_stock) * (market_returns - weighted_mean_market))\n",
    "                    \n",
    "                    # Calcular varianza ponderada\n",
    "                    var = np.sum(weights_recent * (market_returns - weighted_mean_market)**2)\n",
    "                    \n",
    "                    if var > 0:\n",
    "                        beta = cov / var\n",
    "                        beta_improved[ticker] = beta\n",
    "                        \n",
    "        factors['Beta_improved'] = pd.Series(beta_improved)\n",
    "        \n",
    "        # 5. LIQUIDEZ\n",
    "        \n",
    "        # MEJORA 22: Tendencia de volumen\n",
    "        vol_trend = {}\n",
    "        for ticker in valid_stocks:\n",
    "            data = self.stock_data[ticker].loc[lookback_medium:date]\n",
    "            if data.shape[0] > 60:\n",
    "                # Calcular tendencia de volumen: volumen reciente vs histórico\n",
    "                recent_vol = data['volume'].tail(20).mean()\n",
    "                historical_vol = data['volume'].mean()\n",
    "                \n",
    "                if historical_vol > 0:\n",
    "                    vol_trend[ticker] = recent_vol / historical_vol\n",
    "                    \n",
    "        factors['Volume_trend'] = pd.Series(vol_trend)\n",
    "        \n",
    "        # MEJORA 23: Mejor ratio de iliquidez\n",
    "        amihud_improved = {}\n",
    "        for ticker in valid_stocks:\n",
    "            data = self.stock_data[ticker].loc[lookback_medium:date]\n",
    "            if data.shape[0] > 60:\n",
    "                # Solo considerar días con volumen significativo\n",
    "                valid_days = data[data['volume'] > 0]\n",
    "                if len(valid_days) > 30:\n",
    "                    # MEJORA: Ratio de Amihud mejorado (|return| / (price * volume))\n",
    "                    price_vol = valid_days['close'] * valid_days['volume']\n",
    "                    illiq = (valid_days['returns'].abs() / price_vol).mean() * 1e9  # Escalar\n",
    "                    if illiq > 0:\n",
    "                        amihud_improved[ticker] = illiq\n",
    "                        \n",
    "        factors['Illiquidity_improved'] = pd.Series(amihud_improved)\n",
    "        \n",
    "        # 6. REVERSAL (Nuevo factor)\n",
    "        \n",
    "        # MEJORA 24: Añadir factor de reversión a corto plazo\n",
    "        reversal_st = {}\n",
    "        for ticker in valid_stocks:\n",
    "            data = self.stock_data[ticker].loc[lookback_short:date]\n",
    "            if data.shape[0] > 20:\n",
    "                # Retorno 1 semana (invertido para capturar reversión)\n",
    "                ret_1w = data['close'].iloc[-1] / data['close'].iloc[-min(5, data.shape[0])] - 1\n",
    "                reversal_st[ticker] = -ret_1w  # Invertir para que valores positivos indiquen potencial de reversión\n",
    "                \n",
    "        factors['Reversal_1w'] = pd.Series(reversal_st)\n",
    "        \n",
    "        # MEJORA 25: Reversión respecto a media móvil\n",
    "        reversal_ma = {}\n",
    "        for ticker in valid_stocks:\n",
    "            data = self.stock_data[ticker].loc[lookback_short:date]\n",
    "            if data.shape[0] > 20:\n",
    "                # Calcular desviación de precio respecto a media móvil de 20 días\n",
    "                if not data['close'].empty:\n",
    "                    ma20 = data['close'].rolling(20).mean().iloc[-1]\n",
    "                    current = data['close'].iloc[-1]\n",
    "                    if ma20 > 0:\n",
    "                        # Negativo si precio > MA (potencial reversión a la baja)\n",
    "                        # Positivo si precio < MA (potencial reversión al alza)\n",
    "                        reversal_ma[ticker] = (ma20 / current) - 1\n",
    "                        \n",
    "        factors['Reversal_ma20'] = pd.Series(reversal_ma)\n",
    "        \n",
    "        # Normalizar todos los factores (z-score) y controlar outliers\n",
    "        normalized_factors = {}\n",
    "        for factor_name, factor_values in factors.items():\n",
    "            if len(factor_values) > 0:\n",
    "                # Winsorizar para controlar outliers (recortar los extremos al 1% y 99%)\n",
    "                lower_bound = factor_values.quantile(0.01)\n",
    "                upper_bound = factor_values.quantile(0.99)\n",
    "                winsorized = factor_values.clip(lower=lower_bound, upper=upper_bound)\n",
    "                \n",
    "                # Z-score normalization\n",
    "                mean = winsorized.mean()\n",
    "                std = winsorized.std()\n",
    "                if std > 0:\n",
    "                    normalized = (winsorized - mean) / std\n",
    "                    normalized_factors[factor_name] = normalized\n",
    "        \n",
    "        # Invertir factores donde valores menores son mejores\n",
    "        for factor_name in ['EVEB_proxy', 'Volatility_weighted', 'Illiquidity_improved']:\n",
    "            if factor_name in normalized_factors:\n",
    "                normalized_factors[factor_name] = -normalized_factors[factor_name]\n",
    "        \n",
    "        # MEJORA 26: Organizar factores por categoría con pesos internos mejorados\n",
    "        factor_by_category = {\n",
    "            'Value': [('PB_proxy', 0.5), ('EVEB_proxy', 0.5)],\n",
    "            'Momentum': [('Momentum_weighted', 0.6), ('Momentum_sharpe', 0.4)],\n",
    "            'Quality': [('Downside_stability', 0.6), ('Return_consistency', 0.4)],\n",
    "            'Volatility': [('Volatility_weighted', 0.4), ('Beta_improved', 0.6)],\n",
    "            'Liquidity': [('Volume_trend', 0.5), ('Illiquidity_improved', 0.5)],\n",
    "            'Reversal': [('Reversal_1w', 0.5), ('Reversal_ma20', 0.5)]\n",
    "        }\n",
    "        \n",
    "        # Asegurar que todas las acciones tengan datos en todas las categorías\n",
    "        common_stocks = set()\n",
    "        for factor in normalized_factors.values():\n",
    "            if common_stocks:\n",
    "                common_stocks = common_stocks.intersection(set(factor.index))\n",
    "            else:\n",
    "                common_stocks = set(factor.index)\n",
    "        \n",
    "        # Filtrar a acciones comunes\n",
    "        filtered_factors = {}\n",
    "        for factor_name, factor_values in normalized_factors.items():\n",
    "            filtered_factors[factor_name] = factor_values.loc[list(common_stocks)]\n",
    "        \n",
    "        return filtered_factors, factor_by_category\n",
    "    \n",
    "    def thompson_sampling(self, regime: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Implementa Thompson Sampling para seleccionar categorías de factores\n",
    "        basado en sus distribuciones de creencia.\n",
    "        \n",
    "        Args:\n",
    "            regime: Régimen de mercado actual\n",
    "        \n",
    "        Returns:\n",
    "            Diccionario con pesos para cada categoría\n",
    "        \"\"\"\n",
    "        samples = {}\n",
    "        \n",
    "        # MEJORA 27: Ajuste de pesos según régimen de mercado\n",
    "        # Aplicar factores de ajuste para reforzar ciertas categorías en ciertos regímenes\n",
    "        regime_adjustments = {\n",
    "            0: {  # Régimen 0: Baja volatilidad/alcista\n",
    "                'Value': 1.0,\n",
    "                'Momentum': 1.2,  # Potenciar momentum en mercados alcistas\n",
    "                'Quality': 0.8,\n",
    "                'Volatility': 0.8,\n",
    "                'Liquidity': 0.8,\n",
    "                'Reversal': 0.8\n",
    "            },\n",
    "            1: {  # Régimen 1: Transición/neutral\n",
    "                'Value': 1.0,\n",
    "                'Momentum': 1.0,\n",
    "                'Quality': 1.0,\n",
    "                'Volatility': 1.0,\n",
    "                'Liquidity': 1.0,\n",
    "                'Reversal': 1.0\n",
    "            },\n",
    "            2: {  # Régimen 2: Alta volatilidad/bajista\n",
    "                'Value': 0.8,\n",
    "                'Momentum': 0.8,\n",
    "                'Quality': 1.2,  # Potenciar calidad en mercados bajistas\n",
    "                'Volatility': 1.0,\n",
    "                'Liquidity': 1.1,\n",
    "                'Reversal': 1.2  # Potenciar reversión en mercados volátiles\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Para cada categoría, muestrear de su distribución Beta con ajuste\n",
    "        for category in self.factor_categories:\n",
    "            alpha = self.belief_distributions[category][regime]['alpha']\n",
    "            beta = self.belief_distributions[category][regime]['beta']\n",
    "            \n",
    "            # MEJORA 28: Tomar múltiples muestras y promediar para más estabilidad\n",
    "            n_samples = 5\n",
    "            category_samples = []\n",
    "            \n",
    "            for _ in range(n_samples):\n",
    "                sample = np.random.beta(alpha, beta)\n",
    "                category_samples.append(sample)\n",
    "            \n",
    "            # Usar promedio de muestras\n",
    "            avg_sample = np.mean(category_samples)\n",
    "            \n",
    "            # Aplicar ajuste de régimen\n",
    "            adjustment = regime_adjustments.get(regime, {}).get(category, 1.0)\n",
    "            samples[category] = avg_sample * adjustment\n",
    "        \n",
    "        # Normalizar para obtener pesos\n",
    "        total = sum(samples.values())\n",
    "        weights = {category: sample / total for category, sample in samples.items()}\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def update_beliefs(self, date: pd.Timestamp, lookback_days: int = 30):\n",
    "        \"\"\"\n",
    "        Actualiza las distribuciones de creencia basado en rendimiento reciente\n",
    "        de cada categoría en cada régimen.\n",
    "        \n",
    "        Args:\n",
    "            date: Fecha actual\n",
    "            lookback_days: Días hacia atrás para evaluar rendimiento\n",
    "        \"\"\"\n",
    "        # Calcular ventana de tiempo\n",
    "        start_date = date - pd.Timedelta(days=lookback_days)\n",
    "        \n",
    "        # Obtener regímenes en el período\n",
    "        if self.regimes is None or start_date not in self.regimes.index:\n",
    "            return\n",
    "            \n",
    "        regime_period = self.regimes.loc[start_date:date]\n",
    "        \n",
    "        # Para cada régimen presente en el período\n",
    "        for regime in regime_period['regime'].unique():\n",
    "            regime_dates = regime_period[regime_period['regime'] == regime].index\n",
    "            if len(regime_dates) < 5:  # Necesitamos suficientes días en el régimen\n",
    "                continue\n",
    "                \n",
    "            # Para cada categoría, evaluar rendimiento\n",
    "            for category in self.factor_categories:\n",
    "                # Suponiendo que tenemos un método para evaluar rendimiento de categoría\n",
    "                performance = self._evaluate_category_performance(category, regime_dates)\n",
    "                \n",
    "                # Actualizar distribuciones de creencia\n",
    "                if performance is not None:\n",
    "                    # MEJORA 29: Mejorar actualización Bayesiana con escala de rendimiento\n",
    "                    # En lugar de éxito/fracaso binario, usar una escala continua\n",
    "                    performance_score = (performance + 1) / 2  # Convertir a [0, 1]\n",
    "                    \n",
    "                    # Limitar valores extremos\n",
    "                    performance_score = max(0.1, min(0.9, performance_score))\n",
    "                    \n",
    "                    # Factores de actualización proporcionales al rendimiento\n",
    "                    success_update = performance_score * 2  # Máx 1.8\n",
    "                    failure_update = (1 - performance_score) * 2  # Máx 1.8\n",
    "                    \n",
    "                    # Actualizar distribución de creencia (prior conjugado Beta)\n",
    "                    self.belief_distributions[category][regime]['alpha'] += success_update\n",
    "                    self.belief_distributions[category][regime]['beta'] += failure_update\n",
    "                    \n",
    "                    # Aplicar decay exponencial para dar más peso a observaciones recientes\n",
    "                    decay_factor = 0.95\n",
    "                    alpha = self.belief_distributions[category][regime]['alpha']\n",
    "                    beta = self.belief_distributions[category][regime]['beta']\n",
    "                    \n",
    "                    # Decay\n",
    "                    self.belief_distributions[category][regime]['alpha'] = 1 + decay_factor * (alpha - 1)\n",
    "                    self.belief_distributions[category][regime]['beta'] = 1 + decay_factor * (beta - 1)\n",
    "    \n",
    "    def _evaluate_category_performance(self, category: str, dates: pd.DatetimeIndex) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Evalúa el rendimiento de una categoría de factores en un período específico.\n",
    "        \n",
    "        Args:\n",
    "            category: Categoría de factores\n",
    "            dates: Fechas para evaluar rendimiento\n",
    "        \n",
    "        Returns:\n",
    "            Score de rendimiento (escala -1 a 1)\n",
    "        \"\"\"\n",
    "        # MEJORA 30: Evaluación de rendimiento más realista\n",
    "        \n",
    "        # Si no tenemos suficientes fechas, retornar None\n",
    "        if len(dates) < 5:\n",
    "            return None\n",
    "            \n",
    "        # Verificar si tenemos datos de rendimiento previos\n",
    "        if category in self.factor_performance and len(self.factor_performance[category]) > 0:\n",
    "            # Filtrar a fechas relevantes\n",
    "            perf = [p for d, p in self.factor_performance[category] if d in dates]\n",
    "            if len(perf) > 0:\n",
    "                return np.mean(perf)\n",
    "        \n",
    "        # Si no hay datos previos, usar simulación más realista basada en regímenes conocidos\n",
    "        regime = self.regimes.loc[dates[0], 'regime']\n",
    "        \n",
    "        # MEJORA 31: Simulación de rendimiento basada en literatura académica\n",
    "        # Valores basados en estudios sobre factores en diferentes regímenes\n",
    "        expected_performances = {\n",
    "            0: {  # Régimen 0: Baja volatilidad/alcista\n",
    "                'Value': 0.1,       # Value tiende a funcionar mal en mercados alcistas\n",
    "                'Momentum': 0.5,    # Momentum funciona bien en mercados alcistas\n",
    "                'Quality': 0.3,     # Quality es neutral-positivo\n",
    "                'Volatility': 0.2,  # Volatilidad baja es positiva pero no extraordinaria\n",
    "                'Liquidity': 0.1,   # Liquidez menos importante en mercados tranquilos\n",
    "                'Reversal': -0.1    # Reversión funciona mal en tendencias fuertes\n",
    "            },\n",
    "            1: {  # Régimen 1: Transición/neutral\n",
    "                'Value': 0.3,       # Value funciona mejor en transiciones\n",
    "                'Momentum': 0.2,    # Momentum es menos efectivo en transiciones\n",
    "                'Quality': 0.4,     # Quality es consistente\n",
    "                'Volatility': 0.3,  # Volatilidad controlada es positiva\n",
    "                'Liquidity': 0.2,   # Liquidez más importante en transiciones\n",
    "                'Reversal': 0.3     # Reversión funciona en mercados laterales\n",
    "            },\n",
    "            2: {  # Régimen 2: Alta volatilidad/bajista\n",
    "                'Value': 0.4,       # Value puede funcionar en crisis (pero con lag)\n",
    "                'Momentum': -0.2,   # Momentum sufre en mercados volátiles\n",
    "                'Quality': 0.5,     # Quality es defensivo\n",
    "                'Volatility': 0.4,  # Baja volatilidad es muy valiosa\n",
    "                'Liquidity': 0.4,   # Liquidez es crítica\n",
    "                'Reversal': 0.4     # Reversión funciona muy bien en volatilidad\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Obtener rendimiento esperado para esta categoría en este régimen\n",
    "        base_performance = expected_performances.get(regime, {}).get(category, 0)\n",
    "        \n",
    "        # Añadir ruido aleatorio más acotado\n",
    "        noise = np.random.normal(0, 0.1)  # Desviación estándar reducida\n",
    "        \n",
    "        return max(-1.0, min(1.0, base_performance + noise))  # Limitar a [-1, 1]\n",
    "    \n",
    "    def calculate_factor_scores(self, factors: Dict[str, pd.Series], factor_by_category: Dict[str, List[Tuple[str, float]]]) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Calcula scores compuestos para cada categoría de factores.\n",
    "        \n",
    "        Args:\n",
    "            factors: Diccionario de factores individuales\n",
    "            factor_by_category: Mapping de categorías a factores con pesos\n",
    "            \n",
    "        Returns:\n",
    "            Diccionario con scores para cada categoría\n",
    "        \"\"\"\n",
    "        category_scores = {}\n",
    "        \n",
    "        # MEJORA 32: Usar pesos internos para cada factor en su categoría\n",
    "        for category, factor_items in factor_by_category.items():\n",
    "            available_items = [(f, w) for f, w in factor_items if f in factors]\n",
    "            \n",
    "            if available_items:\n",
    "                # Calcular suma ponderada de factores\n",
    "                scores = None\n",
    "                total_weight = 0\n",
    "                \n",
    "                for factor_name, weight in available_items:\n",
    "                    if scores is None:\n",
    "                        scores = factors[factor_name] * weight\n",
    "                    else:\n",
    "                        scores += factors[factor_name] * weight\n",
    "                    total_weight += weight\n",
    "                \n",
    "                if total_weight > 0 and scores is not None:\n",
    "                    category_scores[category] = scores / total_weight\n",
    "        \n",
    "        return category_scores\n",
    "    \n",
    "    def construct_portfolio(self, date: pd.Timestamp) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Construye el portafolio para una fecha específica utilizando\n",
    "        el enfoque de aprendizaje por refuerzo.\n",
    "        \n",
    "        Args:\n",
    "            date: Fecha para la que se construye el portafolio\n",
    "            \n",
    "        Returns:\n",
    "            Series con pesos del portafolio\n",
    "        \"\"\"\n",
    "        # Verificar si tenemos identificación de régimen para esta fecha\n",
    "        if self.regimes is None or date not in self.regimes.index:\n",
    "            print(f\"No hay datos de régimen para la fecha {date}. Usando equiponderación.\")\n",
    "            # Usar equiponderación entre categorías si no hay datos de régimen\n",
    "            category_weights = {cat: 1.0/len(self.factor_categories) for cat in self.factor_categories}\n",
    "        else:\n",
    "            # Obtener régimen actual\n",
    "            current_regime = self.regimes.loc[date, 'regime']\n",
    "            self.current_regime = current_regime\n",
    "            \n",
    "            # Usar Thompson Sampling para seleccionar pesos de categorías\n",
    "            category_weights = self.thompson_sampling(current_regime)\n",
    "        \n",
    "        # Calcular factores\n",
    "        factors, factor_by_category = self.calculate_factors(date)\n",
    "        \n",
    "        # Si no hay suficientes factores, retornar portafolio vacío\n",
    "        if not factors or len(factors) == 0:\n",
    "            return pd.Series(dtype='float64')\n",
    "        \n",
    "        # Calcular scores por categoría\n",
    "        category_scores = self.calculate_factor_scores(factors, factor_by_category)\n",
    "        \n",
    "        # MEJORA 33: Combinar scores con pesos adaptativos\n",
    "        # Ajustar ligeramente pesos según historial reciente\n",
    "        if hasattr(self, 'previous_weights') and self.previous_weights is not None:\n",
    "            # Obtener rendimientos recientes\n",
    "            lookback_start = date - pd.Timedelta(days=30)\n",
    "            recent_returns = {}\n",
    "            \n",
    "            for ticker in self.previous_weights.index:\n",
    "                if ticker in self.stock_data and lookback_start in self.stock_data[ticker].index:\n",
    "                    recent_returns[ticker] = self.stock_data[ticker].loc[lookback_start:date, 'returns'].mean() * 21  # Mensualizado\n",
    "            \n",
    "            if recent_returns:\n",
    "                # Calcular rendimiento reciente ponderado del portafolio anterior\n",
    "                recent_portfolio_return = sum(self.previous_weights.get(ticker, 0) * recent_returns.get(ticker, 0) \n",
    "                                             for ticker in self.previous_weights.index)\n",
    "                \n",
    "                # Ajustar ligeramente hacia factores que funcionaron bien\n",
    "                if recent_portfolio_return > 0.01:  # Si rendimiento mensualizado > 1%\n",
    "                    # Dar más peso a factores que produjeron mejores resultados\n",
    "                    for category in self.factor_categories:\n",
    "                        if category in category_weights:\n",
    "                            category_weights[category] *= (1 + min(0.2, recent_portfolio_return * 2))  # Límite de 20% aumento\n",
    "                    \n",
    "                    # Renormalizar\n",
    "                    total = sum(category_weights.values())\n",
    "                    category_weights = {k: v/total for k, v in category_weights.items()}\n",
    "        \n",
    "        # Combinar scores ponderados por los pesos de categorías\n",
    "        combined_score = pd.Series(0, index=next(iter(factors.values())).index)\n",
    "        \n",
    "        for category, weight in category_weights.items():\n",
    "            if category in category_scores:\n",
    "                combined_score += category_scores[category] * weight\n",
    "        \n",
    "        # Normalizar scores\n",
    "        combined_score = (combined_score - combined_score.mean()) / combined_score.std()\n",
    "        \n",
    "        # Rankear acciones\n",
    "        ranked_stocks = combined_score.sort_values(ascending=False)\n",
    "        \n",
    "        # MEJORA 34: Selección de stocks más óptima (cuantiles en lugar de umbral fijo)\n",
    "        # Usar selección dinámica basada en distribución de scores\n",
    "        upper_quantile = ranked_stocks.quantile(0.7)  # Top 30%\n",
    "        selected_stocks = ranked_stocks[ranked_stocks >= upper_quantile].index\n",
    "        \n",
    "        # Asignar pesos usando función sigmoide para suavizar transición \n",
    "        weights = pd.Series(0, index=ranked_stocks.index)\n",
    "        for stock in selected_stocks:\n",
    "            # Convertir score a peso usando función sigmoide\n",
    "            score = ranked_stocks[stock]\n",
    "            weight = 1 / (1 + np.exp(-2 * (score - upper_quantile)))  # Sigmoide centrada en upper_quantile\n",
    "            weights[stock] = weight\n",
    "        \n",
    "        # Normalizar pesos\n",
    "        if weights.sum() > 0:\n",
    "            weights = weights / weights.sum()\n",
    "        \n",
    "        # Ajustar pesos para cumplir restricciones\n",
    "        weights = self._apply_constraints(weights, date)\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def _apply_constraints(self, weights: pd.Series, date: pd.Timestamp) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Aplica restricciones de portafolio: beta neutral, límites de concentración, etc.\n",
    "        \n",
    "        Args:\n",
    "            weights: Pesos iniciales del portafolio\n",
    "            date: Fecha actual\n",
    "            \n",
    "        Returns:\n",
    "            Pesos ajustados\n",
    "        \"\"\"\n",
    "        # Si no hay pesos positivos, retornar serie vacía\n",
    "        if weights.sum() == 0:\n",
    "            return weights\n",
    "            \n",
    "        # Recortar a máximo peso por acción\n",
    "        weights = weights.clip(upper=self.max_stock_weight)\n",
    "        \n",
    "        # Renormalizar\n",
    "        if weights.sum() > 0:\n",
    "            weights = weights / weights.sum()\n",
    "        \n",
    "        # MEJORA 35: Ajuste de beta más robusto\n",
    "        try:\n",
    "            # Calcular betas\n",
    "            betas = {}\n",
    "            lookback_start = date - pd.Timedelta(days=252)\n",
    "            \n",
    "            for ticker in weights.index[weights > 0]:\n",
    "                if ticker in self.stock_data:\n",
    "                    stock_data = self.stock_data[ticker].loc[lookback_start:date]\n",
    "                    market_slice = self.market_data.loc[lookback_start:date]\n",
    "                    \n",
    "                    common_dates = stock_data.index.intersection(market_slice.index)\n",
    "                    if len(common_dates) > 60:\n",
    "                        stock_returns = stock_data.loc[common_dates, 'returns']\n",
    "                        market_returns = market_slice.loc[common_dates, 'returns']\n",
    "                        \n",
    "                        # Remover NaN\n",
    "                        valid_mask = ~(np.isnan(stock_returns) | np.isnan(market_returns))\n",
    "                        stock_returns = stock_returns[valid_mask]\n",
    "                        market_returns = market_returns[valid_mask]\n",
    "                        \n",
    "                        if len(stock_returns) > 60:\n",
    "                            # MEJORA: Usar datos recientes con mayor peso\n",
    "                            n = len(stock_returns)\n",
    "                            weights_recent = np.exp(np.linspace(0, 1, n)) - 1\n",
    "                            weights_recent = weights_recent / weights_recent.sum()\n",
    "                            \n",
    "                            # CORRECCIÓN: Cálculo de beta ponderado\n",
    "                            # Calcular medias ponderadas\n",
    "                            weighted_mean_stock = np.sum(stock_returns * weights_recent)\n",
    "                            weighted_mean_market = np.sum(market_returns * weights_recent)\n",
    "                            \n",
    "                            # Calcular covarianza ponderada\n",
    "                            cov = np.sum(weights_recent * (stock_returns - weighted_mean_stock) * \n",
    "                                        (market_returns - weighted_mean_market))\n",
    "                            \n",
    "                            # Calcular varianza ponderada\n",
    "                            var = np.sum(weights_recent * (market_returns - weighted_mean_market)**2)\n",
    "                            \n",
    "                            if var > 0:\n",
    "                                betas[ticker] = cov / var\n",
    "            \n",
    "            # Si tenemos suficientes betas, ajustar portafolio\n",
    "            if len(betas) > 5:\n",
    "                beta_series = pd.Series(betas)\n",
    "                portfolio_beta = (weights.loc[beta_series.index] * beta_series).sum()\n",
    "                \n",
    "                # MEJORA 36: Mejor ajuste de beta usando optimización iterativa\n",
    "                if abs(portfolio_beta - self.target_beta) > self.beta_range:\n",
    "                    # Ordenar acciones por beta\n",
    "                    sorted_betas = beta_series.sort_values()\n",
    "                    \n",
    "                    iterations = 0\n",
    "                    max_iterations = 10\n",
    "                    \n",
    "                    while abs(portfolio_beta - self.target_beta) > self.beta_range and iterations < max_iterations:\n",
    "                        if portfolio_beta > self.target_beta + self.beta_range:\n",
    "                            # Reducir acciones de alto beta, aumentar las de bajo beta\n",
    "                            # Identificar tickers con betas extremos\n",
    "                            high_beta_tickers = sorted_betas.tail(int(len(sorted_betas) * 0.3)).index\n",
    "                            low_beta_tickers = sorted_betas.head(int(len(sorted_betas) * 0.3)).index\n",
    "                            \n",
    "                            # Ajustar pesos\n",
    "                            adjustment = min(0.05, abs(portfolio_beta - self.target_beta) / 5)\n",
    "                            \n",
    "                            for ticker in high_beta_tickers:\n",
    "                                if ticker in weights.index:\n",
    "                                    weights[ticker] *= (1 - adjustment)\n",
    "                                    \n",
    "                            for ticker in low_beta_tickers:\n",
    "                                if ticker in weights.index:\n",
    "                                    weights[ticker] *= (1 + adjustment)\n",
    "                                    \n",
    "                        elif portfolio_beta < self.target_beta - self.beta_range:\n",
    "                            # Aumentar acciones de alto beta, reducir las de bajo beta\n",
    "                            high_beta_tickers = sorted_betas.tail(int(len(sorted_betas) * 0.3)).index\n",
    "                            low_beta_tickers = sorted_betas.head(int(len(sorted_betas) * 0.3)).index\n",
    "                            \n",
    "                            # Ajustar pesos\n",
    "                            adjustment = min(0.05, abs(portfolio_beta - self.target_beta) / 5)\n",
    "                            \n",
    "                            for ticker in high_beta_tickers:\n",
    "                                if ticker in weights.index:\n",
    "                                    weights[ticker] *= (1 + adjustment)\n",
    "                                    \n",
    "                            for ticker in low_beta_tickers:\n",
    "                                if ticker in weights.index:\n",
    "                                    weights[ticker] *= (1 - adjustment)\n",
    "                        \n",
    "                        # Renormalizar\n",
    "                        if weights.sum() > 0:\n",
    "                            weights = weights / weights.sum()\n",
    "                        \n",
    "                        # Recalcular beta de portafolio\n",
    "                        portfolio_beta = (weights.loc[beta_series.index] * beta_series).sum()\n",
    "                        iterations += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error ajustando beta: {str(e)}\")\n",
    "        \n",
    "        # Renormalizar\n",
    "        if weights.sum() > 0:\n",
    "            weights = weights / weights.sum()\n",
    "        \n",
    "        # MEJORA 37: Ajuste de volatilidad más preciso\n",
    "        try:\n",
    "            # Calcular matriz de varianza-covarianza (simplificada)\n",
    "            relevant_tickers = [ticker for ticker in weights.index if weights[ticker] > 0]\n",
    "            \n",
    "            if len(relevant_tickers) > 5:\n",
    "                lookback_start = date - pd.Timedelta(days=252)\n",
    "                returns_data = {}\n",
    "                \n",
    "                # Recopilar datos de retornos\n",
    "                for ticker in relevant_tickers:\n",
    "                    if ticker in self.stock_data:\n",
    "                        stock_data = self.stock_data[ticker].loc[lookback_start:date]\n",
    "                        returns_data[ticker] = stock_data['returns']\n",
    "                \n",
    "                # Crear DataFrame de retornos\n",
    "                returns_df = pd.DataFrame(returns_data)\n",
    "                returns_df = returns_df.fillna(0)  # Llenar NaNs con ceros\n",
    "                \n",
    "                # Calcular volatilidad y correlación\n",
    "                vol_annual = returns_df.std() * np.sqrt(252)\n",
    "                corr_matrix = returns_df.corr()\n",
    "                \n",
    "                # Calcular volatilidad estimada del portafolio\n",
    "                portfolio_vol = 0\n",
    "                for i, ticker_i in enumerate(relevant_tickers):\n",
    "                    for j, ticker_j in enumerate(relevant_tickers):\n",
    "                        if ticker_i in vol_annual.index and ticker_j in vol_annual.index:\n",
    "                            weight_i = weights[ticker_i]\n",
    "                            weight_j = weights[ticker_j]\n",
    "                            vol_i = vol_annual[ticker_i]\n",
    "                            vol_j = vol_annual[ticker_j]\n",
    "                            corr_ij = corr_matrix.loc[ticker_i, ticker_j]\n",
    "                            \n",
    "                            portfolio_vol += weight_i * weight_j * vol_i * vol_j * corr_ij\n",
    "                \n",
    "                portfolio_vol = np.sqrt(portfolio_vol)\n",
    "                \n",
    "                # Ajustar volatilidad si es necesario\n",
    "                if portfolio_vol > self.volatility_target * 1.2:  # 20% de margen\n",
    "                    # Reducir exposición proporcionalmente\n",
    "                    scale_factor = self.volatility_target / portfolio_vol\n",
    "                    weights *= scale_factor\n",
    "                    \n",
    "                    # El resto en \"efectivo\" (dejamos los pesos sin sumar 1 para simular efectivo)\n",
    "                    # En una implementación real, se incluiría posición en activo sin riesgo\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error ajustando volatilidad: {str(e)}\")\n",
    "        \n",
    "        # MEJORA 38: Control de turnover más sofisticado\n",
    "        if hasattr(self, 'previous_weights') and self.previous_weights is not None:\n",
    "            # Calcular turnover\n",
    "            turnover = self._calculate_turnover(self.previous_weights, weights)\n",
    "            \n",
    "            # Si el turnover es mayor al máximo, ajustar usando interpolación\n",
    "            if turnover > self.max_turnover:\n",
    "                # Calcular ponderación óptima entre portafolio previo y nuevo\n",
    "                t = self.max_turnover / turnover\n",
    "                \n",
    "                # Usar interpolación convexa\n",
    "                target_weights = t * weights + (1 - t) * self.previous_weights\n",
    "                \n",
    "                # Limpiar pesos muy pequeños (< 0.1%)\n",
    "                target_weights[target_weights < 0.001] = 0\n",
    "                \n",
    "                # Renormalizar\n",
    "                if target_weights.sum() > 0:\n",
    "                    weights = target_weights / target_weights.sum()\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def _calculate_turnover(self, old_weights: pd.Series, new_weights: pd.Series) -> float:\n",
    "        \"\"\"\n",
    "        Calcula el turnover entre dos conjuntos de pesos.\n",
    "        \n",
    "        Args:\n",
    "            old_weights: Pesos anteriores\n",
    "            new_weights: Nuevos pesos\n",
    "            \n",
    "        Returns:\n",
    "            Turnover como suma de cambios absolutos / 2\n",
    "        \"\"\"\n",
    "        # Unificar índices\n",
    "        all_stocks = old_weights.index.union(new_weights.index)\n",
    "        \n",
    "        old_unified = pd.Series(0, index=all_stocks)\n",
    "        new_unified = pd.Series(0, index=all_stocks)\n",
    "        \n",
    "        old_unified.loc[old_weights.index] = old_weights\n",
    "        new_unified.loc[new_weights.index] = new_weights\n",
    "        \n",
    "        # Calcular turnover\n",
    "        turnover = np.sum(np.abs(old_unified - new_unified)) / 2\n",
    "        \n",
    "        return turnover\n",
    "    \n",
    "    def calculate_portfolio_return(self, weights: pd.Series, date: pd.Timestamp) -> float:\n",
    "        \"\"\"\n",
    "        Calcula el retorno del portafolio para una fecha específica.\n",
    "        \n",
    "        Args:\n",
    "            weights: Pesos del portafolio\n",
    "            date: Fecha actual\n",
    "            \n",
    "        Returns:\n",
    "            Retorno del portafolio\n",
    "        \"\"\"\n",
    "        next_date = self._get_next_trading_date(date)\n",
    "        \n",
    "        if next_date is None:\n",
    "            return 0.0\n",
    "        \n",
    "        portfolio_return = 0.0\n",
    "        \n",
    "        for ticker, weight in weights.items():\n",
    "            if ticker in self.stock_data and next_date in self.stock_data[ticker].index:\n",
    "                stock_return = self.stock_data[ticker].loc[next_date, 'returns']\n",
    "                if not np.isnan(stock_return):\n",
    "                    portfolio_return += weight * stock_return\n",
    "        \n",
    "        # Aplicar costo de transacción si tenemos pesos anteriores\n",
    "        if hasattr(self, 'previous_weights') and self.previous_weights is not None:\n",
    "            turnover = self._calculate_turnover(self.previous_weights, weights)\n",
    "            transaction_cost = turnover * self.transaction_cost\n",
    "            portfolio_return -= transaction_cost\n",
    "        \n",
    "        # MEJORA 39: Guardar rendimiento por factor para aprendizaje\n",
    "        try:\n",
    "            if hasattr(self, 'current_regime') and self.current_regime is not None:\n",
    "                # Guardar rendimientos de categorías para aprendizaje futuro\n",
    "                factors, factor_by_category = self.calculate_factors(date)\n",
    "                if factors:\n",
    "                    category_scores = self.calculate_factor_scores(factors, factor_by_category)\n",
    "                    \n",
    "                    for category, scores in category_scores.items():\n",
    "                        # Calcular rendimiento hipotético de esta categoría\n",
    "                        top_stocks = scores.sort_values(ascending=False).head(10).index\n",
    "                        category_return = 0\n",
    "                        \n",
    "                        for ticker in top_stocks:\n",
    "                            if ticker in self.stock_data and next_date in self.stock_data[ticker].index:\n",
    "                                stock_return = self.stock_data[ticker].loc[next_date, 'returns']\n",
    "                                if not np.isnan(stock_return):\n",
    "                                    category_return += stock_return / len(top_stocks)\n",
    "                        \n",
    "                        # Ajustar por transacción\n",
    "                        category_return -= self.transaction_cost * 0.2  # Estimación simple\n",
    "                        \n",
    "                        # Guardar rendimiento\n",
    "                        if category not in self.factor_performance:\n",
    "                            self.factor_performance[category] = []\n",
    "                        \n",
    "                        self.factor_performance[category].append((date, category_return))\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error guardando rendimiento por factor: {str(e)}\")\n",
    "        \n",
    "        return portfolio_return\n",
    "    \n",
    "    def _get_next_trading_date(self, date: pd.Timestamp) -> Optional[pd.Timestamp]:\n",
    "        \"\"\"\n",
    "        Obtiene la siguiente fecha de trading disponible.\n",
    "        \n",
    "        Args:\n",
    "            date: Fecha actual\n",
    "            \n",
    "        Returns:\n",
    "            Siguiente fecha de trading o None si no hay datos\n",
    "        \"\"\"\n",
    "        # Obtener todas las fechas disponibles en los datos de mercado\n",
    "        market_dates = self.market_data.index\n",
    "        \n",
    "        # Encontrar fechas futuras\n",
    "        future_dates = market_dates[market_dates > date]\n",
    "        \n",
    "        if len(future_dates) > 0:\n",
    "            return future_dates[0]\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def run_backtest(self):\n",
    "        \"\"\"\n",
    "        Ejecuta el backtest de la estrategia para todo el período.\n",
    "        \"\"\"\n",
    "        print(\"Ejecutando backtest...\")\n",
    "        \n",
    "        # Obtener fechas de rebalanceo\n",
    "        rebalance_dates = pd.date_range(\n",
    "            start=self.start_date, \n",
    "            end=self.end_date, \n",
    "            freq=self.portfolio_rebalance_frequency\n",
    "        )\n",
    "        \n",
    "        # Filtrar a fechas disponibles en los datos\n",
    "        rebalance_dates = [date for date in rebalance_dates if date in self.market_data.index]\n",
    "        \n",
    "        # Para almacenar resultados\n",
    "        self.portfolio_weights_history = []\n",
    "        portfolio_returns = []\n",
    "        dates = []\n",
    "        \n",
    "        # Para almacenar pesos previos\n",
    "        self.previous_weights = None\n",
    "        \n",
    "        # MEJORA 40: Agregar stop-loss dinámico\n",
    "        cumulative_return = 1.0\n",
    "        max_cumulative = 1.0\n",
    "        in_market = True  # Indicador de si estamos invertidos o en efectivo\n",
    "        \n",
    "        # Ejecutar para cada fecha de rebalanceo\n",
    "        for date in tqdm(rebalance_dates, desc=\"Procesando fechas de rebalanceo\"):\n",
    "            try:\n",
    "                # Verificar stop-loss\n",
    "                if cumulative_return < max_cumulative * 0.85:  # 15% drawdown\n",
    "                    # Activar modo defensivo\n",
    "                    print(f\"Stop-loss activado en {date}. Drawdown: {(1 - cumulative_return/max_cumulative)*100:.2f}%\")\n",
    "                    in_market = False\n",
    "                    \n",
    "                # Verificar condición para volver al mercado\n",
    "                if not in_market:\n",
    "                    # Verificar si el régimen ha cambiado a favorable\n",
    "                    if date in self.regimes.index and self.regimes.loc[date, 'regime'] == 0:\n",
    "                        # Régimen favorable (baja volatilidad), volver al mercado\n",
    "                        print(f\"Volviendo al mercado en {date}. Régimen favorable detectado.\")\n",
    "                        in_market = True\n",
    "                \n",
    "                if in_market:\n",
    "                    # Actualizar creencias sobre factores\n",
    "                    self.update_beliefs(date)\n",
    "                    \n",
    "                    # Construir portafolio\n",
    "                    weights = self.construct_portfolio(date)\n",
    "                    \n",
    "                    # Guardar pesos\n",
    "                    self.portfolio_weights_history.append((date, weights))\n",
    "                    \n",
    "                    # Calcular retorno\n",
    "                    portfolio_return = self.calculate_portfolio_return(weights, date)\n",
    "                    \n",
    "                    # Actualizar pesos previos\n",
    "                    self.previous_weights = weights\n",
    "                else:\n",
    "                    # Estamos en efectivo\n",
    "                    weights = pd.Series(0, index=self.previous_weights.index if self.previous_weights is not None else [])\n",
    "                    self.portfolio_weights_history.append((date, weights))\n",
    "                    portfolio_return = 0.0  # Considerar tasa libre de riesgo en una implementación real\n",
    "                    self.previous_weights = weights\n",
    "                \n",
    "                # Guardar resultados\n",
    "                dates.append(date)\n",
    "                portfolio_returns.append(portfolio_return)\n",
    "                \n",
    "                # Actualizar equity\n",
    "                cumulative_return *= (1 + portfolio_return)\n",
    "                max_cumulative = max(max_cumulative, cumulative_return)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error en fecha {date}: {str(e)}\")\n",
    "                import traceback\n",
    "                logging.error(traceback.format_exc())\n",
    "        \n",
    "        # Crear series de retornos\n",
    "        self.portfolio_returns = pd.Series(portfolio_returns, index=dates)\n",
    "        \n",
    "        # Calcular equity curve\n",
    "        self.equity_curve = (1 + self.portfolio_returns).cumprod()\n",
    "        \n",
    "        # Calcular métricas\n",
    "        self.calculate_performance_metrics()\n",
    "        \n",
    "        # Guardar resultados\n",
    "        self.save_results()\n",
    "        \n",
    "        # Visualizar resultados\n",
    "        self.plot_results()\n",
    "    \n",
    "    def calculate_performance_metrics(self):\n",
    "        \"\"\"\n",
    "        Calcula métricas de rendimiento de la estrategia.\n",
    "        \"\"\"\n",
    "        returns = self.portfolio_returns\n",
    "        \n",
    "        # Obtener retornos del mercado para el mismo período\n",
    "        market_returns = self.market_data.loc[returns.index, 'returns']\n",
    "        \n",
    "        # Equity curves\n",
    "        equity_curve = (1 + returns).cumprod()\n",
    "        market_equity = (1 + market_returns).cumprod()\n",
    "        \n",
    "        # Retorno anualizado\n",
    "        n_years = len(returns) / 252  # Aproximadamente 252 días de trading por año\n",
    "        total_return = equity_curve.iloc[-1] - 1\n",
    "        annual_return = (1 + total_return) ** (1 / n_years) - 1\n",
    "        \n",
    "        market_total_return = market_equity.iloc[-1] - 1\n",
    "        market_annual_return = (1 + market_total_return) ** (1 / n_years) - 1\n",
    "        \n",
    "        # Volatilidad anualizada\n",
    "        volatility = returns.std() * np.sqrt(252)\n",
    "        market_volatility = market_returns.std() * np.sqrt(252)\n",
    "        \n",
    "        # Sharpe Ratio\n",
    "        risk_free_rate = 0.02  # Tasa libre de riesgo (2%)\n",
    "        sharpe_ratio = (annual_return - risk_free_rate) / volatility\n",
    "        market_sharpe = (market_annual_return - risk_free_rate) / market_volatility\n",
    "        \n",
    "        # Máximo drawdown\n",
    "        cum_returns = equity_curve\n",
    "        running_max = cum_returns.cummax()\n",
    "        drawdown = (cum_returns / running_max) - 1\n",
    "        max_drawdown = drawdown.min()\n",
    "        \n",
    "        market_cum_returns = market_equity\n",
    "        market_running_max = market_cum_returns.cummax()\n",
    "        market_drawdown = (market_cum_returns / market_running_max) - 1\n",
    "        market_max_drawdown = market_drawdown.min()\n",
    "        \n",
    "        # Calmar Ratio (retorno anualizado / máximo drawdown absoluto)\n",
    "        calmar_ratio = annual_return / abs(max_drawdown) if max_drawdown != 0 else float('inf')\n",
    "        market_calmar = market_annual_return / abs(market_max_drawdown) if market_max_drawdown != 0 else float('inf')\n",
    "        \n",
    "        # Alpha y Beta\n",
    "        # Calcular regression para beta y alpha\n",
    "        X = market_returns.values.reshape(-1, 1)\n",
    "        X = np.concatenate([np.ones_like(X), X], axis=1)  # Añadir columna de unos para intercepto\n",
    "        y = returns.values\n",
    "        \n",
    "        # Eliminar NaNs\n",
    "        mask = ~np.isnan(X).any(axis=1) & ~np.isnan(y)\n",
    "        X_clean = X[mask]\n",
    "        y_clean = y[mask]\n",
    "        \n",
    "        if len(X_clean) > 0 and len(y_clean) > 0:\n",
    "            try:\n",
    "                # Regresión lineal simple\n",
    "                beta_alpha = np.linalg.lstsq(X_clean, y_clean, rcond=None)[0]\n",
    "                alpha, beta = beta_alpha[0], beta_alpha[1]\n",
    "                \n",
    "                # Anualizar alpha\n",
    "                alpha_annualized = alpha * 252\n",
    "            except:\n",
    "                alpha_annualized = 0.0\n",
    "                beta = 1.0\n",
    "        else:\n",
    "            alpha_annualized = 0.0\n",
    "            beta = 1.0\n",
    "        \n",
    "        # Guardar métricas\n",
    "        self.metrics = {\n",
    "            'total_return': total_return,\n",
    "            'annual_return': annual_return,\n",
    "            'volatility': volatility,\n",
    "            'sharpe_ratio': sharpe_ratio,\n",
    "            'max_drawdown': max_drawdown,\n",
    "            'calmar_ratio': calmar_ratio,\n",
    "            'alpha': alpha_annualized,\n",
    "            'beta': beta,\n",
    "            'market_return': market_total_return,\n",
    "            'market_annual_return': market_annual_return,\n",
    "            'market_volatility': market_volatility,\n",
    "            'market_sharpe': market_sharpe,\n",
    "            'market_max_drawdown': market_max_drawdown,\n",
    "            'market_calmar': market_calmar\n",
    "        }\n",
    "    \n",
    "    def save_results(self):\n",
    "        \"\"\"\n",
    "        Guarda los resultados del backtest.\n",
    "        \"\"\"\n",
    "        # Guardar retornos\n",
    "        self.portfolio_returns.to_csv('./artifacts/results/data/portfolio_returns.csv')\n",
    "        \n",
    "        # Guardar equity curve\n",
    "        self.equity_curve.to_csv('./artifacts/results/data/equity_curve.csv')\n",
    "        \n",
    "        # Guardar métricas de rendimiento\n",
    "        pd.Series(self.metrics).to_csv('./artifacts/results/data/performance_metrics.csv')\n",
    "        \n",
    "        # Guardar pesos del portafolio a lo largo del tiempo\n",
    "        weights_df = pd.DataFrame({date.strftime('%Y-%m-%d'): weights for date, weights in self.portfolio_weights_history})\n",
    "        weights_df.to_csv('./artifacts/results/data/portfolio_weights_history.csv')\n",
    "        \n",
    "        # Guardar creencias finales sobre factores\n",
    "        belief_df = pd.DataFrame([\n",
    "            {\n",
    "                'category': category,\n",
    "                'regime': regime,\n",
    "                'alpha': self.belief_distributions[category][regime]['alpha'],\n",
    "                'beta': self.belief_distributions[category][regime]['beta'],\n",
    "                'expected_value': self.belief_distributions[category][regime]['alpha'] / \n",
    "                                (self.belief_distributions[category][regime]['alpha'] + self.belief_distributions[category][regime]['beta'])\n",
    "            }\n",
    "            for category in self.factor_categories\n",
    "            for regime in range(self.n_regimes)\n",
    "        ])\n",
    "        belief_df.to_csv('./artifacts/results/data/factor_beliefs.csv', index=False)\n",
    "    \n",
    "    def plot_results(self):\n",
    "        \"\"\"\n",
    "        Genera visualizaciones de los resultados del backtest.\n",
    "        \"\"\"\n",
    "        # Gráfico de rendimiento\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.plot(self.equity_curve, label='AMAR Strategy')\n",
    "        \n",
    "        # Añadir benchmark de mercado\n",
    "        market_equity = (1 + self.market_data.loc[self.portfolio_returns.index, 'returns']).cumprod()\n",
    "        plt.plot(market_equity, label='Market Benchmark', alpha=0.7)\n",
    "        \n",
    "        plt.title('Performance Comparison')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Cumulative Return')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig('./artifacts/results/figures/performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Gráfico de drawdown\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        \n",
    "        # Calcular drawdown\n",
    "        cum_returns = self.equity_curve\n",
    "        running_max = cum_returns.cummax()\n",
    "        drawdown = (cum_returns / running_max) - 1\n",
    "        \n",
    "        market_cum_returns = market_equity\n",
    "        market_running_max = market_cum_returns.cummax()\n",
    "        market_drawdown = (market_cum_returns / market_running_max) - 1\n",
    "        \n",
    "        plt.plot(drawdown, label='AMAR Strategy', color='blue')\n",
    "        plt.plot(market_drawdown, label='Market Benchmark', color='red', alpha=0.7)\n",
    "        \n",
    "        plt.title('Drawdown Comparison')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Drawdown')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig('./artifacts/results/figures/drawdown_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Histograma de retornos\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        plt.hist(self.portfolio_returns, bins=50, alpha=0.5, label='AMAR Strategy')\n",
    "        plt.hist(self.market_data.loc[self.portfolio_returns.index, 'returns'], bins=50, alpha=0.5, label='Market Benchmark')\n",
    "        \n",
    "        plt.title('Return Distribution')\n",
    "        plt.xlabel('Daily Return')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig('./artifacts/results/figures/return_distribution.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Gráfico de pesos por categoría de factores\n",
    "        self.plot_factor_category_weights()\n",
    "        \n",
    "        # Gráfico de creencias sobre factores\n",
    "        self.plot_factor_beliefs()\n",
    "    \n",
    "    def plot_factor_category_weights(self):\n",
    "        \"\"\"\n",
    "        Visualiza la evolución de pesos por categoría de factores.\n",
    "        \"\"\"\n",
    "        # Extraer fechas y pesos por categoría\n",
    "        dates = [date for date, _ in self.portfolio_weights_history]\n",
    "        \n",
    "        # Si tenemos suficientes fechas, crear visualización\n",
    "        if len(dates) > 5:\n",
    "            # Para simplificar, mostraremos la evolución de pesos para categorías en últimos 20 rebalanceos\n",
    "            n_samples = min(20, len(dates))\n",
    "            sample_dates = dates[-n_samples:]\n",
    "            \n",
    "            # Crear diccionario para almacenar pesos por categoría\n",
    "            category_weights = {category: [] for category in self.factor_categories}\n",
    "            \n",
    "            # Para cada fecha, obtener pesos por categoría (según régimen)\n",
    "            for date in sample_dates:\n",
    "                # Obtener régimen para esta fecha\n",
    "                if self.regimes is not None and date in self.regimes.index:\n",
    "                    regime = self.regimes.loc[date, 'regime']\n",
    "                    \n",
    "                    # Obtener pesos usando Thompson Sampling\n",
    "                    weights = self.thompson_sampling(regime)\n",
    "                    \n",
    "                    # Guardar pesos\n",
    "                    for category in self.factor_categories:\n",
    "                        category_weights[category].append(weights.get(category, 0))\n",
    "            \n",
    "            # Crear dataframe\n",
    "            weights_df = pd.DataFrame(category_weights, index=sample_dates)\n",
    "            \n",
    "            # Graficar\n",
    "            plt.figure(figsize=(14, 7))\n",
    "            weights_df.plot(kind='bar', stacked=True, ax=plt.gca(), colormap='viridis')\n",
    "            \n",
    "            plt.title('Factor Category Weights Evolution')\n",
    "            plt.xlabel('Date')\n",
    "            plt.ylabel('Weight')\n",
    "            plt.legend(title='Factor Category')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('./artifacts/results/figures/factor_category_weights.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "    \n",
    "    def plot_factor_beliefs(self):\n",
    "        \"\"\"\n",
    "        Visualiza las creencias finales sobre factores en cada régimen.\n",
    "        \"\"\"\n",
    "        # Crear dataframe para visualización\n",
    "        belief_data = []\n",
    "        \n",
    "        for category in self.factor_categories:\n",
    "            for regime in range(self.n_regimes):\n",
    "                alpha = self.belief_distributions[category][regime]['alpha']\n",
    "                beta = self.belief_distributions[category][regime]['beta']\n",
    "                \n",
    "                # Calcular valor esperado de la distribución Beta\n",
    "                expected_value = alpha / (alpha + beta)\n",
    "                \n",
    "                belief_data.append({\n",
    "                    'Category': category,\n",
    "                    'Regime': f'Regime {regime}',\n",
    "                    'Expected Performance': expected_value\n",
    "                })\n",
    "        \n",
    "        belief_df = pd.DataFrame(belief_data)\n",
    "        \n",
    "        # Convertir a formato wide para heatmap\n",
    "        belief_matrix = belief_df.pivot(index='Category', columns='Regime', values='Expected Performance')\n",
    "        \n",
    "        # Crear heatmap\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        ax = sns.heatmap(belief_matrix, annot=True, cmap='RdYlGn', fmt='.3f', linewidths=.5, cbar_kws={'label': 'Expected Performance'})\n",
    "        \n",
    "        plt.title('Factor Performance Beliefs by Market Regime')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('./artifacts/results/figures/factor_beliefs_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Gráfico adicional: distribuciones Beta para cada combinación\n",
    "        rows, cols = 3, len(self.factor_categories)\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(15, 10))\n",
    "        \n",
    "        for i, category in enumerate(self.factor_categories):\n",
    "            for j in range(self.n_regimes):\n",
    "                alpha = self.belief_distributions[category][j]['alpha']\n",
    "                beta = self.belief_distributions[category][j]['beta']\n",
    "                \n",
    "                # Generar puntos para distribución Beta\n",
    "                x = np.linspace(0, 1, 100)\n",
    "                y = stats.beta.pdf(x, alpha, beta)\n",
    "                \n",
    "                # Graficar\n",
    "                axes[j, i].plot(x, y)\n",
    "                axes[j, i].set_title(f'{category} - Regime {j}')\n",
    "                axes[j, i].set_xlim(0, 1)\n",
    "                axes[j, i].set_ylim(0, max(y) * 1.1)\n",
    "                \n",
    "                # Añadir valor esperado\n",
    "                expected = alpha / (alpha + beta)\n",
    "                axes[j, i].axvline(expected, color='red', linestyle='--', alpha=0.6)\n",
    "                axes[j, i].text(expected + 0.02, max(y) * 0.8, f'E={expected:.3f}', color='red')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('./artifacts/results/figures/factor_belief_distributions.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "    def run_strategy(self):\n",
    "        \"\"\"\n",
    "        Ejecuta la estrategia completa: descarga datos, identifica regímenes,\n",
    "        realiza backtest y walk-forward testing.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 1. Descargar datos\n",
    "            self.download_data()\n",
    "            \n",
    "            # 2. Identificar regímenes de mercado\n",
    "            self.identify_regimes()\n",
    "            \n",
    "            # 3. Ejecutar backtest\n",
    "            self.run_backtest()\n",
    "                       \n",
    "            # 5. Imprimir resumen de resultados\n",
    "            self.print_summary()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error ejecutando estrategia: {str(e)}\")\n",
    "            import traceback\n",
    "            logging.error(traceback.format_exc())\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"\n",
    "        Imprime un resumen de los resultados de la estrategia.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'metrics') or not self.metrics:\n",
    "            print(\"No hay métricas disponibles. Asegúrate de ejecutar el backtest primero.\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"RESUMEN DE RESULTADOS - ESTRATEGIA AMAR (OPTIMIZADA)\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(f\"\\nPeríodo de Backtest: {self.start_date} a {self.end_date}\")\n",
    "        print(f\"Retorno Total: {self.metrics['total_return']:.2%}\")\n",
    "        print(f\"Retorno Anualizado: {self.metrics['annual_return']:.2%}\")\n",
    "        print(f\"Volatilidad Anualizada: {self.metrics['volatility']:.2%}\")\n",
    "        print(f\"Sharpe Ratio: {self.metrics['sharpe_ratio']:.2f}\")\n",
    "        print(f\"Máximo Drawdown: {self.metrics['max_drawdown']:.2%}\")\n",
    "        print(f\"Calmar Ratio: {self.metrics['calmar_ratio']:.2f}\")\n",
    "        print(f\"Alpha Anualizado: {self.metrics['alpha']:.2%}\")\n",
    "        print(f\"Beta: {self.metrics['beta']:.2f}\")\n",
    "        \n",
    "        print(\"\\nBenchmark (Mercado):\")\n",
    "        print(f\"Retorno Total: {self.metrics['market_return']:.2%}\")\n",
    "        print(f\"Retorno Anualizado: {self.metrics['market_annual_return']:.2%}\")\n",
    "        print(f\"Volatilidad Anualizada: {self.metrics['market_volatility']:.2%}\")\n",
    "        print(f\"Sharpe Ratio: {self.metrics['market_sharpe']:.2f}\")\n",
    "        print(f\"Máximo Drawdown: {self.metrics['market_max_drawdown']:.2%}\")\n",
    "        \n",
    "        print(\"\\nResultados guardados en: ./artifacts/results/\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "\n",
    "# Función principal para ejecutar la estrategia\n",
    "def run_amar_strategy(start_date='2015-01-01', end_date='2022-12-31'):\n",
    "    \"\"\"\n",
    "    Ejecuta la estrategia AMAR mejorada con los parámetros optimizados.\n",
    "    \n",
    "    Args:\n",
    "        start_date: Fecha de inicio del backtest\n",
    "        end_date: Fecha de fin del backtest\n",
    "    \"\"\"\n",
    "    # Crear instancia de la estrategia mejorada\n",
    "    strategy = AMAR(\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        lookback_years=1,\n",
    "        regime_update_frequency='M',\n",
    "        portfolio_rebalance_frequency='W-FRI',\n",
    "        n_regimes=3,\n",
    "        market_index='SPY',\n",
    "        target_beta=0.,          # Estrategia neutral al mercado\n",
    "        beta_range=0.1,\n",
    "        max_stock_weight=0.05,    # Permitir más concentración en mejores señales\n",
    "        max_sector_deviation=0.05,\n",
    "        max_turnover=0.2,         # Reducir costos de transacción\n",
    "        volatility_target=0.1,   # Controlar mejor el riesgo\n",
    "        min_liquidity=5e6,        # Mayor umbral de liquidez\n",
    "        transaction_cost=0.0015,  # Más realista (0.15%)\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Ejecutar estrategia\n",
    "    strategy.run_strategy()\n",
    "    \n",
    "    return strategy\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ejecutar estrategia mejorada\n",
    "    strategy = run_amar_strategy(start_date='2024-01-01', end_date='2025-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6efdba0-0acb-4d7d-ba5b-009e4dadac76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abd636e9-8df0-4fd7-acc3-3fc6a7c23796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "976c73c4-bee7-41d3-a5b0-236efc8c52a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando datos para 503 activos...\n",
      "Datos procesados para 500 activos válidos.\n",
      "Identificando regímenes de mercado...\n",
      "Ejecutando backtest...\n",
      "\n",
      "==================================================\n",
      "RESUMEN DE RESULTADOS - ESTRATEGIA MEJORADA\n",
      "==================================================\n",
      "\n",
      "Período: 2020-01-01 a 2025-01-01\n",
      "Retorno Total: 4.12%\n",
      "Retorno Anualizado: 4.12%\n",
      "Volatilidad: 9.90%\n",
      "Sharpe Ratio: 0.42\n",
      "Máximo Drawdown: -11.20%\n",
      "Beta: -0.01\n",
      "Alpha: 4.38%\n",
      "\n",
      "S&P 500:\n",
      "Retorno Total: 19.51%\n",
      "Retorno Anualizado: 19.51%\n",
      "\n",
      "Resultados guardados en: ./results/\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Configuración básica\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Directorios básicos\n",
    "os.makedirs('./results', exist_ok=True)\n",
    "\n",
    "class EnhancedAMAR:\n",
    "    \"\"\"\n",
    "    Implementación simplificada y mejorada de la estrategia AMAR\n",
    "    con enfoque en rendimiento superior al mercado.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 start_date: str = '2015-01-01',\n",
    "                 end_date: str = None,\n",
    "                 lookback_years: int = 2,\n",
    "                 portfolio_rebalance_frequency: str = 'W-FRI',\n",
    "                 n_regimes: int = 2,  # Simplificado a 2 regímenes\n",
    "                 market_index: str = 'SPY',\n",
    "                 target_beta: float = 0.7,  # Cambiado a exposición controlada al mercado\n",
    "                 max_stock_weight: float = 0.07,  # Permitir mayor concentración en mejores oportunidades\n",
    "                 volatility_target: float = 0.15,  # Permitir mayor volatilidad para mayor rendimiento\n",
    "                 transaction_cost: float = 0.001,\n",
    "                 random_state: int = 42):\n",
    "        \"\"\"\n",
    "        Inicializa la estrategia mejorada con parámetros optimizados.\n",
    "        \"\"\"\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date if end_date else datetime.now().strftime('%Y-%m-%d')\n",
    "        self.lookback_years = lookback_years\n",
    "        self.portfolio_rebalance_frequency = portfolio_rebalance_frequency\n",
    "        self.n_regimes = n_regimes\n",
    "        self.market_index = market_index\n",
    "        self.target_beta = target_beta\n",
    "        self.max_stock_weight = max_stock_weight\n",
    "        self.volatility_target = volatility_target\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Atributos internos simplificados\n",
    "        self.data = None\n",
    "        self.sp500_stocks = None\n",
    "        self.market_data = None\n",
    "        self.regimes = None\n",
    "        self.portfolio_weights = pd.DataFrame()\n",
    "        self.portfolio_returns = pd.Series(dtype='float64')\n",
    "        self.current_regime = None\n",
    "        \n",
    "        # Simplificación de factores - Enfoque en los más efectivos\n",
    "        self.factor_categories = ['Value', 'Momentum', 'Quality']\n",
    "        \n",
    "        # Métricas de rendimiento\n",
    "        self.metrics = {}\n",
    "        \n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    def get_sp500_tickers(self) -> List[str]:\n",
    "        \"\"\"Obtiene tickers del S&P 500 desde Wikipedia.\"\"\"\n",
    "        try:\n",
    "            table = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "            df = table[0]\n",
    "            tickers = df['Symbol'].str.replace('.', '-').tolist()\n",
    "            return tickers\n",
    "        except Exception as e:\n",
    "            print(f\"Error al obtener tickers: {str(e)}\")\n",
    "            # Lista reducida en caso de error\n",
    "            return ['AAPL', 'MSFT', 'AMZN', 'GOOGL', 'META', 'TSLA', 'BRK-B', 'JPM', 'JNJ', 'V', 'PG', 'UNH', 'HD']\n",
    "    \n",
    "    def download_data(self):\n",
    "        \"\"\"Descarga datos históricos con enfoque simplificado.\"\"\"\n",
    "        try:\n",
    "            # Calcular fecha de inicio para datos históricos\n",
    "            extended_start = (datetime.strptime(self.start_date, '%Y-%m-%d') - \n",
    "                             timedelta(days=int(365.25 * self.lookback_years))).strftime('%Y-%m-%d')\n",
    "            \n",
    "            # Obtener tickers del S&P 500\n",
    "            self.sp500_stocks = self.get_sp500_tickers()\n",
    "            print(f\"Descargando datos para {len(self.sp500_stocks)} activos...\")\n",
    "            \n",
    "            # Incluir índice de mercado\n",
    "            tickers = self.sp500_stocks + [self.market_index]\n",
    "            \n",
    "            # Descargar datos\n",
    "            self.data = yf.download(\n",
    "                tickers, \n",
    "                start=extended_start, \n",
    "                end=self.end_date,\n",
    "                group_by='ticker',\n",
    "                progress=False\n",
    "            )\n",
    "            \n",
    "            # Procesar datos del mercado\n",
    "            self.market_data = pd.DataFrame({\n",
    "                'close': self.data[self.market_index]['Close'],\n",
    "                'returns': self.data[self.market_index]['Close'].pct_change()\n",
    "            })\n",
    "            \n",
    "            # Procesar datos de acciones\n",
    "            stocks_data = {}\n",
    "            for ticker in self.sp500_stocks:\n",
    "                try:\n",
    "                    if ticker in self.data.columns.levels[0]:\n",
    "                        stock_data = pd.DataFrame({\n",
    "                            'close': self.data[ticker]['Close'],\n",
    "                            'volume': self.data[ticker]['Volume'],\n",
    "                        })\n",
    "                        stock_data['returns'] = stock_data['close'].pct_change()\n",
    "                        \n",
    "                        # Solo mantener acciones con suficientes datos\n",
    "                        if stock_data['close'].dropna().shape[0] > 252:\n",
    "                            stocks_data[ticker] = stock_data\n",
    "                except Exception as e:\n",
    "                    print(f\"Error procesando {ticker}: {str(e)}\")\n",
    "            \n",
    "            self.stock_data = stocks_data\n",
    "            print(f\"Datos procesados para {len(stocks_data)} activos válidos.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error descargando datos: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def calculate_regime_features(self) -> pd.DataFrame:\n",
    "        \"\"\"Calcula características simplificadas para identificación de regímenes.\"\"\"\n",
    "        market = self.market_data.copy()\n",
    "        \n",
    "        # Características simplificadas y efectivas\n",
    "        features = pd.DataFrame(index=market.index)\n",
    "        \n",
    "        # 1. Tendencia de precios\n",
    "        market['sma50'] = market['close'].rolling(50).mean()\n",
    "        market['sma200'] = market['close'].rolling(200).mean()\n",
    "        \n",
    "        features['trend_indicator'] = market['sma50'] / market['sma200'] - 1\n",
    "        \n",
    "        # 2. Volatilidad\n",
    "        features['volatility'] = market['returns'].rolling(21).std() * np.sqrt(252)\n",
    "        \n",
    "        # 3. Momentum\n",
    "        features['momentum'] = market['close'].pct_change(63)  # 3 meses\n",
    "        \n",
    "        # Imputar valores faltantes\n",
    "        features = features.fillna(method='bfill').fillna(features.mean())\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def identify_regimes(self):\n",
    "        \"\"\"Identifica regímenes de mercado con enfoque simplificado.\"\"\"\n",
    "        print(\"Identificando regímenes de mercado...\")\n",
    "        \n",
    "        # Calcular características\n",
    "        features = self.calculate_regime_features()\n",
    "        \n",
    "        # Usar solo datos a partir de start_date\n",
    "        training_data = features[features.index >= self.start_date].copy()\n",
    "        \n",
    "        if training_data.shape[0] > 30:\n",
    "            # Normalizar características\n",
    "            scaler = StandardScaler()\n",
    "            normalized_data = scaler.fit_transform(training_data)\n",
    "            \n",
    "            # Ajustar GMM simplificado\n",
    "            gmm = GaussianMixture(\n",
    "                n_components=self.n_regimes,\n",
    "                covariance_type='full',\n",
    "                random_state=self.random_state,\n",
    "                n_init=10\n",
    "            )\n",
    "            \n",
    "            gmm.fit(normalized_data)\n",
    "            labels = gmm.predict(normalized_data)\n",
    "            \n",
    "            # Ordenar regímenes por volatilidad (0: baja, 1: alta)\n",
    "            regime_volatility = {}\n",
    "            for i in range(self.n_regimes):\n",
    "                mask = (labels == i)\n",
    "                if mask.sum() > 0:\n",
    "                    regime_volatility[i] = training_data.loc[mask, 'volatility'].mean()\n",
    "            \n",
    "            sorted_regimes = sorted(regime_volatility.items(), key=lambda x: x[1])\n",
    "            regime_map = {old: new for new, (old, _) in enumerate(sorted_regimes)}\n",
    "            \n",
    "            # Reasignar etiquetas\n",
    "            new_labels = np.array([regime_map[label] for label in labels])\n",
    "            \n",
    "            # Crear DataFrame de regímenes\n",
    "            self.regimes = pd.DataFrame(index=training_data.index, columns=['regime'])\n",
    "            self.regimes['regime'] = new_labels\n",
    "            \n",
    "            # Simple plot para verificar regímenes\n",
    "            self._plot_simple_regimes()\n",
    "        else:\n",
    "            print(\"Datos insuficientes para identificar regímenes.\")\n",
    "    \n",
    "    def _plot_simple_regimes(self):\n",
    "        \"\"\"Visualización simple de regímenes.\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        # Graficar precio de mercado\n",
    "        ax.plot(self.market_data.loc[self.regimes.index, 'close'], 'k-', label='Mercado')\n",
    "        \n",
    "        # Colorear regímenes\n",
    "        colors = ['green', 'red']  # 0: baja volatilidad, 1: alta volatilidad\n",
    "        labels = ['Baja Volatilidad', 'Alta Volatilidad']\n",
    "        \n",
    "        # Agrupar regímenes consecutivos\n",
    "        for regime in range(self.n_regimes):\n",
    "            regime_periods = self.regimes[self.regimes['regime'] == regime].index\n",
    "            if len(regime_periods) > 0:\n",
    "                ax.fill_between(regime_periods, 0, 1, \n",
    "                               transform=ax.get_xaxis_transform(),\n",
    "                               alpha=0.3, color=colors[regime], label=labels[regime])\n",
    "        \n",
    "        ax.set_title('Regímenes de Mercado')\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('./results/market_regimes.png', dpi=150)\n",
    "        plt.close()\n",
    "    \n",
    "    def calculate_factors(self, date: pd.Timestamp) -> Dict[str, pd.Series]:\n",
    "        \"\"\"Calcula factores simplificados pero efectivos.\"\"\"\n",
    "        # Períodos de lookback\n",
    "        lookback_long = date - pd.Timedelta(days=365)\n",
    "        lookback_short = date - pd.Timedelta(days=63)\n",
    "        \n",
    "        # Filtrar acciones con datos suficientes\n",
    "        valid_stocks = [ticker for ticker, data in self.stock_data.items() \n",
    "                       if data.loc[:date].shape[0] > 252]\n",
    "        \n",
    "        factors = {}\n",
    "        \n",
    "        # 1. VALOR (Price to máximo histórico como proxy)\n",
    "        pb_proxy = {}\n",
    "        for ticker in valid_stocks:\n",
    "            data = self.stock_data[ticker].loc[lookback_long:date]\n",
    "            if data.shape[0] > 0:\n",
    "                current_price = data['close'].iloc[-1]\n",
    "                historical_high = data['close'].max()\n",
    "                if historical_high > 0:\n",
    "                    pb_proxy[ticker] = current_price / historical_high\n",
    "        factors['Value'] = -pd.Series(pb_proxy)  # Invertido para que valores más altos sean mejores\n",
    "        \n",
    "        # 2. MOMENTUM (3, 6, 12 meses con ponderación)\n",
    "        weighted_momentum = {}\n",
    "        periods = [(63, 0.4), (126, 0.3), (252, 0.3)]  # períodos y pesos\n",
    "        \n",
    "        for ticker in valid_stocks:\n",
    "            data = self.stock_data[ticker].loc[:date]\n",
    "            if data.shape[0] > 252:\n",
    "                momentum_sum = 0.0\n",
    "                weight_sum = 0.0\n",
    "                \n",
    "                for days, weight in periods:\n",
    "                    if data.shape[0] >= days:\n",
    "                        ret = data['close'].iloc[-1] / data['close'].iloc[-min(days, len(data))] - 1\n",
    "                        momentum_sum += ret * weight\n",
    "                        weight_sum += weight\n",
    "                \n",
    "                if weight_sum > 0:\n",
    "                    weighted_momentum[ticker] = momentum_sum / weight_sum\n",
    "                    \n",
    "        factors['Momentum'] = pd.Series(weighted_momentum)\n",
    "        \n",
    "        # 3. CALIDAD (Estabilidad de retornos - menor drawdown)\n",
    "        quality = {}\n",
    "        for ticker in valid_stocks:\n",
    "            data = self.stock_data[ticker].loc[lookback_long:date]\n",
    "            if data.shape[0] > 126:\n",
    "                # Calidad basada en resistencia a caídas\n",
    "                cum_returns = (1 + data['returns']).cumprod()\n",
    "                if len(cum_returns) > 0:\n",
    "                    max_dd = (cum_returns / cum_returns.cummax() - 1).min()\n",
    "                    quality[ticker] = -max_dd  # Invertir para que valores más altos sean mejores\n",
    "                    \n",
    "        factors['Quality'] = pd.Series(quality)\n",
    "        \n",
    "        # Normalizar y controlar outliers para todos los factores\n",
    "        normalized_factors = {}\n",
    "        for factor_name, factor_values in factors.items():\n",
    "            if len(factor_values) > 0:\n",
    "                # Winsorización para controlar outliers\n",
    "                lower_bound = factor_values.quantile(0.02)\n",
    "                upper_bound = factor_values.quantile(0.98)\n",
    "                winsorized = factor_values.clip(lower=lower_bound, upper=upper_bound)\n",
    "                \n",
    "                # Z-score normalization\n",
    "                mean = winsorized.mean()\n",
    "                std = winsorized.std()\n",
    "                if std > 0:\n",
    "                    normalized = (winsorized - mean) / std\n",
    "                    normalized_factors[factor_name] = normalized\n",
    "        \n",
    "        return normalized_factors\n",
    "    \n",
    "    def get_factor_weights(self, date: pd.Timestamp) -> Dict[str, float]:\n",
    "        \"\"\"Determina pesos dinámicos para factores según régimen de mercado.\"\"\"\n",
    "        # Si no hay regímenes identificados, usar pesos iguales\n",
    "        if self.regimes is None or date not in self.regimes.index:\n",
    "            return {factor: 1.0/len(self.factor_categories) for factor in self.factor_categories}\n",
    "        \n",
    "        # Obtener régimen actual\n",
    "        regime = self.regimes.loc[date, 'regime']\n",
    "        \n",
    "        # Asignar pesos según régimen - optimizado basado en evidencia empírica\n",
    "        if regime == 0:  # Baja volatilidad / alcista\n",
    "            return {\n",
    "                'Value': 0.2,      # Valor menos importante en mercados alcistas\n",
    "                'Momentum': 0.55,  # Momentum muy efectivo en tendencias claras\n",
    "                'Quality': 0.25    # Calidad siempre importante\n",
    "            }\n",
    "        else:  # Alta volatilidad / bajista\n",
    "            return {\n",
    "                'Value': 0.35,     # Valor más importante en mercados volátiles\n",
    "                'Momentum': 0.15,  # Momentum menos fiable en alta volatilidad\n",
    "                'Quality': 0.5     # Calidad crucial en mercados estresados\n",
    "            }\n",
    "    \n",
    "    def construct_portfolio(self, date: pd.Timestamp) -> pd.Series:\n",
    "        \"\"\"Construye portafolio con enfoque simplificado pero efectivo.\"\"\"\n",
    "        # Calcular factores\n",
    "        factors = self.calculate_factors(date)\n",
    "        \n",
    "        if not factors:\n",
    "            return pd.Series(dtype='float64')\n",
    "        \n",
    "        # Obtener pesos de factores según régimen\n",
    "        factor_weights = self.get_factor_weights(date)\n",
    "        \n",
    "        # Combinar factores con pesos\n",
    "        combined_score = pd.Series(0, index=next(iter(factors.values())).index)\n",
    "        \n",
    "        for factor, weight in factor_weights.items():\n",
    "            if factor in factors:\n",
    "                combined_score += factors[factor] * weight\n",
    "        \n",
    "        # Seleccionar mejores acciones (top 15%)\n",
    "        num_stocks = max(20, int(0.15 * len(combined_score)))\n",
    "        top_stocks = combined_score.nlargest(num_stocks).index\n",
    "        \n",
    "        # Asignar pesos basados en ranking\n",
    "        weights = pd.Series(0, index=combined_score.index)\n",
    "        \n",
    "        # Pesos proporcionales al score\n",
    "        for i, stock in enumerate(top_stocks):\n",
    "            weights[stock] = combined_score[stock]\n",
    "        \n",
    "        # Normalizar y aplicar peso máximo\n",
    "        if weights.sum() > 0:\n",
    "            weights = weights / weights.sum()\n",
    "            weights = weights.clip(upper=self.max_stock_weight)\n",
    "            weights = weights / weights.sum()\n",
    "        \n",
    "        # Aplicar controles de riesgo\n",
    "        weights = self._apply_risk_controls(weights, date)\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def _apply_risk_controls(self, weights: pd.Series, date: pd.Timestamp) -> pd.Series:\n",
    "        \"\"\"Aplica controles de riesgo simplificados y efectivos.\"\"\"\n",
    "        # Si no hay pesos, retornar serie vacía\n",
    "        if weights.sum() == 0:\n",
    "            return weights\n",
    "        \n",
    "        # 1. Control de Beta\n",
    "        try:\n",
    "            # Calcular betas individuales\n",
    "            betas = {}\n",
    "            lookback_start = date - pd.Timedelta(days=252)\n",
    "            \n",
    "            for ticker in weights.index[weights > 0]:\n",
    "                if ticker in self.stock_data:\n",
    "                    stock_data = self.stock_data[ticker].loc[lookback_start:date]\n",
    "                    market_slice = self.market_data.loc[lookback_start:date]\n",
    "                    \n",
    "                    common_dates = stock_data.index.intersection(market_slice.index)\n",
    "                    if len(common_dates) > 60:\n",
    "                        stock_returns = stock_data.loc[common_dates, 'returns']\n",
    "                        market_returns = market_slice.loc[common_dates, 'returns']\n",
    "                        \n",
    "                        # Eliminar NaN\n",
    "                        valid_mask = ~(np.isnan(stock_returns) | np.isnan(market_returns))\n",
    "                        stock_returns = stock_returns[valid_mask]\n",
    "                        market_returns = market_returns[valid_mask]\n",
    "                        \n",
    "                        if len(stock_returns) > 60:\n",
    "                            # Calcular beta simple\n",
    "                            cov = np.cov(stock_returns, market_returns)[0, 1]\n",
    "                            var = np.var(market_returns)\n",
    "                            if var > 0:\n",
    "                                betas[ticker] = cov / var\n",
    "            \n",
    "            # Ajustar portafolio si tenemos suficientes betas\n",
    "            if len(betas) > 10:\n",
    "                beta_series = pd.Series(betas)\n",
    "                portfolio_beta = (weights.loc[beta_series.index] * beta_series).sum()\n",
    "                \n",
    "                # Solo ajustar si estamos muy lejos del objetivo\n",
    "                if abs(portfolio_beta - self.target_beta) > 0.2:\n",
    "                    # Ordenar acciones por beta\n",
    "                    sorted_betas = beta_series.sort_values()\n",
    "                    \n",
    "                    if portfolio_beta > self.target_beta + 0.2:\n",
    "                        # Reducir exposición a acciones de alto beta\n",
    "                        high_beta = sorted_betas.tail(int(len(sorted_betas) * 0.3))\n",
    "                        for ticker in high_beta.index:\n",
    "                            weights[ticker] *= 0.7\n",
    "                    elif portfolio_beta < self.target_beta - 0.2:\n",
    "                        # Aumentar exposición a acciones de alto beta\n",
    "                        high_beta = sorted_betas.tail(int(len(sorted_betas) * 0.3))\n",
    "                        for ticker in high_beta.index:\n",
    "                            weights[ticker] *= 1.3\n",
    "                    \n",
    "                    # Renormalizar\n",
    "                    weights = weights / weights.sum()\n",
    "        except Exception as e:\n",
    "            print(f\"Error en control de beta: {str(e)}\")\n",
    "        \n",
    "        # 2. Control de Volatilidad (simplificado)\n",
    "        try:\n",
    "            # Estimar volatilidad del portafolio\n",
    "            stock_vol = {}\n",
    "            for ticker in weights.index[weights > 0]:\n",
    "                if ticker in self.stock_data:\n",
    "                    returns = self.stock_data[ticker].loc[date-pd.Timedelta(days=63):date, 'returns']\n",
    "                    if len(returns) > 30:\n",
    "                        stock_vol[ticker] = returns.std() * np.sqrt(252)\n",
    "            \n",
    "            if stock_vol:\n",
    "                # Estimación simple de volatilidad de portafolio\n",
    "                vol_series = pd.Series(stock_vol)\n",
    "                avg_vol = (weights.loc[vol_series.index] * vol_series).sum()\n",
    "                \n",
    "                # Escalar si volatilidad estimada es demasiado alta\n",
    "                if avg_vol > self.volatility_target * 1.5:\n",
    "                    scale = self.volatility_target / avg_vol\n",
    "                    weights *= scale\n",
    "        except Exception as e:\n",
    "            print(f\"Error en control de volatilidad: {str(e)}\")\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def run_backtest(self):\n",
    "        \"\"\"Ejecuta backtest con lógica simplificada.\"\"\"\n",
    "        print(\"Ejecutando backtest...\")\n",
    "        \n",
    "        # Obtener fechas de rebalanceo\n",
    "        rebalance_dates = pd.date_range(\n",
    "            start=self.start_date, \n",
    "            end=self.end_date, \n",
    "            freq=self.portfolio_rebalance_frequency\n",
    "        )\n",
    "        \n",
    "        # Filtrar a fechas disponibles en datos\n",
    "        rebalance_dates = [date for date in rebalance_dates if date in self.market_data.index]\n",
    "        \n",
    "        # Almacenar resultados\n",
    "        portfolio_weights_history = []\n",
    "        portfolio_returns = []\n",
    "        dates = []\n",
    "        \n",
    "        previous_weights = None\n",
    "        \n",
    "        # Ejecutar para cada fecha de rebalanceo\n",
    "        for date in rebalance_dates:\n",
    "            try:\n",
    "                # Construir portafolio\n",
    "                weights = self.construct_portfolio(date)\n",
    "                \n",
    "                # Guardar pesos\n",
    "                portfolio_weights_history.append((date, weights))\n",
    "                \n",
    "                # Calcular retorno\n",
    "                next_date = self._get_next_trading_date(date)\n",
    "                if next_date:\n",
    "                    portfolio_return = 0.0\n",
    "                    \n",
    "                    for ticker, weight in weights.items():\n",
    "                        if ticker in self.stock_data and next_date in self.stock_data[ticker].index:\n",
    "                            stock_return = self.stock_data[ticker].loc[next_date, 'returns']\n",
    "                            if not np.isnan(stock_return):\n",
    "                                portfolio_return += weight * stock_return\n",
    "                    \n",
    "                    # Aplicar costo de transacción si tenemos pesos previos\n",
    "                    if previous_weights is not None:\n",
    "                        turnover = self._calculate_turnover(previous_weights, weights)\n",
    "                        transaction_cost = turnover * self.transaction_cost\n",
    "                        portfolio_return -= transaction_cost\n",
    "                    \n",
    "                    # Guardar resultados\n",
    "                    dates.append(date)\n",
    "                    portfolio_returns.append(portfolio_return)\n",
    "                \n",
    "                # Actualizar pesos previos\n",
    "                previous_weights = weights\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error en fecha {date}: {str(e)}\")\n",
    "        \n",
    "        # Crear series de retornos\n",
    "        self.portfolio_returns = pd.Series(portfolio_returns, index=dates)\n",
    "        \n",
    "        # Calcular equity curve\n",
    "        self.equity_curve = (1 + self.portfolio_returns).cumprod()\n",
    "        \n",
    "        # Calcular métricas\n",
    "        self.calculate_performance_metrics()\n",
    "        \n",
    "        # Visualizar resultados\n",
    "        self.plot_results()\n",
    "    \n",
    "    def _calculate_turnover(self, old_weights: pd.Series, new_weights: pd.Series) -> float:\n",
    "        \"\"\"Calcula turnover entre conjuntos de pesos.\"\"\"\n",
    "        all_stocks = old_weights.index.union(new_weights.index)\n",
    "        \n",
    "        old_unified = pd.Series(0, index=all_stocks)\n",
    "        new_unified = pd.Series(0, index=all_stocks)\n",
    "        \n",
    "        old_unified.loc[old_weights.index] = old_weights\n",
    "        new_unified.loc[new_weights.index] = new_weights\n",
    "        \n",
    "        turnover = np.sum(np.abs(old_unified - new_unified)) / 2\n",
    "        \n",
    "        return turnover\n",
    "    \n",
    "    def _get_next_trading_date(self, date: pd.Timestamp) -> Optional[pd.Timestamp]:\n",
    "        \"\"\"Obtiene siguiente fecha de trading disponible.\"\"\"\n",
    "        market_dates = self.market_data.index\n",
    "        future_dates = market_dates[market_dates > date]\n",
    "        \n",
    "        if len(future_dates) > 0:\n",
    "            return future_dates[0]\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def calculate_performance_metrics(self):\n",
    "        \"\"\"Calcula métricas de rendimiento simplificadas.\"\"\"\n",
    "        returns = self.portfolio_returns\n",
    "        \n",
    "        # Obtener retornos del mercado para el mismo período\n",
    "        market_returns = self.market_data.loc[returns.index, 'returns']\n",
    "        \n",
    "        # Equity curves\n",
    "        equity_curve = (1 + returns).cumprod()\n",
    "        market_equity = (1 + market_returns).cumprod()\n",
    "        \n",
    "        # Retorno anualizado\n",
    "        n_years = len(returns) / 252\n",
    "        total_return = equity_curve.iloc[-1] - 1\n",
    "        annual_return = (1 + total_return) ** (1 / n_years) - 1\n",
    "        \n",
    "        market_total_return = market_equity.iloc[-1] - 1\n",
    "        market_annual_return = (1 + market_total_return) ** (1 / n_years) - 1\n",
    "        \n",
    "        # Volatilidad anualizada\n",
    "        volatility = returns.std() * np.sqrt(252)\n",
    "        market_volatility = market_returns.std() * np.sqrt(252)\n",
    "        \n",
    "        # Sharpe Ratio (asumiendo tasa libre de riesgo 0 para simplificar)\n",
    "        sharpe_ratio = annual_return / volatility\n",
    "        market_sharpe = market_annual_return / market_volatility\n",
    "        \n",
    "        # Máximo drawdown\n",
    "        running_max = equity_curve.cummax()\n",
    "        drawdown = (equity_curve / running_max) - 1\n",
    "        max_drawdown = drawdown.min()\n",
    "        \n",
    "        market_running_max = market_equity.cummax()\n",
    "        market_drawdown = (market_equity / market_running_max) - 1\n",
    "        market_max_drawdown = market_drawdown.min()\n",
    "        \n",
    "        # Calcular beta\n",
    "        cov = np.cov(returns, market_returns)[0, 1]\n",
    "        var = np.var(market_returns)\n",
    "        if var > 0:\n",
    "            beta = cov / var\n",
    "        else:\n",
    "            beta = 1.0\n",
    "        \n",
    "        # Alpha simple\n",
    "        alpha = annual_return - (beta * market_annual_return)\n",
    "        \n",
    "        # Guardar métricas\n",
    "        self.metrics = {\n",
    "            'total_return': total_return,\n",
    "            'annual_return': annual_return,\n",
    "            'volatility': volatility,\n",
    "            'sharpe_ratio': sharpe_ratio,\n",
    "            'max_drawdown': max_drawdown,\n",
    "            'beta': beta,\n",
    "            'alpha': alpha,\n",
    "            'market_return': market_total_return,\n",
    "            'market_annual': market_annual_return\n",
    "        }\n",
    "    \n",
    "    def plot_results(self):\n",
    "        \"\"\"Genera visualizaciones simplificadas de resultados.\"\"\"\n",
    "        # Único gráfico de rendimiento con métricas\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Graficar equity curves\n",
    "        plt.plot(self.equity_curve, label='Estrategia Mejorada', linewidth=2)\n",
    "        \n",
    "        # Graficar benchmark\n",
    "        market_equity = (1 + self.market_data.loc[self.portfolio_returns.index, 'returns']).cumprod()\n",
    "        plt.plot(market_equity, label='S&P 500', alpha=0.7, linewidth=1.5)\n",
    "        \n",
    "        # Añadir texto con métricas clave\n",
    "        metrics_text = (\n",
    "            f\"Estrategia:\\n\"\n",
    "            f\"Retorno Anual: {self.metrics['annual_return']:.2%}\\n\"\n",
    "            f\"Volatilidad: {self.metrics['volatility']:.2%}\\n\"\n",
    "            f\"Sharpe: {self.metrics['sharpe_ratio']:.2f}\\n\"\n",
    "            f\"Max DD: {self.metrics['max_drawdown']:.2%}\\n\\n\"\n",
    "            f\"S&P 500:\\n\"\n",
    "            f\"Retorno Anual: {self.metrics['market_annual']:.2%}\\n\"\n",
    "            f\"Alpha: {self.metrics['alpha']:.2%}\\n\"\n",
    "            f\"Beta: {self.metrics['beta']:.2f}\"\n",
    "        )\n",
    "        \n",
    "        # Posición del texto: 80% del eje x, 20% del eje y\n",
    "        plt.annotate(metrics_text, xy=(0.02, 0.02), xycoords='axes fraction', \n",
    "                    bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", alpha=0.8))\n",
    "        \n",
    "        plt.title('Rendimiento de la Estrategia Mejorada vs S&P 500')\n",
    "        plt.xlabel('Fecha')\n",
    "        plt.ylabel('Crecimiento de $1')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('./results/performance.png', dpi=200)\n",
    "        plt.close()\n",
    "    \n",
    "    def run_strategy(self):\n",
    "        \"\"\"Ejecuta la estrategia completa.\"\"\"\n",
    "        try:\n",
    "            # 1. Descargar datos\n",
    "            self.download_data()\n",
    "            \n",
    "            # 2. Identificar regímenes\n",
    "            self.identify_regimes()\n",
    "            \n",
    "            # 3. Ejecutar backtest\n",
    "            self.run_backtest()\n",
    "            \n",
    "            # 4. Imprimir resumen\n",
    "            self.print_summary()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error ejecutando estrategia: {str(e)}\")\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Imprime resumen de resultados.\"\"\"\n",
    "        if not self.metrics:\n",
    "            print(\"No hay métricas disponibles.\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"RESUMEN DE RESULTADOS - ESTRATEGIA MEJORADA\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(f\"\\nPeríodo: {self.start_date} a {self.end_date}\")\n",
    "        print(f\"Retorno Total: {self.metrics['total_return']:.2%}\")\n",
    "        print(f\"Retorno Anualizado: {self.metrics['annual_return']:.2%}\")\n",
    "        print(f\"Volatilidad: {self.metrics['volatility']:.2%}\")\n",
    "        print(f\"Sharpe Ratio: {self.metrics['sharpe_ratio']:.2f}\")\n",
    "        print(f\"Máximo Drawdown: {self.metrics['max_drawdown']:.2%}\")\n",
    "        print(f\"Beta: {self.metrics['beta']:.2f}\")\n",
    "        print(f\"Alpha: {self.metrics['alpha']:.2%}\")\n",
    "        \n",
    "        print(\"\\nS&P 500:\")\n",
    "        print(f\"Retorno Total: {self.metrics['market_return']:.2%}\")\n",
    "        print(f\"Retorno Anualizado: {self.metrics['market_annual']:.2%}\")\n",
    "        \n",
    "        print(\"\\nResultados guardados en: ./results/\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "def run_enhanced_strategy(start_date='2020-01-01', end_date='2025-01-01'):\n",
    "    \"\"\"Ejecuta la estrategia mejorada con parámetros optimizados.\"\"\"\n",
    "    strategy = EnhancedAMAR(\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        target_beta=0.7,          # Exposición controlada al mercado\n",
    "        volatility_target=0.15,   # Mayor tolerancia a volatilidad para mayor rendimiento\n",
    "        max_stock_weight=0.07,    # Mayor concentración en mejores oportunidades\n",
    "        transaction_cost=0.001    # Costos realistas\n",
    "    )\n",
    "    \n",
    "    strategy.run_strategy()\n",
    "    return strategy\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    strategy = run_enhanced_strategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e5da396-1b3a-4dc8-8346-a383903db955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1637fdb2-78e4-45b8-b3e8-3874220e7328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
