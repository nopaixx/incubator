{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48ac7dd0-1a77-415f-b5cd-64490c82d625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Estrategia de Arbitraje Estadístico Multi-Régimen\n",
      "=========================================================\n",
      "Datos cargados para 99 acciones desde 2024-01-01 00:00:00 hasta 2025-12-01 00:00:00\n",
      "\n",
      "Ejecutando backtest...\n",
      "Iniciando backtest...\n",
      "Ejecutando backtest desde 2024-09-09 00:00:00 hasta 2025-04-17 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|███████████████████████████              | 101/153 [01:04<00:09,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecha: 2025-02-03 00:00:00, Equity: 0.9272, Pares activos: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 153/153 [01:51<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Métricas de Rendimiento:\n",
      "Retorno Total: -8.58%\n",
      "Retorno Anualizado: -3.86%\n",
      "Volatilidad Anualizada: 6.56%\n",
      "Sharpe Ratio: -0.89\n",
      "Sortino Ratio: -0.41\n",
      "Máximo Drawdown: -11.83%\n",
      "Calmar Ratio: -0.33\n",
      "Win Rate: 2.96%\n",
      "\n",
      "Ejecutando análisis walk-forward...\n",
      "Ejecutando análisis walk-forward con 2 ventanas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:11<00:11, 12.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ventana 1: 2024-10-02 00:00:00 - 2025-01-02 00:00:00, Retorno: 0.0000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:25<00:00, 12.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ventana 2: 2025-01-02 00:00:00 - 2025-04-04 00:00:00, Retorno: 0.0000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Análisis completado. Resultados guardados en './artifacts/results/'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "from statsmodels.tsa.stattools import coint, adfuller\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import pickle\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.dates as mdates\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Crear directorios para resultados\n",
    "os.makedirs('./artifacts/results', exist_ok=True)\n",
    "os.makedirs('./artifacts/results/figures', exist_ok=True)\n",
    "os.makedirs('./artifacts/results/data', exist_ok=True)\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    filename='./artifacts/errors.txt',\n",
    "    level=logging.ERROR,\n",
    "    format='[%(asctime)s] %(levelname)s: %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# Configurar logging adicional para depuración\n",
    "debug_logger = logging.getLogger('debug')\n",
    "debug_handler = logging.FileHandler('./artifacts/debug.txt')\n",
    "debug_handler.setFormatter(logging.Formatter('[%(asctime)s] DEBUG: %(message)s'))\n",
    "debug_logger.setLevel(logging.INFO)\n",
    "debug_logger.addHandler(debug_handler)\n",
    "\n",
    "# Ignorar advertencias\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class StatisticalArbitrageStrategy:\n",
    "    def __init__(self, start_date='2010-01-01', end_date=None, universe_size=100, \n",
    "                 max_active_pairs=20, trading_params=None):\n",
    "        \"\"\"\n",
    "        Inicializa la estrategia de arbitraje estadístico multi-régimen.\n",
    "        \n",
    "        Args:\n",
    "            start_date (str): Fecha de inicio para el análisis\n",
    "            end_date (str): Fecha de fin para el análisis (None = hoy)\n",
    "            universe_size (int): Número de acciones a considerar del S&P 500\n",
    "            max_active_pairs (int): Número máximo de pares activos simultáneamente\n",
    "            trading_params (dict): Parámetros de trading\n",
    "        \"\"\"\n",
    "        self.start_date = pd.to_datetime(start_date)\n",
    "        self.end_date = pd.to_datetime(end_date) if end_date else pd.to_datetime(datetime.now().date())\n",
    "        self.universe_size = universe_size\n",
    "        self.max_active_pairs = max_active_pairs\n",
    "        \n",
    "        # Parámetros de trading por defecto\n",
    "        self.trading_params = trading_params or {\n",
    "            'z_entry': 2.0,       # Umbral de entrada\n",
    "            'z_exit': 0.0,        # Umbral de salida\n",
    "            'max_holding_period': 20,  # Período máximo de tenencia (días)\n",
    "            'transaction_cost': 0.0005,  # Costo de transacción (5 bps)\n",
    "            'lookback_short': 5,   # Ventana corta para z-score\n",
    "            'lookback_medium': 21, # Ventana media para z-score\n",
    "            'lookback_long': 63,   # Ventana larga para z-score\n",
    "            'regime_lookback': 126, # Ventana para detección de régimen\n",
    "            'pair_lookback': 252,  # Ventana para selección de pares\n",
    "            'rebalance_frequency': 21, # Frecuencia de rebalanceo (días)\n",
    "            'coint_threshold': 0.05, # Umbral para test de cointegración\n",
    "            'min_half_life': 5,    # Half-life mínimo para considerar un par\n",
    "            'max_half_life': 126,  # Half-life máximo para considerar un par\n",
    "        }\n",
    "        \n",
    "        # Variables de estado\n",
    "        self.universe = None\n",
    "        self.price_data = None\n",
    "        self.active_pairs = {}\n",
    "        self.pair_history = {}\n",
    "        self.regime_model = None\n",
    "        self.current_regime = None\n",
    "        self.regime_history = {}\n",
    "        self.performance_metrics = {}\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Carga los datos de precios del universo de acciones del S&P 500.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Obtener lista de símbolos del S&P 500 desde Wikipedia\n",
    "            sp500_url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "            sp500_table = pd.read_html(sp500_url)\n",
    "            sp500_symbols = sp500_table[0]['Symbol'].tolist()\n",
    "            \n",
    "            # Filtrar símbolos con caracteres especiales\n",
    "            sp500_symbols = [s.replace('.', '-') for s in sp500_symbols if '/' not in s]\n",
    "            \n",
    "            debug_logger.info(f\"Obtenidos {len(sp500_symbols)} símbolos del S&P 500\")\n",
    "            \n",
    "            # Seleccionar un subconjunto aleatorio si es necesario\n",
    "            if self.universe_size < len(sp500_symbols):\n",
    "                np.random.seed(42)  # Para reproducibilidad\n",
    "                self.universe = np.random.choice(sp500_symbols, self.universe_size, replace=False)\n",
    "            else:\n",
    "                self.universe = sp500_symbols\n",
    "            \n",
    "            debug_logger.info(f\"Seleccionados {len(self.universe)} símbolos para el universo\")\n",
    "            \n",
    "            # Descargar datos de precios\n",
    "            self.price_data = yf.download(\n",
    "                list(self.universe),  # Convert numpy array to list\n",
    "                start=self.start_date - pd.Timedelta(days=365),\n",
    "                end=self.end_date,\n",
    "                progress=False\n",
    "            )['Close']\n",
    "            \n",
    "            debug_logger.info(f\"Descargados datos para {self.price_data.shape[1]} símbolos\")\n",
    "            \n",
    "            # Verificar datos antes de procesar\n",
    "            before_filter = self.price_data.shape[1]\n",
    "            \n",
    "            # Manejar valores faltantes\n",
    "            self.price_data = self.price_data.ffill().bfill()\n",
    "            \n",
    "            # Filtrar acciones con demasiados valores faltantes\n",
    "            missing_pct = self.price_data.isna().mean()\n",
    "            valid_stocks = missing_pct[missing_pct < 0.1].index.tolist()\n",
    "            self.price_data = self.price_data[valid_stocks]\n",
    "            \n",
    "            # Filtrar acciones con demasiados retornos cero o muy bajos\n",
    "            zero_returns = (self.price_data.pct_change() == 0).mean()\n",
    "            non_zero_stocks = zero_returns[zero_returns < 0.3].index.tolist()\n",
    "            self.price_data = self.price_data[non_zero_stocks]\n",
    "            \n",
    "            # Filtrar acciones con muy baja volatilidad\n",
    "            volatility = self.price_data.pct_change().std()\n",
    "            valid_vol_stocks = volatility[volatility > 0.005].index.tolist()  # Al menos 0.5% de volatilidad diaria\n",
    "            self.price_data = self.price_data[valid_vol_stocks]\n",
    "            \n",
    "            # Actualizar universo\n",
    "            self.universe = list(self.price_data.columns)\n",
    "            \n",
    "            debug_logger.info(f\"Después de filtrado: {len(self.universe)} símbolos (eliminados {before_filter - len(self.universe)})\")\n",
    "            \n",
    "            # Guardar lista de símbolos\n",
    "            pd.Series(self.universe).to_csv('./artifacts/results/data/universe.csv', index=False)\n",
    "            \n",
    "            print(f\"Datos cargados para {len(self.universe)} acciones desde {self.start_date} hasta {self.end_date}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al cargar datos: {str(e)}\", exc_info=True)\n",
    "            return False\n",
    "    \n",
    "    def calculate_market_features(self, current_date):\n",
    "        \"\"\"\n",
    "        Calcula características del mercado para la detección de regímenes.\n",
    "        \n",
    "        Args:\n",
    "            current_date (datetime): Fecha actual para el análisis\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Características del mercado\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Filtrar datos hasta la fecha actual\n",
    "            cutoff_date = pd.to_datetime(current_date)\n",
    "            lookback = self.trading_params['regime_lookback']\n",
    "            \n",
    "            # Asegurar que tenemos suficientes datos\n",
    "            start_date = cutoff_date - pd.Timedelta(days=lookback*2)\n",
    "            \n",
    "            # Descargar datos del SPY como proxy del mercado\n",
    "            spy_data = yf.download('SPY', start=start_date, end=cutoff_date, progress=False)\n",
    "            \n",
    "            if spy_data.empty:\n",
    "                debug_logger.info(f\"No se pudieron obtener datos de SPY para {start_date} a {cutoff_date}\")\n",
    "                raise ValueError(f\"No se pudieron obtener datos de SPY para {start_date} a {cutoff_date}\")\n",
    "            \n",
    "            # Calcular retornos\n",
    "            spy_data['returns'] = spy_data['Close'].pct_change()\n",
    "            \n",
    "            # Calcular características\n",
    "            features = pd.DataFrame(index=[cutoff_date])\n",
    "            \n",
    "            # Volatilidad (diferentes ventanas)\n",
    "            features['volatility_10d'] = spy_data['returns'].rolling(10).std().iloc[-1] * np.sqrt(252)\n",
    "            features['volatility_30d'] = spy_data['returns'].rolling(30).std().iloc[-1] * np.sqrt(252)\n",
    "            features['volatility_60d'] = spy_data['returns'].rolling(60).std().iloc[-1] * np.sqrt(252)\n",
    "            \n",
    "            # Momentum (diferentes ventanas)\n",
    "            features['momentum_10d'] = spy_data['Close'].pct_change(10).iloc[-1]\n",
    "            features['momentum_30d'] = spy_data['Close'].pct_change(30).iloc[-1]\n",
    "            features['momentum_60d'] = spy_data['Close'].pct_change(60).iloc[-1]\n",
    "            \n",
    "            # Volatilidad relativa (ratio de volatilidades)\n",
    "            features['rel_vol_10_30'] = features['volatility_10d'] / features['volatility_30d']\n",
    "            features['rel_vol_10_60'] = features['volatility_10d'] / features['volatility_60d']\n",
    "            \n",
    "            # Drawdown\n",
    "            rolling_max = spy_data['Close'].rolling(60, min_periods=1).max()\n",
    "            drawdown = (spy_data['Close'] / rolling_max - 1.0)\n",
    "            features['max_drawdown_60d'] = drawdown.rolling(60).min().iloc[-1]\n",
    "            \n",
    "            # Asimetría y curtosis\n",
    "            features['skewness_30d'] = spy_data['returns'].rolling(30).skew().iloc[-1]\n",
    "            features['kurtosis_30d'] = spy_data['returns'].rolling(30).kurt().iloc[-1]\n",
    "            \n",
    "            # Manejar valores faltantes\n",
    "            features = features.fillna(method='ffill').fillna(0)\n",
    "            \n",
    "            return features\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al calcular características del mercado: {str(e)}\", exc_info=True)\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def detect_market_regime(self, current_date, i=0):\n",
    "        \"\"\"\n",
    "        Detecta el régimen de mercado actual utilizando clustering.\n",
    "        \n",
    "        Args:\n",
    "            current_date (datetime): Fecha actual para el análisis\n",
    "            i (int): Índice de iteración para actualización periódica\n",
    "        \n",
    "        Returns:\n",
    "            int: Identificador del régimen actual\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Calcular características del mercado\n",
    "            features = self.calculate_market_features(current_date)\n",
    "            \n",
    "            if features.empty:\n",
    "                debug_logger.info(f\"Características de mercado vacías para {current_date}. Usando régimen neutral.\")\n",
    "                return 1  # Régimen neutral por defecto\n",
    "            \n",
    "            # Escalar características\n",
    "            scaler = StandardScaler()\n",
    "            scaled_features = scaler.fit_transform(features)\n",
    "            \n",
    "            # Inicializar o actualizar modelo periódicamente\n",
    "            if self.regime_model is None or i % 63 == 0:  # Actualizar cada ~3 meses\n",
    "                # Verificar si tenemos suficientes datos para entrenar\n",
    "                if scaled_features.shape[0] < 2:\n",
    "                    # Si no hay suficientes muestras para clustering\n",
    "                    debug_logger.info(f\"Datos insuficientes para entrenar modelo de clustering en {current_date}\")\n",
    "                    return 1  # Régimen neutral por defecto\n",
    "                    \n",
    "                try:\n",
    "                    # Entrenar modelo de clustering\n",
    "                    self.regime_model = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "                    self.regime_model.fit(scaled_features)\n",
    "                    debug_logger.info(f\"Modelo de regímenes entrenado en {current_date}\")\n",
    "                except Exception as e:\n",
    "                    debug_logger.info(f\"Error al entrenar modelo de clustering: {str(e)}\")\n",
    "                    return 1  # Régimen neutral por defecto\n",
    "            \n",
    "            # Predecir régimen\n",
    "            if self.regime_model is None:\n",
    "                return 1  # Régimen neutral por defecto\n",
    "                \n",
    "            try:\n",
    "                regime = int(self.regime_model.predict(scaled_features)[0])\n",
    "            except Exception as e:\n",
    "                debug_logger.info(f\"Error al predecir régimen: {str(e)}\")\n",
    "                return 1  # Régimen neutral por defecto\n",
    "            \n",
    "            # Guardar historial de regímenes\n",
    "            self.regime_history[current_date] = regime\n",
    "            \n",
    "            return regime\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al detectar régimen de mercado: {str(e)}\", exc_info=True)\n",
    "            return 1  # Régimen neutral por defecto\n",
    "    \n",
    "    def find_cointegrated_pairs(self, current_date):\n",
    "        \"\"\"\n",
    "        Encuentra pares cointegrados utilizando datos históricos hasta la fecha actual.\n",
    "        \n",
    "        Args:\n",
    "            current_date (datetime): Fecha actual para el análisis\n",
    "        \n",
    "        Returns:\n",
    "            list: Lista de pares cointegrados con sus estadísticas\n",
    "        \"\"\"\n",
    "        try:\n",
    "            debug_logger.info(f\"Buscando pares cointegrados para {current_date}\")\n",
    "            \n",
    "            # Implementar separación temporal estricta\n",
    "            selection_end = current_date - pd.Timedelta(days=63)\n",
    "            validation_start = selection_end\n",
    "            validation_end = current_date\n",
    "            \n",
    "            # Filtrar datos para selección y validación\n",
    "            selection_data = self.price_data.loc[:selection_end].copy()\n",
    "            validation_data = self.price_data.loc[validation_start:validation_end].copy()\n",
    "            \n",
    "            # Asegurar suficientes datos\n",
    "            min_samples = self.trading_params['pair_lookback']\n",
    "            if len(selection_data) < min_samples:\n",
    "                debug_logger.info(f\"Datos insuficientes para selección de pares: {len(selection_data)} < {min_samples}\")\n",
    "                return []\n",
    "            \n",
    "            # Usar solo los últimos datos para selección\n",
    "            selection_data = selection_data.iloc[-min_samples:]\n",
    "            \n",
    "            # Calcular retornos logarítmicos para selección\n",
    "            log_prices = np.log(selection_data)\n",
    "            \n",
    "            # Lista para almacenar pares cointegrados\n",
    "            cointegrated_pairs = []\n",
    "            \n",
    "            # Obtener lista de símbolos con datos suficientes\n",
    "            valid_symbols = selection_data.columns[selection_data.isna().sum() < min_samples * 0.1].tolist()\n",
    "            \n",
    "            debug_logger.info(f\"Símbolos válidos para análisis de pares: {len(valid_symbols)}\")\n",
    "            \n",
    "            # Limitar número de combinaciones para eficiencia\n",
    "            if len(valid_symbols) > 50:\n",
    "                np.random.seed(42 + pd.Timestamp(current_date).dayofyear)\n",
    "                valid_symbols = np.random.choice(valid_symbols, 50, replace=False)\n",
    "                debug_logger.info(f\"Limitado a 50 símbolos para eficiencia\")\n",
    "            \n",
    "            # Contador para seguimiento\n",
    "            pairs_tested = 0\n",
    "            pairs_cointegrated = 0\n",
    "            \n",
    "            # Probar todas las combinaciones de pares\n",
    "            for ticker1, ticker2 in itertools.combinations(valid_symbols, 2):\n",
    "                pairs_tested += 1\n",
    "                \n",
    "                # Verificar datos suficientes\n",
    "                pair_data = log_prices[[ticker1, ticker2]].dropna()\n",
    "                if len(pair_data) < min_samples * 0.9:\n",
    "                    continue\n",
    "                \n",
    "                # Verificar variabilidad\n",
    "                if (pair_data[ticker1].std() <= 0.001 or pair_data[ticker2].std() <= 0.001):\n",
    "                    continue\n",
    "                \n",
    "                # Test de cointegración\n",
    "                try:\n",
    "                    score, pvalue, _ = coint(pair_data[ticker1], pair_data[ticker2])\n",
    "                except Exception as e:\n",
    "                    debug_logger.info(f\"Error en test de cointegración para {ticker1}-{ticker2}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                if pvalue < self.trading_params['coint_threshold']:\n",
    "                    pairs_cointegrated += 1\n",
    "                    \n",
    "                    # Calcular beta (ratio de cointegración) de manera segura\n",
    "                    try:\n",
    "                        # Añadir constante para la regresión\n",
    "                        X = sm.add_constant(pair_data[ticker2])\n",
    "                        \n",
    "                        # Verificar que X tiene más de una columna (constante + ticker2)\n",
    "                        if X.shape[1] <= 1:\n",
    "                            debug_logger.info(f\"Matriz X inválida para regresión de {ticker1}-{ticker2}\")\n",
    "                            continue\n",
    "                            \n",
    "                        # Ejecutar regresión\n",
    "                        model = OLS(pair_data[ticker1], X)\n",
    "                        results = model.fit()\n",
    "                        \n",
    "                        # Extraer beta con manejo de errores\n",
    "                        if len(results.params) < 2:\n",
    "                            debug_logger.info(f\"Parámetros insuficientes en regresión para {ticker1}-{ticker2}\")\n",
    "                            continue\n",
    "                            \n",
    "                        # Intentar obtener el coeficiente de diferentes formas\n",
    "                        try:\n",
    "                            # Intentar por nombre de columna/índice\n",
    "                            if ticker2 in results.params:\n",
    "                                beta = results.params[ticker2]\n",
    "                            elif 'x1' in results.params:\n",
    "                                beta = results.params['x1']\n",
    "                            elif results.params.index[1] == ticker2:\n",
    "                                beta = results.params[results.params.index[1]]\n",
    "                            # Intentar por posición\n",
    "                            else:\n",
    "                                beta = results.params.iloc[1]\n",
    "                        except (IndexError, KeyError) as e:\n",
    "                            debug_logger.info(f\"Error al extraer beta para {ticker1}-{ticker2}: {str(e)}\")\n",
    "                            continue\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        debug_logger.info(f\"Error en regresión OLS para {ticker1}-{ticker2}: {str(e)}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Calcular spread\n",
    "                    spread = pair_data[ticker1] - beta * pair_data[ticker2]\n",
    "                    \n",
    "                    # Verificar que el spread tiene variabilidad\n",
    "                    if spread.std() <= 1e-8:\n",
    "                        debug_logger.info(f\"Spread constante para {ticker1}-{ticker2}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Calcular half-life de reversión a la media\n",
    "                    half_life = self._calculate_half_life(spread)\n",
    "                    \n",
    "                    # Filtrar por half-life\n",
    "                    min_hl = self.trading_params['min_half_life']\n",
    "                    max_hl = self.trading_params['max_half_life']\n",
    "                    \n",
    "                    if min_hl <= half_life <= max_hl:\n",
    "                        # Validar en datos recientes\n",
    "                        valid = self._validate_pair(ticker1, ticker2, beta, validation_data)\n",
    "                        \n",
    "                        if valid:\n",
    "                            # Calcular métricas adicionales\n",
    "                            spread_mean = spread.mean()\n",
    "                            spread_std = spread.std()\n",
    "                            sharpe = self._calculate_pair_sharpe(ticker1, ticker2, beta, validation_data)\n",
    "                            \n",
    "                            cointegrated_pairs.append({\n",
    "                                'ticker1': ticker1,\n",
    "                                'ticker2': ticker2,\n",
    "                                'beta': beta,\n",
    "                                'half_life': half_life,\n",
    "                                'pvalue': pvalue,\n",
    "                                'spread_mean': spread_mean,\n",
    "                                'spread_std': spread_std,\n",
    "                                'sharpe': sharpe\n",
    "                            })\n",
    "            \n",
    "            # Ordenar pares por sharpe ratio y half-life\n",
    "            if cointegrated_pairs:\n",
    "                cointegrated_pairs.sort(key=lambda x: (-x['sharpe'], x['half_life']))\n",
    "            \n",
    "            debug_logger.info(f\"Probados {pairs_tested} pares, encontrados {pairs_cointegrated} cointegrados, {len(cointegrated_pairs)} válidos\")\n",
    "            \n",
    "            return cointegrated_pairs\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al encontrar pares cointegrados: {str(e)}\", exc_info=True)\n",
    "            return []\n",
    "    \n",
    "    def _calculate_half_life(self, spread):\n",
    "        \"\"\"\n",
    "        Calcula el half-life de reversión a la media para un spread.\n",
    "        \n",
    "        Args:\n",
    "            spread (pd.Series): Serie temporal del spread\n",
    "        \n",
    "        Returns:\n",
    "            float: Half-life en días\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Calcular cambios en el spread\n",
    "            spread_lag = spread.shift(1)\n",
    "            delta_spread = spread - spread_lag\n",
    "            \n",
    "            # Eliminar NaN\n",
    "            spread_lag = spread_lag.dropna()\n",
    "            delta_spread = delta_spread.dropna()\n",
    "            \n",
    "            # Verificar datos suficientes\n",
    "            if len(spread_lag) < 10:\n",
    "                return 126  # Valor por defecto si hay pocos datos\n",
    "            \n",
    "            # Verificar variabilidad\n",
    "            if spread_lag.std() <= 1e-8 or delta_spread.std() <= 1e-8:\n",
    "                return 126  # Valor por defecto si hay poca variabilidad\n",
    "            \n",
    "            # Regresión para estimar velocidad de reversión\n",
    "            X = sm.add_constant(spread_lag)\n",
    "            model = OLS(delta_spread, X)\n",
    "            results = model.fit()\n",
    "            \n",
    "            # Extraer coeficiente gamma con manejo de errores\n",
    "            if len(results.params) < 2:\n",
    "                return 126  # Valor por defecto si la regresión falló\n",
    "            \n",
    "            # Intentar obtener gamma de diferentes formas\n",
    "            try:\n",
    "                # Intentar por nombre de variable\n",
    "                if 'x1' in results.params:\n",
    "                    gamma = results.params['x1']\n",
    "                # Intentar por posición/índice\n",
    "                elif len(results.params) > 1:\n",
    "                    gamma = results.params.iloc[1]  # Segundo elemento (índice 1)\n",
    "                else:\n",
    "                    gamma = -0.05  # Valor predeterminado\n",
    "            except (IndexError, KeyError):\n",
    "                gamma = -0.05  # Valor predeterminado\n",
    "            \n",
    "            # Validar gamma\n",
    "            if gamma >= 0 or gamma < -1:  # No hay reversión a la media o reversión extrema\n",
    "                return 126  # Valor máximo predeterminado\n",
    "            \n",
    "            # Calcular half-life\n",
    "            half_life = -np.log(2) / gamma\n",
    "            \n",
    "            # Limitar a valores razonables\n",
    "            return max(1, min(126, half_life))\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al calcular half-life: {str(e)}\", exc_info=True)\n",
    "            return 126  # Valor por defecto\n",
    "    \n",
    "    def _validate_pair(self, ticker1, ticker2, beta, validation_data):\n",
    "        \"\"\"\n",
    "        Valida un par en datos recientes.\n",
    "        \n",
    "        Args:\n",
    "            ticker1 (str): Primer ticker\n",
    "            ticker2 (str): Segundo ticker\n",
    "            beta (float): Coeficiente de cointegración\n",
    "            validation_data (pd.DataFrame): Datos para validación\n",
    "        \n",
    "        Returns:\n",
    "            bool: True si el par es válido, False en caso contrario\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Verificar datos suficientes\n",
    "            pair_data = validation_data[[ticker1, ticker2]].dropna()\n",
    "            if len(pair_data) < 10:  # Mínimo 10 días para validación\n",
    "                return False\n",
    "            \n",
    "            # Calcular spread en datos de validación\n",
    "            log_prices = np.log(pair_data)\n",
    "            spread = log_prices[ticker1] - beta * log_prices[ticker2]\n",
    "            \n",
    "            # Verificar variabilidad del spread\n",
    "            if spread.std() <= 1e-8:\n",
    "                debug_logger.info(f\"Spread constante en validación para {ticker1}-{ticker2}\")\n",
    "                return False\n",
    "            \n",
    "            # Test de estacionariedad en datos de validación\n",
    "            try:\n",
    "                adf_result = adfuller(spread)\n",
    "            except ValueError:\n",
    "                # Fallo en test ADF (spread constante u otro problema)\n",
    "                debug_logger.info(f\"Error en test ADF para {ticker1}-{ticker2} en validación\")\n",
    "                return False\n",
    "            \n",
    "            # Verificar estacionariedad\n",
    "            if adf_result[1] > 0.1:  # p-valor mayor a 0.1\n",
    "                return False\n",
    "            \n",
    "            # Verificar volatilidad del spread\n",
    "            spread_std = spread.std()\n",
    "            if spread_std <= 1e-8 or np.isnan(spread_std):\n",
    "                return False\n",
    "            \n",
    "            # Verificar cruces por la media\n",
    "            mean_crossings = ((spread.shift(1) - spread.mean()) * \n",
    "                              (spread - spread.mean()) < 0).sum()\n",
    "            if mean_crossings < 3:  # Menos de 3 cruces\n",
    "                return False\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al validar par: {str(e)}\", exc_info=True)\n",
    "            return False\n",
    "    \n",
    "    def _calculate_pair_sharpe(self, ticker1, ticker2, beta, data):\n",
    "        \"\"\"\n",
    "        Calcula el Sharpe ratio para un par.\n",
    "        \n",
    "        Args:\n",
    "            ticker1 (str): Primer ticker\n",
    "            ticker2 (str): Segundo ticker\n",
    "            beta (float): Coeficiente de cointegración\n",
    "            data (pd.DataFrame): Datos de precios\n",
    "        \n",
    "        Returns:\n",
    "            float: Sharpe ratio anualizado\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Verificar datos suficientes\n",
    "            pair_data = data[[ticker1, ticker2]].dropna()\n",
    "            if len(pair_data) < 10:\n",
    "                return 0\n",
    "            \n",
    "            # Calcular retornos diarios\n",
    "            returns1 = pair_data[ticker1].pct_change()\n",
    "            returns2 = pair_data[ticker2].pct_change()\n",
    "            \n",
    "            # Verificar datos válidos\n",
    "            if returns1.std() <= 1e-8 or returns2.std() <= 1e-8:\n",
    "                return 0\n",
    "            \n",
    "            # Calcular spread\n",
    "            log_prices = np.log(pair_data)\n",
    "            spread = log_prices[ticker1] - beta * log_prices[ticker2]\n",
    "            \n",
    "            # Verificar validez del spread\n",
    "            if spread.std() <= 1e-8 or spread.isna().any():\n",
    "                return 0\n",
    "            \n",
    "            # Calcular z-score\n",
    "            spread_mean = spread.rolling(window=10, min_periods=5).mean()\n",
    "            spread_std = spread.rolling(window=10, min_periods=5).std()\n",
    "            \n",
    "            # Evitar división por cero\n",
    "            valid_idx = spread_std > 1e-8\n",
    "            z_score = pd.Series(index=spread.index, data=np.nan)\n",
    "            z_score.loc[valid_idx] = (spread.loc[valid_idx] - spread_mean.loc[valid_idx]) / spread_std.loc[valid_idx]\n",
    "            z_score = z_score.fillna(0)\n",
    "            \n",
    "            # Generar señales\n",
    "            position = np.zeros(len(z_score))\n",
    "            position[z_score < -2] = 1    # Comprar spread\n",
    "            position[z_score > 2] = -1    # Vender spread\n",
    "            \n",
    "            # Calcular retornos de la estrategia\n",
    "            if len(position) <= 1 or len(returns1) <= 1:\n",
    "                return 0\n",
    "                \n",
    "            pair_return = pd.Series(position[:-1]) * (returns1.values[1:] - beta * returns2.values[1:])\n",
    "            \n",
    "            # Verificar validez de retornos\n",
    "            if len(pair_return) < 5 or pair_return.std() <= 1e-8:\n",
    "                return 0\n",
    "                \n",
    "            # Calcular Sharpe ratio\n",
    "            sharpe = (pair_return.mean() / pair_return.std()) * np.sqrt(252)\n",
    "            \n",
    "            # Limitar valores extremos\n",
    "            return max(-10, min(10, sharpe))\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al calcular Sharpe ratio: {str(e)}\", exc_info=True)\n",
    "            return 0\n",
    "    \n",
    "    def generate_signals(self, ticker1, ticker2, beta, price_data, current_date, lookback=None):\n",
    "        \"\"\"\n",
    "        Genera señales de trading para un par.\n",
    "        \n",
    "        Args:\n",
    "            ticker1 (str): Primer ticker\n",
    "            ticker2 (str): Segundo ticker\n",
    "            beta (float): Coeficiente de cointegración\n",
    "            price_data (dict): Datos de precios\n",
    "            current_date (datetime): Fecha actual\n",
    "            lookback (int): Período de lookback\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame con señales\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Configurar lookback\n",
    "            if lookback is None:\n",
    "                lookback = self.trading_params['pair_lookback']\n",
    "            \n",
    "            # Filtrar datos hasta la fecha actual\n",
    "            cutoff_date = pd.to_datetime(current_date)\n",
    "            \n",
    "            # Extraer precios\n",
    "            prices1 = price_data[ticker1].copy()\n",
    "            prices2 = price_data[ticker2].copy()\n",
    "            \n",
    "            # Verificar datos válidos\n",
    "            if prices1.isna().sum() > len(prices1) * 0.1 or prices2.isna().sum() > len(prices2) * 0.1:\n",
    "                debug_logger.info(f\"Demasiados NaN en precios para {ticker1}-{ticker2}\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Manejar valores faltantes\n",
    "            prices1 = prices1.ffill().bfill()\n",
    "            prices2 = prices2.ffill().bfill()\n",
    "            \n",
    "            # Calcular retornos\n",
    "            returns1 = prices1.pct_change()\n",
    "            returns2 = prices2.pct_change()\n",
    "            \n",
    "            # Verificar retornos válidos\n",
    "            if returns1.std() <= 1e-8 or returns2.std() <= 1e-8:\n",
    "                debug_logger.info(f\"Baja volatilidad en retornos para {ticker1}-{ticker2}\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Calcular spread logarítmico\n",
    "            log_prices1 = np.log(prices1)\n",
    "            log_prices2 = np.log(prices2)\n",
    "            spread = log_prices1 - beta * log_prices2\n",
    "            \n",
    "            # Verificar spread válido\n",
    "            if spread.std() <= 1e-8:\n",
    "                debug_logger.info(f\"Spread constante para {ticker1}-{ticker2}\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Calcular z-scores para diferentes horizontes\n",
    "            lookback_short = self.trading_params['lookback_short']\n",
    "            lookback_medium = self.trading_params['lookback_medium']\n",
    "            lookback_long = self.trading_params['lookback_long']\n",
    "            \n",
    "            # Z-score de corto plazo con manejo de errores\n",
    "            short_mean = spread.rolling(window=lookback_short, min_periods=3).mean()\n",
    "            short_std = spread.rolling(window=lookback_short, min_periods=3).std()\n",
    "            valid_idx_short = short_std > 1e-8\n",
    "            \n",
    "            z_short = pd.Series(index=spread.index, data=0.0)\n",
    "            z_short.loc[valid_idx_short] = ((spread - short_mean) / short_std).loc[valid_idx_short]\n",
    "            \n",
    "            # Z-score de medio plazo\n",
    "            med_mean = spread.rolling(window=lookback_medium, min_periods=10).mean()\n",
    "            med_std = spread.rolling(window=lookback_medium, min_periods=10).std()\n",
    "            valid_idx_med = med_std > 1e-8\n",
    "            \n",
    "            z_medium = pd.Series(index=spread.index, data=0.0)\n",
    "            z_medium.loc[valid_idx_med] = ((spread - med_mean) / med_std).loc[valid_idx_med]\n",
    "            \n",
    "            # Z-score de largo plazo\n",
    "            long_mean = spread.rolling(window=lookback_long, min_periods=30).mean()\n",
    "            long_std = spread.rolling(window=lookback_long, min_periods=30).std()\n",
    "            valid_idx_long = long_std > 1e-8\n",
    "            \n",
    "            z_long = pd.Series(index=spread.index, data=0.0)\n",
    "            z_long.loc[valid_idx_long] = ((spread - long_mean) / long_std).loc[valid_idx_long]\n",
    "            \n",
    "            # Combinar z-scores (ponderación adaptativa)\n",
    "            z_score = 0.5 * z_short + 0.3 * z_medium + 0.2 * z_long\n",
    "            \n",
    "            # Crear DataFrame de señales\n",
    "            signals = pd.DataFrame(index=spread.index)\n",
    "            signals['spread'] = spread\n",
    "            signals['z_score'] = z_score\n",
    "            signals['z_short'] = z_short\n",
    "            signals['z_medium'] = z_medium\n",
    "            signals['z_long'] = z_long\n",
    "            signals['return1'] = returns1\n",
    "            signals['return2'] = returns2\n",
    "            \n",
    "            # Implementar señal condicional no lineal\n",
    "            signals['signal_intensity'] = signals['z_score'].copy()\n",
    "            \n",
    "            # Ajuste condicional basado en concordancia de señales\n",
    "            for i in range(1, len(signals)):\n",
    "                if abs(signals['z_short'].iloc[i]) > 2 and np.sign(signals['z_short'].iloc[i]) == np.sign(signals['z_medium'].iloc[i]):\n",
    "                    signals.loc[signals.index[i], 'signal_intensity'] = 1.5 * signals['z_score'].iloc[i]\n",
    "                elif np.sign(signals['z_short'].iloc[i]) != np.sign(signals['z_medium'].iloc[i]):\n",
    "                    signals.loc[signals.index[i], 'signal_intensity'] = 0.5 * signals['z_score'].iloc[i]\n",
    "            \n",
    "            # Calcular SNR para filtrar señales de baja calidad\n",
    "            signals['signal_mean'] = signals['z_score'].rolling(21, min_periods=10).apply(lambda x: np.abs(x).mean())\n",
    "            signals['signal_std'] = signals['z_score'].rolling(21, min_periods=10).std()\n",
    "            \n",
    "            # Evitar división por cero\n",
    "            valid_snr = signals['signal_std'] > 1e-8\n",
    "            signals['snr'] = 0.0\n",
    "            signals.loc[valid_snr, 'snr'] = signals.loc[valid_snr, 'signal_mean'] / signals.loc[valid_snr, 'signal_std']\n",
    "            \n",
    "            # Generar posiciones\n",
    "            signals['position'] = 0.0\n",
    "            \n",
    "            # Umbral de entrada y salida\n",
    "            z_entry = self.trading_params['z_entry']\n",
    "            z_exit = self.trading_params['z_exit']\n",
    "            \n",
    "            # Inicializar posición\n",
    "            position = 0\n",
    "            entry_date = None\n",
    "            \n",
    "            # Generar señales\n",
    "            for i in range(1, len(signals)):\n",
    "                date = signals.index[i]\n",
    "                \n",
    "                # Filtrar por SNR\n",
    "                snr_threshold = 0.5\n",
    "                snr_valid = signals['snr'].iloc[i] > snr_threshold\n",
    "                \n",
    "                # Lógica de entrada y salida\n",
    "                if position == 0:  # Sin posición\n",
    "                    if signals['signal_intensity'].iloc[i] < -z_entry and snr_valid:\n",
    "                        position = 1  # Comprar spread (long ticker1, short ticker2)\n",
    "                        entry_date = date\n",
    "                    elif signals['signal_intensity'].iloc[i] > z_entry and snr_valid:\n",
    "                        position = -1  # Vender spread (short ticker1, long ticker2)\n",
    "                        entry_date = date\n",
    "                \n",
    "                elif position == 1:  # Posición larga en spread\n",
    "                    # Salir si el z-score cruza el umbral de salida o se alcanza el período máximo\n",
    "                    days_held = (date - entry_date).days if entry_date else 0\n",
    "                    if (signals['signal_intensity'].iloc[i] >= z_exit or \n",
    "                        days_held > self.trading_params['max_holding_period']):\n",
    "                        position = 0\n",
    "                        entry_date = None\n",
    "                \n",
    "                elif position == -1:  # Posición corta en spread\n",
    "                    # Salir si el z-score cruza el umbral de salida o se alcanza el período máximo\n",
    "                    days_held = (date - entry_date).days if entry_date else 0\n",
    "                    if (signals['signal_intensity'].iloc[i] <= -z_exit or \n",
    "                        days_held > self.trading_params['max_holding_period']):\n",
    "                        position = 0\n",
    "                        entry_date = None\n",
    "                \n",
    "                signals.loc[date, 'position'] = position\n",
    "            \n",
    "            # Calcular retornos de la estrategia\n",
    "            signals['pair_return'] = signals['position'].shift(1) * (\n",
    "                signals['return1'] - beta * signals['return2']\n",
    "            )\n",
    "            \n",
    "            # Calcular retornos acumulados\n",
    "            signals['cumulative_return'] = (1 + signals['pair_return']).cumprod() - 1\n",
    "            \n",
    "            return signals\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al generar señales: {str(e)}\", exc_info=True)\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def update_trading_parameters(self, current_regime, previous_regime, performance_history):\n",
    "        \"\"\"\n",
    "        Actualiza los parámetros de trading según el régimen actual.\n",
    "        \n",
    "        Args:\n",
    "            current_regime (int): Régimen actual\n",
    "            previous_regime (int): Régimen anterior\n",
    "            performance_history (dict): Historial de rendimiento\n",
    "        \n",
    "        Returns:\n",
    "            dict: Parámetros actualizados\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Parámetros específicos por régimen\n",
    "            regime_params = {\n",
    "                0: {'z_entry': 2.5, 'z_exit': 0.5, 'max_holding_period': 15},  # Régimen volátil: más conservador\n",
    "                1: {'z_entry': 2.0, 'z_exit': 0.0, 'max_holding_period': 20},  # Régimen neutral\n",
    "                2: {'z_entry': 1.5, 'z_exit': 0.0, 'max_holding_period': 25}   # Régimen estable: más agresivo\n",
    "            }\n",
    "            \n",
    "            # Asegurar que el régimen está dentro del rango esperado\n",
    "            current_regime = max(0, min(2, current_regime))\n",
    "            previous_regime = max(0, min(2, previous_regime))\n",
    "            \n",
    "            # Calcular probabilidad de cambio de régimen\n",
    "            p_change = abs(current_regime - previous_regime) / 2  # Simplificación\n",
    "            \n",
    "            # Actualizar parámetros gradualmente\n",
    "            updated_params = self.trading_params.copy()\n",
    "            \n",
    "            # Aplicar parámetros del régimen actual\n",
    "            for param, value in regime_params[current_regime].items():\n",
    "                # Interpolación lineal entre parámetros actuales y nuevos\n",
    "                current_value = updated_params[param]\n",
    "                target_value = value\n",
    "                updated_params[param] = current_value * (1 - p_change) + target_value * p_change\n",
    "            \n",
    "            # Ajustar según rendimiento reciente si hay suficientes datos\n",
    "            if len(performance_history) > 10:\n",
    "                recent_returns = list(performance_history.values())[-10:]\n",
    "                \n",
    "                # Evitar división por cero\n",
    "                if np.std(recent_returns) > 1e-8:\n",
    "                    recent_sharpe = np.mean(recent_returns) / np.std(recent_returns) * np.sqrt(252)\n",
    "                    \n",
    "                    # Ajustar agresividad según Sharpe ratio reciente\n",
    "                    if recent_sharpe < 0.5:\n",
    "                        # Más conservador\n",
    "                        updated_params['z_entry'] = min(3.0, updated_params['z_entry'] * 1.1)\n",
    "                        updated_params['max_holding_period'] = max(10, updated_params['max_holding_period'] * 0.9)\n",
    "                    elif recent_sharpe > 1.5:\n",
    "                        # Más agresivo\n",
    "                        updated_params['z_entry'] = max(1.5, updated_params['z_entry'] * 0.9)\n",
    "                        updated_params['max_holding_period'] = min(30, updated_params['max_holding_period'] * 1.1)\n",
    "            \n",
    "            debug_logger.info(f\"Parámetros actualizados para régimen {current_regime}: z_entry={updated_params['z_entry']:.2f}, z_exit={updated_params['z_exit']:.2f}\")\n",
    "            \n",
    "            return updated_params\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al actualizar parámetros: {str(e)}\", exc_info=True)\n",
    "            return self.trading_params\n",
    "    \n",
    "    def manage_portfolio(self, current_date, active_pairs, available_pairs):\n",
    "        \"\"\"\n",
    "        Gestiona la cartera de pares activos.\n",
    "        \n",
    "        Args:\n",
    "            current_date (datetime): Fecha actual\n",
    "            active_pairs (dict): Pares actualmente en cartera\n",
    "            available_pairs (list): Nuevos pares disponibles\n",
    "        \n",
    "        Returns:\n",
    "            dict: Pares actualizados en cartera\n",
    "        \"\"\"\n",
    "        try:\n",
    "            debug_logger.info(f\"Gestionando cartera para {current_date}. Pares activos: {len(active_pairs)}, Pares disponibles: {len(available_pairs)}\")\n",
    "            \n",
    "            updated_portfolio = {}\n",
    "            \n",
    "            # Evaluar pares activos\n",
    "            for pair_id, pair_info in active_pairs.items():\n",
    "                ticker1, ticker2 = pair_info['ticker1'], pair_info['ticker2']\n",
    "                beta = pair_info['beta']\n",
    "                entry_date = pair_info['entry_date']\n",
    "                \n",
    "                # Verificar si tenemos los datos necesarios\n",
    "                if ticker1 not in self.price_data.columns or ticker2 not in self.price_data.columns:\n",
    "                    debug_logger.info(f\"Descartando par {ticker1}-{ticker2} por falta de datos\")\n",
    "                    continue\n",
    "                \n",
    "                # Verificar si debemos mantener el par\n",
    "                days_held = (current_date - entry_date).days\n",
    "                \n",
    "                # Obtener señales actualizadas\n",
    "                signals = self.generate_signals(\n",
    "                    ticker1, ticker2, beta, \n",
    "                    {ticker1: self.price_data[ticker1], ticker2: self.price_data[ticker2]},\n",
    "                    current_date\n",
    "                )\n",
    "                \n",
    "                if signals.empty:\n",
    "                    debug_logger.info(f\"Señales vacías para par {ticker1}-{ticker2}\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    current_position = signals['position'].iloc[-1]\n",
    "                    current_z_score = signals['z_score'].iloc[-1]\n",
    "                except (IndexError, KeyError):\n",
    "                    debug_logger.info(f\"Error al extraer posición/z-score para par {ticker1}-{ticker2}\")\n",
    "                    continue\n",
    "                \n",
    "                # Decidir si mantener o cerrar la posición\n",
    "                if current_position != 0:\n",
    "                    # Mantener par activo\n",
    "                    updated_portfolio[pair_id] = pair_info\n",
    "                    updated_portfolio[pair_id]['current_position'] = current_position\n",
    "                    updated_portfolio[pair_id]['current_z_score'] = current_z_score\n",
    "                    updated_portfolio[pair_id]['days_held'] = days_held\n",
    "                    \n",
    "                    # Actualizar rendimiento\n",
    "                    if 'cumulative_return' in signals.columns and not signals['cumulative_return'].empty:\n",
    "                        try:\n",
    "                            updated_portfolio[pair_id]['current_return'] = signals['cumulative_return'].iloc[-1]\n",
    "                        except (IndexError, KeyError):\n",
    "                            updated_portfolio[pair_id]['current_return'] = 0.0\n",
    "                    \n",
    "                    debug_logger.info(f\"Manteniendo par {ticker1}-{ticker2} con posición {current_position}\")\n",
    "                else:\n",
    "                    debug_logger.info(f\"Cerrando posición para par {ticker1}-{ticker2}\")\n",
    "            \n",
    "            # Añadir nuevos pares si hay espacio\n",
    "            remaining_slots = self.max_active_pairs - len(updated_portfolio)\n",
    "            \n",
    "            if remaining_slots > 0 and available_pairs:\n",
    "                debug_logger.info(f\"Evaluando {len(available_pairs)} nuevos pares para {remaining_slots} slots disponibles\")\n",
    "                \n",
    "                # Ordenar pares disponibles por potencial\n",
    "                sorted_pairs = sorted(available_pairs, key=lambda x: (-x['sharpe'], x['half_life']))\n",
    "                \n",
    "                # Añadir nuevos pares\n",
    "                added_pairs = 0\n",
    "                for pair in sorted_pairs:\n",
    "                    if added_pairs >= remaining_slots:\n",
    "                        break\n",
    "                        \n",
    "                    # Verificar datos disponibles\n",
    "                    ticker1, ticker2 = pair['ticker1'], pair['ticker2']\n",
    "                    if ticker1 not in self.price_data.columns or ticker2 not in self.price_data.columns:\n",
    "                        continue\n",
    "                        \n",
    "                    pair_id = f\"{ticker1}_{ticker2}_{current_date.strftime('%Y%m%d')}\"\n",
    "                    \n",
    "                    # Generar señales iniciales\n",
    "                    signals = self.generate_signals(\n",
    "                        ticker1, ticker2, pair['beta'],\n",
    "                        {ticker1: self.price_data[ticker1], ticker2: self.price_data[ticker2]},\n",
    "                        current_date\n",
    "                    )\n",
    "                    \n",
    "                    if signals.empty:\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        current_position = signals['position'].iloc[-1]\n",
    "                        current_z_score = signals['z_score'].iloc[-1]\n",
    "                    except (IndexError, KeyError):\n",
    "                        continue\n",
    "                    \n",
    "                    # Solo añadir si hay señal activa\n",
    "                    if current_position != 0:\n",
    "                        updated_portfolio[pair_id] = {\n",
    "                            'ticker1': ticker1,\n",
    "                            'ticker2': ticker2,\n",
    "                            'beta': pair['beta'],\n",
    "                            'entry_date': current_date,\n",
    "                            'current_position': current_position,\n",
    "                            'current_z_score': current_z_score,\n",
    "                            'days_held': 0,\n",
    "                            'current_return': 0.0\n",
    "                        }\n",
    "                        added_pairs += 1\n",
    "                        debug_logger.info(f\"Añadiendo nuevo par {ticker1}-{ticker2} con posición {current_position}\")\n",
    "                \n",
    "                debug_logger.info(f\"Añadidos {added_pairs} nuevos pares a la cartera\")\n",
    "            \n",
    "            return updated_portfolio\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al gestionar cartera: {str(e)}\", exc_info=True)\n",
    "            return active_pairs\n",
    "    \n",
    "    def calculate_portfolio_returns(self, active_pairs, current_date, previous_date):\n",
    "        \"\"\"\n",
    "        Calcula los retornos de la cartera para un día.\n",
    "        \n",
    "        Args:\n",
    "            active_pairs (dict): Pares activos en cartera\n",
    "            current_date (datetime): Fecha actual\n",
    "            previous_date (datetime): Fecha anterior\n",
    "        \n",
    "        Returns:\n",
    "            float: Retorno diario de la cartera\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not active_pairs:\n",
    "                return 0.0\n",
    "            \n",
    "            daily_returns = []\n",
    "            position_sizes = []\n",
    "            \n",
    "            # Verificar que las fechas están en el índice\n",
    "            if previous_date not in self.price_data.index or current_date not in self.price_data.index:\n",
    "                debug_logger.info(f\"Fechas inválidas para cálculo de retornos: {previous_date} o {current_date}\")\n",
    "                return 0.0\n",
    "            \n",
    "            for pair_id, pair_info in active_pairs.items():\n",
    "                ticker1, ticker2 = pair_info['ticker1'], pair_info['ticker2']\n",
    "                beta = pair_info['beta']\n",
    "                position = pair_info['current_position']\n",
    "                \n",
    "                # Verificar que tenemos los datos necesarios\n",
    "                if ticker1 not in self.price_data.columns or ticker2 not in self.price_data.columns:\n",
    "                    continue\n",
    "                \n",
    "                # Extraer precios\n",
    "                try:\n",
    "                    price1_prev = self.price_data.loc[previous_date, ticker1]\n",
    "                    price1_curr = self.price_data.loc[current_date, ticker1]\n",
    "                    price2_prev = self.price_data.loc[previous_date, ticker2]\n",
    "                    price2_curr = self.price_data.loc[current_date, ticker2]\n",
    "                except KeyError:\n",
    "                    continue\n",
    "                \n",
    "                # Verificar datos válidos\n",
    "                if (np.isnan(price1_prev) or np.isnan(price1_curr) or \n",
    "                    np.isnan(price2_prev) or np.isnan(price2_curr) or\n",
    "                    price1_prev <= 0 or price2_prev <= 0):\n",
    "                    continue\n",
    "                \n",
    "                # Calcular retornos\n",
    "                return1 = price1_curr / price1_prev - 1\n",
    "                return2 = price2_curr / price2_prev - 1\n",
    "                \n",
    "                # Calcular retorno del par según posición\n",
    "                pair_return = position * (return1 - beta * return2)\n",
    "                \n",
    "                # Ajustar tamaño de posición según intensidad de señal\n",
    "                try:\n",
    "                    z_score = abs(pair_info['current_z_score'])\n",
    "                    position_size = min(1.0, z_score / self.trading_params['z_entry'])\n",
    "                except KeyError:\n",
    "                    position_size = 0.5  # Valor por defecto\n",
    "                \n",
    "                daily_returns.append(pair_return)\n",
    "                position_sizes.append(position_size)\n",
    "            \n",
    "            # Calcular retorno ponderado de la cartera\n",
    "            if not daily_returns:\n",
    "                return 0.0\n",
    "                \n",
    "            if sum(position_sizes) > 0:\n",
    "                weighted_return = sum(r * s for r, s in zip(daily_returns, position_sizes)) / sum(position_sizes)\n",
    "            else:\n",
    "                weighted_return = np.mean(daily_returns)\n",
    "            \n",
    "            # Aplicar costos de transacción\n",
    "            # Implementar modelo de costos más realista\n",
    "            spread_cost = 0.0005  # 5 bps de spread\n",
    "            market_impact = 0.0010 * (len(active_pairs) > self.max_active_pairs / 2)  # 10 bps adicionales para cambios grandes\n",
    "            transaction_costs = len(active_pairs) * (spread_cost + market_impact) / max(1, len(active_pairs))\n",
    "            \n",
    "            net_return = weighted_return - transaction_costs\n",
    "            \n",
    "            debug_logger.info(f\"Retorno diario: {net_return:.4%} (bruto: {weighted_return:.4%}, costos: {transaction_costs:.4%})\")\n",
    "            \n",
    "            return net_return\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al calcular retornos: {str(e)}\", exc_info=True)\n",
    "            return 0.0\n",
    "    \n",
    "    def backtest(self):\n",
    "        \"\"\"\n",
    "        Ejecuta el backtest de la estrategia.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Resultados del backtest\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Iniciando backtest...\")\n",
    "            \n",
    "            # Verificar datos\n",
    "            if self.price_data is None:\n",
    "                if not self.load_data():\n",
    "                    return pd.DataFrame()\n",
    "            \n",
    "            # Inicializar resultados\n",
    "            results = pd.DataFrame(index=self.price_data.index)\n",
    "            results['return'] = 0.0\n",
    "            results['equity'] = 1.0\n",
    "            results['regime'] = np.nan\n",
    "            results['active_pairs'] = 0\n",
    "            \n",
    "            # Inicializar variables\n",
    "            active_pairs = {}\n",
    "            previous_regime = 1  # Neutral por defecto\n",
    "            performance_history = {}\n",
    "            \n",
    "            # Definir fecha de inicio efectiva (después del período de lookback)\n",
    "            effective_start = self.start_date + pd.Timedelta(days=self.trading_params['pair_lookback'])\n",
    "            \n",
    "            # Filtrar fechas de trading\n",
    "            trading_dates = self.price_data.index[self.price_data.index >= effective_start]\n",
    "            \n",
    "            # Verificar que tenemos suficientes datos\n",
    "            if len(trading_dates) < 2:\n",
    "                print(\"Datos insuficientes para backtest\")\n",
    "                return pd.DataFrame()\n",
    "                \n",
    "            print(f\"Ejecutando backtest desde {trading_dates[0]} hasta {trading_dates[-1]}\")\n",
    "            debug_logger.info(f\"Iniciando backtest con {len(trading_dates)} días de trading\")\n",
    "            \n",
    "            # Ejecutar backtest\n",
    "            for i, current_date in enumerate(tqdm(trading_dates)):\n",
    "                # Saltar primer día (necesitamos día anterior para calcular retornos)\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                \n",
    "                previous_date = trading_dates[i-1]\n",
    "                \n",
    "                # Detectar régimen de mercado\n",
    "                current_regime = self.detect_market_regime(current_date, i)\n",
    "                results.loc[current_date, 'regime'] = current_regime\n",
    "                \n",
    "                # Actualizar parámetros de trading\n",
    "                self.trading_params = self.update_trading_parameters(\n",
    "                    current_regime, previous_regime, performance_history\n",
    "                )\n",
    "                \n",
    "                # Rebalancear cartera periódicamente\n",
    "                if i % self.trading_params['rebalance_frequency'] == 0:\n",
    "                    # Encontrar nuevos pares\n",
    "                    available_pairs = self.find_cointegrated_pairs(current_date)\n",
    "                    debug_logger.info(f\"Encontrados {len(available_pairs)} pares para fecha {current_date}\")\n",
    "                    \n",
    "                    # Gestionar cartera\n",
    "                    active_pairs = self.manage_portfolio(current_date, active_pairs, available_pairs)\n",
    "                    debug_logger.info(f\"Cartera actualizada: {len(active_pairs)} pares activos\")\n",
    "                \n",
    "                # Calcular retornos diarios\n",
    "                daily_return = self.calculate_portfolio_returns(active_pairs, current_date, previous_date)\n",
    "                results.loc[current_date, 'return'] = daily_return\n",
    "                \n",
    "                # Actualizar equity\n",
    "                if i > 0:\n",
    "                    results.loc[current_date, 'equity'] = results.loc[previous_date, 'equity'] * (1 + daily_return)\n",
    "                \n",
    "                # Guardar número de pares activos\n",
    "                results.loc[current_date, 'active_pairs'] = len(active_pairs)\n",
    "                \n",
    "                # Actualizar historial de rendimiento\n",
    "                performance_history[current_date] = daily_return\n",
    "                \n",
    "                # Actualizar régimen anterior\n",
    "                previous_regime = current_regime\n",
    "                \n",
    "                # Mostrar progreso periódicamente\n",
    "                if i % 100 == 0:\n",
    "                    current_equity = results.loc[current_date, 'equity']\n",
    "                    print(f\"Fecha: {current_date}, Equity: {current_equity:.4f}, Pares activos: {len(active_pairs)}\")\n",
    "            \n",
    "            # Calcular métricas de rendimiento\n",
    "            self.calculate_performance_metrics(results)\n",
    "            \n",
    "            # Guardar resultados\n",
    "            results.to_csv('./artifacts/results/data/backtest_results.csv')\n",
    "            \n",
    "            # Generar gráficos\n",
    "            self.plot_results(results)\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en backtest: {str(e)}\", exc_info=True)\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def walk_forward_analysis(self, window_size=252, step_size=63):\n",
    "        \"\"\"\n",
    "        Realiza análisis walk-forward para evaluar la robustez de la estrategia.\n",
    "        \n",
    "        Args:\n",
    "            window_size (int): Tamaño de la ventana de análisis en días\n",
    "            step_size (int): Tamaño del paso entre ventanas en días\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Resultados del análisis walk-forward\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Verificar datos\n",
    "            if self.price_data is None:\n",
    "                if not self.load_data():\n",
    "                    return pd.DataFrame()\n",
    "            \n",
    "            # Definir ventanas de análisis\n",
    "            trading_dates = self.price_data.index[self.price_data.index >= self.start_date]\n",
    "            \n",
    "            if len(trading_dates) < window_size + step_size:\n",
    "                raise ValueError(\"Datos insuficientes para análisis walk-forward\")\n",
    "            \n",
    "            windows = []\n",
    "            for i in range(0, len(trading_dates) - window_size, step_size):\n",
    "                train_start = trading_dates[i]\n",
    "                test_start = trading_dates[i + window_size - step_size]\n",
    "                test_end = trading_dates[min(i + window_size, len(trading_dates) - 1)]\n",
    "                \n",
    "                windows.append({\n",
    "                    'train_start': train_start,\n",
    "                    'test_start': test_start,\n",
    "                    'test_end': test_end\n",
    "                })\n",
    "            \n",
    "            # Inicializar resultados\n",
    "            wf_results = pd.DataFrame()\n",
    "            \n",
    "            print(f\"Ejecutando análisis walk-forward con {len(windows)} ventanas\")\n",
    "            \n",
    "            # Ejecutar análisis para cada ventana\n",
    "            for window_idx, window in enumerate(tqdm(windows)):\n",
    "                debug_logger.info(f\"Procesando ventana {window_idx+1}/{len(windows)}: {window['train_start']} - {window['test_end']}\")\n",
    "                \n",
    "                # Filtrar datos para la ventana actual\n",
    "                window_data = self.price_data.loc[window['train_start']:window['test_end']]\n",
    "                \n",
    "                # Encontrar pares cointegrados usando datos de entrenamiento\n",
    "                train_end = window['test_start'] - pd.Timedelta(days=1)\n",
    "                cointegrated_pairs = self.find_cointegrated_pairs(train_end)\n",
    "                \n",
    "                debug_logger.info(f\"Encontrados {len(cointegrated_pairs)} pares cointegrados para ventana {window_idx+1}\")\n",
    "                \n",
    "                # Evaluar pares en datos de prueba\n",
    "                window_returns = []\n",
    "                \n",
    "                for pair_idx, pair in enumerate(cointegrated_pairs[:self.max_active_pairs]):\n",
    "                    ticker1, ticker2 = pair['ticker1'], pair['ticker2']\n",
    "                    beta = pair['beta']\n",
    "                    \n",
    "                    # Verificar datos disponibles\n",
    "                    if ticker1 not in window_data.columns or ticker2 not in window_data.columns:\n",
    "                        continue\n",
    "                    \n",
    "                    debug_logger.info(f\"Evaluando par {pair_idx+1}: {ticker1}-{ticker2} (beta={beta:.4f})\")\n",
    "                    \n",
    "                    price_data = {\n",
    "                        ticker1: window_data[ticker1],\n",
    "                        ticker2: window_data[ticker2]\n",
    "                    }\n",
    "                    \n",
    "                    # Generar señales usando solo datos hasta test_start\n",
    "                    signals = self.generate_signals(ticker1, ticker2, beta, \n",
    "                                                  {k: v[v.index <= window['test_start']] for k, v in price_data.items()},\n",
    "                                                  window['test_start'], lookback=window_size)\n",
    "                    \n",
    "                    # Filtrar señales para período de prueba\n",
    "                    test_signals = signals.loc[window['test_start']:window['test_end']] if not signals.empty else pd.DataFrame()\n",
    "                    \n",
    "                    if not test_signals.empty and 'pair_return' in test_signals.columns:\n",
    "                        window_returns.append(test_signals['pair_return'])\n",
    "                        debug_logger.info(f\"Par {ticker1}-{ticker2} añadido con {len(test_signals)} señales\")\n",
    "                    else:\n",
    "                        debug_logger.info(f\"Sin señales válidas para par {ticker1}-{ticker2}\")\n",
    "                \n",
    "                # Calcular retorno combinado para la ventana\n",
    "                if window_returns:\n",
    "                    combined_return = pd.concat(window_returns, axis=1).mean(axis=1)\n",
    "                    combined_return.name = 'return'\n",
    "                    \n",
    "                    # Añadir información de la ventana\n",
    "                    combined_return = pd.DataFrame(combined_return)\n",
    "                    combined_return['window_start'] = window['test_start']\n",
    "                    combined_return['window_end'] = window['test_end']\n",
    "                    \n",
    "                    # Añadir a resultados globales\n",
    "                    wf_results = pd.concat([wf_results, combined_return])\n",
    "                    \n",
    "                    # Calcular retorno total para la ventana\n",
    "                    window_equity = (1 + combined_return['return']).cumprod().iloc[-1] - 1 if len(combined_return) > 0 else 0\n",
    "                    print(f\"Ventana {window_idx+1}: {window['test_start']} - {window['test_end']}, Retorno: {window_equity:.4%}\")\n",
    "                else:\n",
    "                    debug_logger.info(f\"Sin retornos para ventana {window_idx+1}\")\n",
    "            \n",
    "            # Calcular equity curve\n",
    "            if not wf_results.empty:\n",
    "                wf_results['equity'] = (1 + wf_results['return']).cumprod()\n",
    "                \n",
    "                # Guardar resultados\n",
    "                wf_results.to_csv('./artifacts/results/data/walk_forward_results.csv')\n",
    "                \n",
    "                # Generar gráficos\n",
    "                self.plot_walk_forward_results(wf_results)\n",
    "            \n",
    "            return wf_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en análisis walk-forward: {str(e)}\", exc_info=True)\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def calculate_performance_metrics(self, results):\n",
    "        \"\"\"\n",
    "        Calcula métricas de rendimiento para los resultados del backtest.\n",
    "        \n",
    "        Args:\n",
    "            results (pd.DataFrame): Resultados del backtest\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Verificar datos suficientes\n",
    "            if results.empty or 'return' not in results.columns:\n",
    "                return\n",
    "            \n",
    "            # Calcular métricas básicas\n",
    "            returns = results['return'].dropna()\n",
    "            \n",
    "            if len(returns) < 10:\n",
    "                return\n",
    "            \n",
    "            # Retorno total\n",
    "            total_return = results['equity'].iloc[-1] / results['equity'].iloc[0] - 1\n",
    "            \n",
    "            # Retorno anualizado\n",
    "            years = len(returns) / 252\n",
    "            annual_return = (1 + total_return) ** (1 / years) - 1\n",
    "            \n",
    "            # Volatilidad anualizada\n",
    "            annual_vol = returns.std() * np.sqrt(252)\n",
    "            \n",
    "            # Sharpe ratio\n",
    "            risk_free_rate = 0.02  # Tasa libre de riesgo (2%)\n",
    "            sharpe_ratio = (annual_return - risk_free_rate) / annual_vol if annual_vol > 0 else 0\n",
    "            \n",
    "            # Sortino ratio\n",
    "            downside_returns = returns[returns < 0]\n",
    "            downside_vol = downside_returns.std() * np.sqrt(252) if len(downside_returns) > 0 else 0\n",
    "            sortino_ratio = (annual_return - risk_free_rate) / downside_vol if downside_vol > 0 else 0\n",
    "            \n",
    "            # Máximo drawdown\n",
    "            equity_curve = results['equity']\n",
    "            rolling_max = equity_curve.cummax()\n",
    "            drawdown = (equity_curve / rolling_max - 1)\n",
    "            max_drawdown = drawdown.min()\n",
    "            \n",
    "            # Calmar ratio\n",
    "            calmar_ratio = annual_return / abs(max_drawdown) if max_drawdown < 0 else 0\n",
    "            \n",
    "            # Métricas por régimen\n",
    "            regime_metrics = {}\n",
    "            for regime in results['regime'].dropna().unique():\n",
    "                regime_returns = returns[results['regime'] == regime]\n",
    "                if len(regime_returns) > 10:\n",
    "                    regime_metrics[int(regime)] = {\n",
    "                        'return': regime_returns.mean() * 252,\n",
    "                        'volatility': regime_returns.std() * np.sqrt(252),\n",
    "                        'sharpe': regime_returns.mean() / regime_returns.std() * np.sqrt(252) if regime_returns.std() > 0 else 0,\n",
    "                        'count': len(regime_returns)\n",
    "                    }\n",
    "            \n",
    "            # Guardar métricas\n",
    "            self.performance_metrics = {\n",
    "                'total_return': total_return,\n",
    "                'annual_return': annual_return,\n",
    "                'annual_volatility': annual_vol,\n",
    "                'sharpe_ratio': sharpe_ratio,\n",
    "                'sortino_ratio': sortino_ratio,\n",
    "                'max_drawdown': max_drawdown,\n",
    "                'calmar_ratio': calmar_ratio,\n",
    "                'win_rate': len(returns[returns > 0]) / len(returns),\n",
    "                'regime_metrics': regime_metrics\n",
    "            }\n",
    "            \n",
    "            # Guardar métricas en archivo\n",
    "            metrics_df = pd.DataFrame({k: [v] for k, v in self.performance_metrics.items() \n",
    "                                     if k != 'regime_metrics'})\n",
    "            metrics_df.to_csv('./artifacts/results/data/performance_metrics.csv', index=False)\n",
    "            \n",
    "            # Guardar métricas por régimen\n",
    "            if regime_metrics:\n",
    "                regime_df = pd.DataFrame.from_dict(\n",
    "                    {f\"Regime {k}\": v for k, v in regime_metrics.items()}, \n",
    "                    orient='index'\n",
    "                )\n",
    "                regime_df.to_csv('./artifacts/results/data/regime_metrics.csv')\n",
    "            \n",
    "            # Imprimir métricas\n",
    "            print(\"\\nMétricas de Rendimiento:\")\n",
    "            print(f\"Retorno Total: {total_return:.2%}\")\n",
    "            print(f\"Retorno Anualizado: {annual_return:.2%}\")\n",
    "            print(f\"Volatilidad Anualizada: {annual_vol:.2%}\")\n",
    "            print(f\"Sharpe Ratio: {sharpe_ratio:.2f}\")\n",
    "            print(f\"Sortino Ratio: {sortino_ratio:.2f}\")\n",
    "            print(f\"Máximo Drawdown: {max_drawdown:.2%}\")\n",
    "            print(f\"Calmar Ratio: {calmar_ratio:.2f}\")\n",
    "            print(f\"Win Rate: {self.performance_metrics['win_rate']:.2%}\")\n",
    "            \n",
    "            # Guardar estadísticas extendidas\n",
    "            stats_file = open('./artifacts/results/data/detailed_stats.txt', 'w')\n",
    "            stats_file.write(\"ESTADÍSTICAS DETALLADAS DE LA ESTRATEGIA\\n\")\n",
    "            stats_file.write(\"=====================================\\n\\n\")\n",
    "            stats_file.write(f\"Período: {results.index[0]} a {results.index[-1]}\\n\")\n",
    "            stats_file.write(f\"Días de trading: {len(returns)}\\n\")\n",
    "            stats_file.write(f\"Retorno total: {total_return:.2%}\\n\")\n",
    "            stats_file.write(f\"Retorno anualizado: {annual_return:.2%}\\n\")\n",
    "            stats_file.write(f\"Volatilidad anualizada: {annual_vol:.2%}\\n\")\n",
    "            stats_file.write(f\"Sharpe ratio: {sharpe_ratio:.2f}\\n\")\n",
    "            stats_file.write(f\"Sortino ratio: {sortino_ratio:.2f}\\n\")\n",
    "            stats_file.write(f\"Máximo drawdown: {max_drawdown:.2%}\\n\")\n",
    "            stats_file.write(f\"Calmar ratio: {calmar_ratio:.2f}\\n\")\n",
    "            stats_file.write(f\"Win rate: {self.performance_metrics['win_rate']:.2%}\\n\\n\")\n",
    "            \n",
    "            # Añadir estadísticas por régimen\n",
    "            stats_file.write(\"ESTADÍSTICAS POR RÉGIMEN\\n\")\n",
    "            stats_file.write(\"========================\\n\\n\")\n",
    "            for regime, stats in regime_metrics.items():\n",
    "                stats_file.write(f\"Régimen {regime}:\\n\")\n",
    "                stats_file.write(f\"  Días: {stats['count']}\\n\")\n",
    "                stats_file.write(f\"  Retorno anualizado: {stats['return']:.2%}\\n\")\n",
    "                stats_file.write(f\"  Volatilidad anualizada: {stats['volatility']:.2%}\\n\")\n",
    "                stats_file.write(f\"  Sharpe ratio: {stats['sharpe']:.2f}\\n\\n\")\n",
    "                \n",
    "            stats_file.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al calcular métricas: {str(e)}\", exc_info=True)\n",
    "    \n",
    "    def plot_results(self, results):\n",
    "        \"\"\"\n",
    "        Genera gráficos para visualizar los resultados del backtest.\n",
    "        \n",
    "        Args:\n",
    "            results (pd.DataFrame): Resultados del backtest\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if results.empty:\n",
    "                return\n",
    "            \n",
    "            # Configurar estilo\n",
    "            plt.style.use('seaborn-v0_8-darkgrid')\n",
    "            \n",
    "            # 1. Equity curve\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.plot(results.index, results['equity'], linewidth=2)\n",
    "            plt.title('Equity Curve', fontsize=14)\n",
    "            plt.xlabel('Fecha')\n",
    "            plt.ylabel('Equity')\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('./artifacts/results/figures/equity_curve.png')\n",
    "            plt.close()\n",
    "            \n",
    "            # 2. Drawdown\n",
    "            equity_curve = results['equity']\n",
    "            rolling_max = equity_curve.cummax()\n",
    "            drawdown = (equity_curve / rolling_max - 1)\n",
    "            \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.plot(results.index, drawdown, linewidth=2, color='red')\n",
    "            plt.title('Drawdown', fontsize=14)\n",
    "            plt.xlabel('Fecha')\n",
    "            plt.ylabel('Drawdown')\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('./artifacts/results/figures/drawdown.png')\n",
    "            plt.close()\n",
    "            \n",
    "            # 3. Retornos mensuales\n",
    "            if len(results) > 30:\n",
    "                monthly_returns = results['return'].resample('M').apply(\n",
    "                    lambda x: (1 + x).prod() - 1\n",
    "                )\n",
    "                \n",
    "                plt.figure(figsize=(14, 7))\n",
    "                monthly_returns.plot(kind='bar', color=np.where(monthly_returns >= 0, 'green', 'red'))\n",
    "                plt.title('Retornos Mensuales', fontsize=14)\n",
    "                plt.xlabel('Fecha')\n",
    "                plt.ylabel('Retorno')\n",
    "                plt.grid(True, axis='y')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig('./artifacts/results/figures/monthly_returns.png')\n",
    "                plt.close()\n",
    "            \n",
    "            # 4. Rendimiento por régimen\n",
    "            if 'regime' in results.columns and not results['regime'].isna().all():\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                \n",
    "                for regime in sorted(results['regime'].dropna().unique()):\n",
    "                    regime_equity = results[results['regime'] == regime]['equity']\n",
    "                    if not regime_equity.empty:\n",
    "                        # Normalizar equity para cada régimen\n",
    "                        normalized_equity = regime_equity / regime_equity.iloc[0]\n",
    "                        plt.plot(regime_equity.index, normalized_equity, \n",
    "                                label=f'Régimen {int(regime)}', linewidth=2)\n",
    "                \n",
    "                plt.title('Rendimiento por Régimen de Mercado', fontsize=14)\n",
    "                plt.xlabel('Fecha')\n",
    "                plt.ylabel('Equity Normalizada')\n",
    "                plt.legend()\n",
    "                plt.grid(True)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig('./artifacts/results/figures/regime_performance.png')\n",
    "                plt.close()\n",
    "            \n",
    "            # 5. Número de pares activos\n",
    "            if 'active_pairs' in results.columns:\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                plt.plot(results.index, results['active_pairs'], linewidth=2, color='purple')\n",
    "                plt.title('Número de Pares Activos', fontsize=14)\n",
    "                plt.xlabel('Fecha')\n",
    "                plt.ylabel('Pares Activos')\n",
    "                plt.grid(True)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig('./artifacts/results/figures/active_pairs.png')\n",
    "                plt.close()\n",
    "            \n",
    "            # 6. Distribución de retornos diarios\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.histplot(results['return'].dropna(), kde=True, bins=50)\n",
    "            plt.title('Distribución de Retornos Diarios', fontsize=14)\n",
    "            plt.xlabel('Retorno')\n",
    "            plt.ylabel('Frecuencia')\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('./artifacts/results/figures/return_distribution.png')\n",
    "            plt.close()\n",
    "            \n",
    "            # 7. Retornos acumulados anuales\n",
    "            annual_returns = results['return'].resample('Y').apply(\n",
    "                lambda x: (1 + x).prod() - 1\n",
    "            )\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            annual_returns.plot(kind='bar', color=np.where(annual_returns >= 0, 'green', 'red'))\n",
    "            plt.title('Retornos Anuales', fontsize=14)\n",
    "            plt.xlabel('Año')\n",
    "            plt.ylabel('Retorno')\n",
    "            plt.grid(True, axis='y')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('./artifacts/results/figures/annual_returns.png')\n",
    "            plt.close()\n",
    "            \n",
    "            # 8. Matriz de correlación de rendimientos por régimen\n",
    "            if 'regime' in results.columns and not results['regime'].isna().all():\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                \n",
    "                # Crear DataFrame de retornos por régimen\n",
    "                regime_returns = pd.DataFrame()\n",
    "                \n",
    "                for regime in sorted(results['regime'].dropna().unique()):\n",
    "                    regime_name = f'Regime_{int(regime)}'\n",
    "                    regime_returns[regime_name] = results.loc[results['regime'] == regime, 'return']\n",
    "                \n",
    "                # Calcular correlación solo si hay suficientes datos\n",
    "                if not regime_returns.empty and all(regime_returns.count() > 5):\n",
    "                    corr_matrix = regime_returns.corr()\n",
    "                    \n",
    "                    # Crear mapa de calor\n",
    "                    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, \n",
    "                               linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "                    plt.title('Correlación de Retornos por Régimen', fontsize=14)\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig('./artifacts/results/figures/regime_correlation.png')\n",
    "                    plt.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al generar gráficos: {str(e)}\", exc_info=True)\n",
    "    \n",
    "    def plot_walk_forward_results(self, results):\n",
    "        \"\"\"\n",
    "        Genera gráficos para visualizar los resultados del análisis walk-forward.\n",
    "        \n",
    "        Args:\n",
    "            results (pd.DataFrame): Resultados del análisis walk-forward\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if results.empty:\n",
    "                return\n",
    "            \n",
    "            # Configurar estilo\n",
    "            plt.style.use('seaborn-v0_8-darkgrid')\n",
    "            \n",
    "            # 1. Equity curve combinada\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.plot(results.index, results['equity'], linewidth=2)\n",
    "            plt.title('Walk-Forward Equity Curve', fontsize=14)\n",
    "            plt.xlabel('Fecha')\n",
    "            plt.ylabel('Equity')\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('./artifacts/results/figures/wf_equity_curve.png')\n",
    "            plt.close()\n",
    "            \n",
    "            # 2. Retornos por ventana\n",
    "            if 'window_start' in results.columns:\n",
    "                window_groups = results.groupby('window_start')\n",
    "                \n",
    "                plt.figure(figsize=(14, 7))\n",
    "                \n",
    "                for window_start, group in window_groups:\n",
    "                    window_equity = (1 + group['return']).cumprod()\n",
    "                    plt.plot(group.index, window_equity, \n",
    "                            label=f'Ventana {window_start.strftime(\"%Y-%m-%d\")}',\n",
    "                            alpha=0.7)\n",
    "                \n",
    "                plt.title('Rendimiento por Ventana de Walk-Forward', fontsize=14)\n",
    "                plt.xlabel('Fecha')\n",
    "                plt.ylabel('Equity')\n",
    "                plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "                plt.grid(True)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig('./artifacts/results/figures/wf_window_performance.png')\n",
    "                plt.close()\n",
    "            \n",
    "            # 3. Distribución de retornos por ventana\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            window_returns = []\n",
    "            window_labels = []\n",
    "            \n",
    "            for window_start, group in results.groupby('window_start'):\n",
    "                window_return = (1 + group['return']).prod() - 1\n",
    "                window_returns.append(window_return)\n",
    "                window_labels.append(window_start.strftime(\"%Y-%m-%d\"))\n",
    "            \n",
    "            plt.bar(window_labels, window_returns, color=np.where(np.array(window_returns) >= 0, 'green', 'red'))\n",
    "            plt.title('Retorno Total por Ventana', fontsize=14)\n",
    "            plt.xlabel('Fecha de Inicio de Ventana')\n",
    "            plt.ylabel('Retorno Total')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.grid(True, axis='y')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('./artifacts/results/figures/wf_window_returns.png')\n",
    "            plt.close()\n",
    "            \n",
    "            # 4. Estadísticas de consistencia entre ventanas\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            # Calcular retornos promedio diarios por ventana\n",
    "            daily_returns = []\n",
    "            labels = []\n",
    "            \n",
    "            for window_start, group in results.groupby('window_start'):\n",
    "                avg_return = group['return'].mean() * 252  # Anualizado\n",
    "                daily_returns.append(avg_return)\n",
    "                labels.append(window_start.strftime(\"%Y-%m-%d\"))\n",
    "            \n",
    "            plt.bar(labels, daily_returns, color=np.where(np.array(daily_returns) >= 0, 'green', 'red'))\n",
    "            plt.axhline(y=np.mean(daily_returns), color='blue', linestyle='-', label=f'Promedio: {np.mean(daily_returns):.2%}')\n",
    "            plt.title('Retorno Anualizado Promedio por Ventana', fontsize=14)\n",
    "            plt.xlabel('Fecha de Inicio de Ventana')\n",
    "            plt.ylabel('Retorno Anualizado')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.legend()\n",
    "            plt.grid(True, axis='y')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('./artifacts/results/figures/wf_window_avg_returns.png')\n",
    "            plt.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al generar gráficos de walk-forward: {str(e)}\", exc_info=True)\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal para ejecutar la estrategia.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Iniciando Estrategia de Arbitraje Estadístico Multi-Régimen\")\n",
    "        print(\"=========================================================\")\n",
    "        \n",
    "        # Inicializar estrategia\n",
    "        strategy = StatisticalArbitrageStrategy(\n",
    "            start_date='2024-01-01',\n",
    "            end_date='2025-12-01',\n",
    "            universe_size=100,\n",
    "            max_active_pairs=15\n",
    "        )\n",
    "        \n",
    "        # Cargar datos\n",
    "        if not strategy.load_data():\n",
    "            print(\"Error al cargar datos. Verificar logs.\")\n",
    "            return\n",
    "        \n",
    "        # Ejecutar backtest\n",
    "        print(\"\\nEjecutando backtest...\")\n",
    "        backtest_results = strategy.backtest()\n",
    "        \n",
    "        if backtest_results.empty:\n",
    "            print(\"Error en backtest. Verificar logs.\")\n",
    "            return\n",
    "        \n",
    "        # Ejecutar análisis walk-forward\n",
    "        # print(\"\\nEjecutando análisis walk-forward...\")\n",
    "        # wf_results = strategy.walk_forward_analysis(window_size=252, step_size=63)\n",
    "        \n",
    "        # if wf_results.empty:\n",
    "        #    print(\"Error en análisis walk-forward. Verificar logs.\")\n",
    "        \n",
    "        print(\"\\nAnálisis completado. Resultados guardados en './artifacts/results/'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en ejecución principal: {str(e)}\", exc_info=True)\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f520d9b-a416-4c80-9d6e-273c719c4c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ce288f-2baa-42dc-aa8b-eef5f91cb896",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
