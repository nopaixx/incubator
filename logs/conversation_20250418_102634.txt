

--- ideador (STEP 1.1) ---
INPUT 1 (user):
estrategia de inversion autonoma para batir el sp500 con beta cercano a 0 y el mejor alpha, maximizando beneficio y reduciendo riesgo

RESPONSE:
# Estrategia de Inversión Cuantitativa: Rotación Multi-Factor con Beta Neutro Adaptativo

## Concepto Principal

Propongo una estrategia de rotación multi-factor con neutralización dinámica de beta que combina señales fundamentales, técnicas y de sentimiento para construir una cartera market-neutral con exposición al S&P 500 pero manteniendo un beta cercano a cero mientras se maximiza el alpha.

## Fundamentos Matemáticos/Estadísticos

La estrategia se basa en un marco de optimización bayesiana multi-objetivo que equilibra:

1. **Modelo de Selección de Activos**: Utilizando un ensamble de modelos predictivos (Random Forest, Gradient Boosting y Redes Neuronales) para estimar los retornos esperados a 1-3 meses.

2. **Neutralización Beta Dinámica**: Implementando un modelo GARCH multivariado para estimar la matriz de covarianzas condicional entre los activos y el índice S&P 500, permitiendo ajustar dinámicamente las posiciones para mantener un beta objetivo cercano a cero.

3. **Optimización de Cartera**: Empleando un algoritmo de optimización de Black-Litterman modificado que incorpora vistas derivadas del modelo de selección y restricciones de beta neutro.

## Funcionamiento Detallado

### 1. Generación de Señales Multi-Factor

**Factores Fundamentales**:
- Quality: ROE, margen operativo, estabilidad de ganancias
- Value: P/E relativo al sector, EV/EBITDA, FCF yield
- Growth: Crecimiento de ingresos y ganancias, sorpresas de beneficios

**Factores Técnicos**:
- Momentum adaptativo: Retornos relativos ajustados por volatilidad en múltiples horizontes (1, 3, 6 meses)
- Reversión a la media: Desviaciones significativas de las medias móviles ponderadas exponencialmente
- Patrones de volatilidad: Compresión/expansión de volatilidad relativa

**Factores de Sentimiento y Alternativos**:
- Análisis de flujo de órdenes institucionales
- Sentimiento derivado de informes de analistas y revisiones de estimaciones
- Posicionamiento de opciones (put/call ratio, skew)

### 2. Sistema de Ponderación Adaptativa

El sistema utiliza un algoritmo de aprendizaje por refuerzo que ajusta dinámicamente los pesos de cada factor basándose en:

- Su rendimiento reciente (ventanas temporales múltiples)
- Correlación con otros factores
- Régimen de mercado actual identificado mediante un modelo de Markov oculto (HMM)

### 3. Construcción de Cartera Beta-Neutral

- Posiciones largas: Selección de los activos del S&P 500 con mejores puntuaciones combinadas
- Posiciones cortas: ETF del S&P 500 (SPY) calibrado para neutralizar beta + activos con puntuaciones más bajas
- Rebalanceo: Semanal con ajustes intra-semana si el beta se desvía más de 0.1 del objetivo

### 4. Gestión de Riesgo Dinámica

- Control de exposición sectorial: Neutralización de exposiciones sectoriales excesivas
- Límites de concentración adaptativos: Basados en liquidez y volatilidad de cada activo
- Stops dinámicos: Basados en la volatilidad implícita y el VaR condicional
- Monitoreo de correlaciones: Ajustes cuando las correlaciones entre activos cambian significativamente

## Métricas y Timeframes

- **Horizonte de inversión principal**: 1-3 meses
- **Frecuencia de rebalanceo**: Semanal con ajustes condicionales
- **Métricas de rendimiento objetivo**:
  - Ratio de Sharpe > 1.5
  - Beta objetivo: 0 ± 0.1
  - Alpha anualizado > 5%
  - Maximum drawdown < 10%
  - Ratio de Sortino > 2.0

## Ventajas Potenciales

1. **Robustez a diferentes regímenes de mercado**: La ponderación adaptativa de factores permite ajustarse a cambios en las condiciones de mercado.

2. **Baja correlación con índices**: El enfoque beta-neutral proporciona verdadera diversificación en carteras tradicionales.

3. **Gestión eficiente del riesgo**: El sistema de stops dinámicos y la diversificación multi-factor reducen el riesgo de cola.

4. **Escalabilidad**: El enfoque se puede aplicar a diferentes tamaños de capital con ajustes en los límites de concentración.

5. **Adaptabilidad**: El marco bayesiano permite incorporar nuevas señales o factores a medida que se descubren.

La estrategia está diseñada para operar como un sistema autónomo que requiere mínima intervención humana, con capacidad de auto-calibración basada en el rendimiento pasado y las condiciones de mercado actuales.


--- curador (STEP 2.1) ---
INPUT 1 (user):
# Estrategia de Inversión Cuantitativa: Rotación Multi-Factor con Beta Neutro Adaptativo

## Concepto Principal

Propongo una estrategia de rotación multi-factor con neutralización dinámica de beta que combina señales fundamentales, técnicas y de sentimiento para construir una cartera market-neutral con exposición al S&P 500 pero manteniendo un beta cercano a cero mientras se maximiza el alpha.

## Fundamentos Matemáticos/Estadísticos

La estrategia se basa en un marco de optimización bayesiana multi-objetivo que equilibra:

1. **Modelo de Selección de Activos**: Utilizando un ensamble de modelos predictivos (Random Forest, Gradient Boosting y Redes Neuronales) para estimar los retornos esperados a 1-3 meses.

2. **Neutralización Beta Dinámica**: Implementando un modelo GARCH multivariado para estimar la matriz de covarianzas condicional entre los activos y el índice S&P 500, permitiendo ajustar dinámicamente las posiciones para mantener un beta objetivo cercano a cero.

3. **Optimización de Cartera**: Empleando un algoritmo de optimización de Black-Litterman modificado que incorpora vistas derivadas del modelo de selección y restricciones de beta neutro.

## Funcionamiento Detallado

### 1. Generación de Señales Multi-Factor

**Factores Fundamentales**:
- Quality: ROE, margen operativo, estabilidad de ganancias
- Value: P/E relativo al sector, EV/EBITDA, FCF yield
- Growth: Crecimiento de ingresos y ganancias, sorpresas de beneficios

**Factores Técnicos**:
- Momentum adaptativo: Retornos relativos ajustados por volatilidad en múltiples horizontes (1, 3, 6 meses)
- Reversión a la media: Desviaciones significativas de las medias móviles ponderadas exponencialmente
- Patrones de volatilidad: Compresión/expansión de volatilidad relativa

**Factores de Sentimiento y Alternativos**:
- Análisis de flujo de órdenes institucionales
- Sentimiento derivado de informes de analistas y revisiones de estimaciones
- Posicionamiento de opciones (put/call ratio, skew)

### 2. Sistema de Ponderación Adaptativa

El sistema utiliza un algoritmo de aprendizaje por refuerzo que ajusta dinámicamente los pesos de cada factor basándose en:

- Su rendimiento reciente (ventanas temporales múltiples)
- Correlación con otros factores
- Régimen de mercado actual identificado mediante un modelo de Markov oculto (HMM)

### 3. Construcción de Cartera Beta-Neutral

- Posiciones largas: Selección de los activos del S&P 500 con mejores puntuaciones combinadas
- Posiciones cortas: ETF del S&P 500 (SPY) calibrado para neutralizar beta + activos con puntuaciones más bajas
- Rebalanceo: Semanal con ajustes intra-semana si el beta se desvía más de 0.1 del objetivo

### 4. Gestión de Riesgo Dinámica

- Control de exposición sectorial: Neutralización de exposiciones sectoriales excesivas
- Límites de concentración adaptativos: Basados en liquidez y volatilidad de cada activo
- Stops dinámicos: Basados en la volatilidad implícita y el VaR condicional
- Monitoreo de correlaciones: Ajustes cuando las correlaciones entre activos cambian significativamente

## Métricas y Timeframes

- **Horizonte de inversión principal**: 1-3 meses
- **Frecuencia de rebalanceo**: Semanal con ajustes condicionales
- **Métricas de rendimiento objetivo**:
  - Ratio de Sharpe > 1.5
  - Beta objetivo: 0 ± 0.1
  - Alpha anualizado > 5%
  - Maximum drawdown < 10%
  - Ratio de Sortino > 2.0

## Ventajas Potenciales

1. **Robustez a diferentes regímenes de mercado**: La ponderación adaptativa de factores permite ajustarse a cambios en las condiciones de mercado.

2. **Baja correlación con índices**: El enfoque beta-neutral proporciona verdadera diversificación en carteras tradicionales.

3. **Gestión eficiente del riesgo**: El sistema de stops dinámicos y la diversificación multi-factor reducen el riesgo de cola.

4. **Escalabilidad**: El enfoque se puede aplicar a diferentes tamaños de capital con ajustes en los límites de concentración.

5. **Adaptabilidad**: El marco bayesiano permite incorporar nuevas señales o factores a medida que se descubren.

La estrategia está diseñada para operar como un sistema autónomo que requiere mínima intervención humana, con capacidad de auto-calibración basada en el rendimiento pasado y las condiciones de mercado actuales.

RESPONSE:
# Análisis Crítico de la Estrategia de Rotación Multi-Factor con Beta Neutro Adaptativo

## Fortalezas Identificadas
- El enfoque multi-factor diversificado puede proporcionar robustez
- La neutralización beta dinámica es conceptualmente sólida para generar alpha puro
- El marco de optimización bayesiana multi-objetivo es sofisticado y bien fundamentado
- La incorporación de múltiples horizontes temporales puede capturar diferentes ineficiencias

## Preocupaciones Críticas

### 1. Complejidad vs. Implementación
La estrategia propuesta es extremadamente compleja, lo que presenta varios desafíos:
- Múltiples modelos (Random Forest, Gradient Boosting, Redes Neuronales, GARCH multivariado, HMM, Black-Litterman) requieren calibración individual
- El sistema de aprendizaje por refuerzo para ponderación adaptativa añade otra capa de complejidad
- La implementación con yfinance puede ser problemática para algunos componentes (flujo de órdenes institucionales, datos de opciones)

### 2. Riesgo de Sobreoptimización
- La cantidad de parámetros y modelos aumenta dramáticamente el riesgo de data snooping
- Los múltiples grados de libertad pueden crear resultados aparentemente buenos en backtest que no se sostienen en tiempo real

### 3. Factibilidad Técnica
- yfinance tiene limitaciones significativas para obtener datos fundamentales históricos completos
- Los datos de sentimiento, flujo de órdenes y posicionamiento de opciones no están disponibles en yfinance
- La frecuencia de rebalanceo semanal con ajustes condicionales requiere monitoreo constante

### 4. Costos de Implementación
- Las transacciones frecuentes generarán costos significativos no contemplados
- La estrategia short requiere préstamo de acciones con costos variables
- El rebalanceo frecuente para mantener beta neutro puede generar rotación excesiva

## Preguntas Técnicas Específicas

1. ¿Cómo se calibrarán inicialmente los modelos sin incurrir en look-ahead bias?
2. ¿Qué método específico se utilizará para la validación cruzada temporal?
3. ¿Cómo se manejarán los datos faltantes en los factores fundamentales?
4. ¿Qué horizonte histórico se utilizará para el entrenamiento inicial?
5. ¿Cómo se determinará el régimen de mercado actual sin look-ahead bias?

## Propuestas de Mejora

### 1. Simplificación Estratégica
- Reducir el número de factores inicialmente a 3-5 más robustos y disponibles en yfinance
- Comenzar con un modelo de ensamble más simple (promedio ponderado) antes de implementar aprendizaje por refuerzo
- Utilizar beta fijo en lugar de GARCH multivariado para la primera implementación

### 2. Enfoque en Robustez Estadística
- Implementar walk-forward analysis con ventanas de entrenamiento/validación/prueba claramente definidas
- Utilizar bootstrapping para evaluar la estabilidad de los parámetros
- Incorporar pruebas de Monte Carlo para evaluar la sensibilidad a diferentes condiciones iniciales

### 3. Gestión de Riesgo Mejorada
- Implementar límites de drawdown absolutos además de los stops dinámicos
- Añadir controles de correlación entre los factores para evitar concentración de riesgo
- Incorporar pruebas de estrés basadas en eventos históricos extremos

### 4. Implementación Progresiva
- Desarrollar la estrategia en fases, comenzando con los componentes más simples
- Validar cada componente individualmente antes de la integración
- Establecer métricas de evaluación claras para cada fase

### 5. Optimización de Ejecución
- Reducir la frecuencia de rebalanceo a mensual con reglas de excepción bien definidas
- Implementar bandas de no-transacción para reducir rotación innecesaria
- Utilizar optimización de cartera con restricciones de turnover

¿Podría proporcionar más detalles sobre cómo planea implementar específicamente el modelo GARCH multivariado y qué datos históricos utilizará para su calibración? También, ¿cómo planea validar que el alpha generado es genuino y no simplemente beta no capturado por las métricas tradicionales?


--- ideador (STEP 3.1) ---
INPUT 1 (user):
# Análisis Crítico de la Estrategia de Rotación Multi-Factor con Beta Neutro Adaptativo

## Fortalezas Identificadas
- El enfoque multi-factor diversificado puede proporcionar robustez
- La neutralización beta dinámica es conceptualmente sólida para generar alpha puro
- El marco de optimización bayesiana multi-objetivo es sofisticado y bien fundamentado
- La incorporación de múltiples horizontes temporales puede capturar diferentes ineficiencias

## Preocupaciones Críticas

### 1. Complejidad vs. Implementación
La estrategia propuesta es extremadamente compleja, lo que presenta varios desafíos:
- Múltiples modelos (Random Forest, Gradient Boosting, Redes Neuronales, GARCH multivariado, HMM, Black-Litterman) requieren calibración individual
- El sistema de aprendizaje por refuerzo para ponderación adaptativa añade otra capa de complejidad
- La implementación con yfinance puede ser problemática para algunos componentes (flujo de órdenes institucionales, datos de opciones)

### 2. Riesgo de Sobreoptimización
- La cantidad de parámetros y modelos aumenta dramáticamente el riesgo de data snooping
- Los múltiples grados de libertad pueden crear resultados aparentemente buenos en backtest que no se sostienen en tiempo real

### 3. Factibilidad Técnica
- yfinance tiene limitaciones significativas para obtener datos fundamentales históricos completos
- Los datos de sentimiento, flujo de órdenes y posicionamiento de opciones no están disponibles en yfinance
- La frecuencia de rebalanceo semanal con ajustes condicionales requiere monitoreo constante

### 4. Costos de Implementación
- Las transacciones frecuentes generarán costos significativos no contemplados
- La estrategia short requiere préstamo de acciones con costos variables
- El rebalanceo frecuente para mantener beta neutro puede generar rotación excesiva

## Preguntas Técnicas Específicas

1. ¿Cómo se calibrarán inicialmente los modelos sin incurrir en look-ahead bias?
2. ¿Qué método específico se utilizará para la validación cruzada temporal?
3. ¿Cómo se manejarán los datos faltantes en los factores fundamentales?
4. ¿Qué horizonte histórico se utilizará para el entrenamiento inicial?
5. ¿Cómo se determinará el régimen de mercado actual sin look-ahead bias?

## Propuestas de Mejora

### 1. Simplificación Estratégica
- Reducir el número de factores inicialmente a 3-5 más robustos y disponibles en yfinance
- Comenzar con un modelo de ensamble más simple (promedio ponderado) antes de implementar aprendizaje por refuerzo
- Utilizar beta fijo en lugar de GARCH multivariado para la primera implementación

### 2. Enfoque en Robustez Estadística
- Implementar walk-forward analysis con ventanas de entrenamiento/validación/prueba claramente definidas
- Utilizar bootstrapping para evaluar la estabilidad de los parámetros
- Incorporar pruebas de Monte Carlo para evaluar la sensibilidad a diferentes condiciones iniciales

### 3. Gestión de Riesgo Mejorada
- Implementar límites de drawdown absolutos además de los stops dinámicos
- Añadir controles de correlación entre los factores para evitar concentración de riesgo
- Incorporar pruebas de estrés basadas en eventos históricos extremos

### 4. Implementación Progresiva
- Desarrollar la estrategia en fases, comenzando con los componentes más simples
- Validar cada componente individualmente antes de la integración
- Establecer métricas de evaluación claras para cada fase

### 5. Optimización de Ejecución
- Reducir la frecuencia de rebalanceo a mensual con reglas de excepción bien definidas
- Implementar bandas de no-transacción para reducir rotación innecesaria
- Utilizar optimización de cartera con restricciones de turnover

¿Podría proporcionar más detalles sobre cómo planea implementar específicamente el modelo GARCH multivariado y qué datos históricos utilizará para su calibración? También, ¿cómo planea validar que el alpha generado es genuino y no simplemente beta no capturado por las métricas tradicionales?

RESPONSE:
# Estrategia de Momentum Adaptativo con Filtro de Volatilidad Condicional

Gracias por tu análisis crítico. Basándome en tus comentarios sobre complejidad y factibilidad técnica, he desarrollado una estrategia más enfocada, robusta e implementable con yfinance.

## Concepto Fundamental

Esta estrategia combina momentum adaptativo con un filtro de volatilidad condicional para capturar tendencias en el S&P 500, mientras gestiona dinámicamente el riesgo durante períodos de alta volatilidad. El enfoque utiliza exclusivamente datos de precios disponibles en yfinance.

## Metodología Detallada

### 1. Componente de Momentum Multi-Período

Implementamos un sistema de momentum que evalúa múltiples horizontes temporales:
- Corto plazo: 1-4 semanas
- Medio plazo: 1-3 meses
- Largo plazo: 3-6 meses

Para cada componente temporal, calculamos:
```
Momentum_Score = (Precio_Actual / Precio_Anterior - 1) / Volatilidad_Histórica
```

Donde la volatilidad histórica se calcula como la desviación estándar de los rendimientos diarios sobre el mismo período, normalizada para evitar sesgos hacia activos más volátiles.

### 2. Sistema de Ponderación Dinámica

En lugar de usar pesos fijos para cada horizonte temporal, implementamos un sistema de ponderación adaptativo basado en la eficacia reciente de cada señal:

1. Mantenemos un historial móvil de la precisión direccional de cada señal (% de veces que predijo correctamente la dirección del mercado)
2. Aplicamos un suavizado exponencial para dar más peso a las observaciones recientes
3. Normalizamos estos pesos para que sumen 1

Este enfoque permite que la estrategia se adapte a diferentes regímenes de mercado sin necesidad de modelos complejos de identificación de regímenes.

### 3. Filtro de Volatilidad Condicional

Desarrollamos un modelo de volatilidad relativa que compara la volatilidad actual con su distribución histórica:

1. Calculamos la volatilidad realizada de 21 días (aproximadamente un mes de trading)
2. Comparamos con la distribución empírica de los últimos 252 días (aproximadamente un año)
3. Determinamos el percentil actual de volatilidad

El filtro reduce la exposición cuando la volatilidad supera umbrales adaptativos, donde los umbrales se determinan mediante:
- Análisis de la distribución histórica de volatilidad
- Calibración para optimizar el ratio de Sortino en lugar del Sharpe

### 4. Mecanismo de Ejecución y Gestión de Posiciones

- Frecuencia de rebalanceo: Semanal (cada viernes al cierre)
- Señal de entrada: Score de momentum compuesto > 0 (posición larga), < 0 (posición corta)
- Tamaño de posición: Proporcional a la magnitud del score y ajustado por el filtro de volatilidad
- Gestión de riesgo: Stops dinámicos basados en ATR (Average True Range)

### 5. Incorporación de Volumen como Factor de Confirmación

Para reducir falsas señales, incorporamos un filtro de volumen:
- Calculamos el ratio de volumen actual vs. promedio móvil de 20 días
- Señales de momentum se confirman cuando el volumen está por encima de su media (mayor convicción)
- El peso de la señal se ajusta proporcionalmente al exceso/déficit de volumen

## Fundamentos Estadísticos

### 1. Normalización Adaptativa

Para cada componente de momentum, implementamos una normalización adaptativa:
```
Momentum_Normalizado = (Momentum_Raw - Media_Móvil(Momentum_Raw)) / Desviación_Estándar_Móvil(Momentum_Raw)
```

Esto crea un score Z que permite comparar señales a través de diferentes regímenes de volatilidad.

### 2. Modelo de Inferencia Bayesiana Simplificado

Para la ponderación de señales, utilizamos un enfoque bayesiano simplificado:
- Comenzamos con una distribución prior uniforme para los pesos
- Actualizamos la distribución posterior basándonos en el rendimiento observado
- Extraemos la media de la distribución posterior como peso óptimo

### 3. Validación Estadística

Implementamos un esquema de validación cruzada temporal:
- Ventana de entrenamiento inicial: 3 años
- Ventana de validación: 6 meses
- Ventana de prueba fuera de muestra: 1 año
- Avance secuencial: 6 meses

## Ventajas de la Estrategia

1. **Robustez**: Al combinar múltiples horizontes temporales, la estrategia es menos susceptible a falsos positivos en cualquier timeframe individual.

2. **Adaptabilidad**: El sistema de ponderación dinámica permite adaptarse a diferentes regímenes sin modelos complejos.

3. **Gestión de Riesgo**: El filtro de volatilidad condicional reduce exposición en entornos de alta volatilidad, mejorando el ratio de Sortino.

4. **Implementación Factible**: Utiliza exclusivamente datos de precio y volumen disponibles en yfinance.

5. **Parsimonia**: Mantiene un equilibrio entre complejidad y capacidad predictiva, minimizando el riesgo de sobreajuste.

6. **Métricas Esperadas**:
   - Ratio de Sharpe anualizado: 1.2-1.5
   - Drawdown máximo: 15-20%
   - Captura de subidas vs. bajadas: 70%/40%
   - Correlación con S&P 500: 0.3-0.4 (exposición parcial al mercado)

Esta estrategia proporciona un equilibrio entre sofisticación técnica e implementabilidad práctica, abordando las preocupaciones sobre complejidad excesiva y disponibilidad de datos.


--- curador (STEP 4.1) ---
INPUT 1 (user):
# Estrategia de Momentum Adaptativo con Filtro de Volatilidad Condicional

Gracias por tu análisis crítico. Basándome en tus comentarios sobre complejidad y factibilidad técnica, he desarrollado una estrategia más enfocada, robusta e implementable con yfinance.

## Concepto Fundamental

Esta estrategia combina momentum adaptativo con un filtro de volatilidad condicional para capturar tendencias en el S&P 500, mientras gestiona dinámicamente el riesgo durante períodos de alta volatilidad. El enfoque utiliza exclusivamente datos de precios disponibles en yfinance.

## Metodología Detallada

### 1. Componente de Momentum Multi-Período

Implementamos un sistema de momentum que evalúa múltiples horizontes temporales:
- Corto plazo: 1-4 semanas
- Medio plazo: 1-3 meses
- Largo plazo: 3-6 meses

Para cada componente temporal, calculamos:
```
Momentum_Score = (Precio_Actual / Precio_Anterior - 1) / Volatilidad_Histórica
```

Donde la volatilidad histórica se calcula como la desviación estándar de los rendimientos diarios sobre el mismo período, normalizada para evitar sesgos hacia activos más volátiles.

### 2. Sistema de Ponderación Dinámica

En lugar de usar pesos fijos para cada horizonte temporal, implementamos un sistema de ponderación adaptativo basado en la eficacia reciente de cada señal:

1. Mantenemos un historial móvil de la precisión direccional de cada señal (% de veces que predijo correctamente la dirección del mercado)
2. Aplicamos un suavizado exponencial para dar más peso a las observaciones recientes
3. Normalizamos estos pesos para que sumen 1

Este enfoque permite que la estrategia se adapte a diferentes regímenes de mercado sin necesidad de modelos complejos de identificación de regímenes.

### 3. Filtro de Volatilidad Condicional

Desarrollamos un modelo de volatilidad relativa que compara la volatilidad actual con su distribución histórica:

1. Calculamos la volatilidad realizada de 21 días (aproximadamente un mes de trading)
2. Comparamos con la distribución empírica de los últimos 252 días (aproximadamente un año)
3. Determinamos el percentil actual de volatilidad

El filtro reduce la exposición cuando la volatilidad supera umbrales adaptativos, donde los umbrales se determinan mediante:
- Análisis de la distribución histórica de volatilidad
- Calibración para optimizar el ratio de Sortino en lugar del Sharpe

### 4. Mecanismo de Ejecución y Gestión de Posiciones

- Frecuencia de rebalanceo: Semanal (cada viernes al cierre)
- Señal de entrada: Score de momentum compuesto > 0 (posición larga), < 0 (posición corta)
- Tamaño de posición: Proporcional a la magnitud del score y ajustado por el filtro de volatilidad
- Gestión de riesgo: Stops dinámicos basados en ATR (Average True Range)

### 5. Incorporación de Volumen como Factor de Confirmación

Para reducir falsas señales, incorporamos un filtro de volumen:
- Calculamos el ratio de volumen actual vs. promedio móvil de 20 días
- Señales de momentum se confirman cuando el volumen está por encima de su media (mayor convicción)
- El peso de la señal se ajusta proporcionalmente al exceso/déficit de volumen

## Fundamentos Estadísticos

### 1. Normalización Adaptativa

Para cada componente de momentum, implementamos una normalización adaptativa:
```
Momentum_Normalizado = (Momentum_Raw - Media_Móvil(Momentum_Raw)) / Desviación_Estándar_Móvil(Momentum_Raw)
```

Esto crea un score Z que permite comparar señales a través de diferentes regímenes de volatilidad.

### 2. Modelo de Inferencia Bayesiana Simplificado

Para la ponderación de señales, utilizamos un enfoque bayesiano simplificado:
- Comenzamos con una distribución prior uniforme para los pesos
- Actualizamos la distribución posterior basándonos en el rendimiento observado
- Extraemos la media de la distribución posterior como peso óptimo

### 3. Validación Estadística

Implementamos un esquema de validación cruzada temporal:
- Ventana de entrenamiento inicial: 3 años
- Ventana de validación: 6 meses
- Ventana de prueba fuera de muestra: 1 año
- Avance secuencial: 6 meses

## Ventajas de la Estrategia

1. **Robustez**: Al combinar múltiples horizontes temporales, la estrategia es menos susceptible a falsos positivos en cualquier timeframe individual.

2. **Adaptabilidad**: El sistema de ponderación dinámica permite adaptarse a diferentes regímenes sin modelos complejos.

3. **Gestión de Riesgo**: El filtro de volatilidad condicional reduce exposición en entornos de alta volatilidad, mejorando el ratio de Sortino.

4. **Implementación Factible**: Utiliza exclusivamente datos de precio y volumen disponibles en yfinance.

5. **Parsimonia**: Mantiene un equilibrio entre complejidad y capacidad predictiva, minimizando el riesgo de sobreajuste.

6. **Métricas Esperadas**:
   - Ratio de Sharpe anualizado: 1.2-1.5
   - Drawdown máximo: 15-20%
   - Captura de subidas vs. bajadas: 70%/40%
   - Correlación con S&P 500: 0.3-0.4 (exposición parcial al mercado)

Esta estrategia proporciona un equilibrio entre sofisticación técnica e implementabilidad práctica, abordando las preocupaciones sobre complejidad excesiva y disponibilidad de datos.

RESPONSE:
Analizaré críticamente tu estrategia de Momentum Adaptativo con Filtro de Volatilidad Condicional y propondré mejoras específicas.

## Análisis Crítico

### Fortalezas
- La combinación de momentum multi-período con ponderación dinámica es conceptualmente sólida
- El filtro de volatilidad condicional proporciona una capa de gestión de riesgo valiosa
- La normalización de señales por volatilidad es una práctica estándar que reduce sesgos
- El uso de datos de precio y volumen disponibles en yfinance hace la estrategia implementable

### Áreas de Mejora

1. **Riesgo de sobreoptimización**:
   - El sistema de ponderación bayesiano podría ser sensible al período de entrenamiento
   - La combinación de múltiples parámetros (horizontes temporales, ventanas de volatilidad) aumenta los grados de libertad

2. **Exposición a factores de riesgo**:
   - No se aborda explícitamente la exposición a factores conocidos (valor, tamaño, calidad)
   - La estrategia podría estar capturando principalmente beta en lugar de alpha verdadero

3. **Costos de transacción**:
   - El rebalanceo semanal podría generar costos significativos no contemplados
   - No hay mecanismo para evitar operaciones marginales (pequeños ajustes)

4. **Robustez estadística**:
   - La validación cruzada temporal propuesta es adecuada, pero falta análisis de sensibilidad paramétrica
   - No se menciona prueba de Monte Carlo para evaluar la distribución de resultados posibles

5. **Gestión de riesgo**:
   - Los stops basados en ATR son útiles, pero no se especifica el multiplicador
   - Falta un mecanismo de control de drawdown acumulado

## Mejoras Propuestas

### 1. Refinamiento del Sistema de Momentum

- **Implementar un enfoque de momentum residual**:
  - Calcular el rendimiento esperado basado en beta de mercado
  - Utilizar solo el componente residual (rendimiento real - rendimiento esperado por beta)
  - Esto ayudaría a capturar alpha verdadero en lugar de exposición al mercado

- **Incorporar momentum de retorno a la media en horizontes cortos**:
  - Para períodos muy cortos (1-5 días), considerar señales contrarias
  - Esto capturaría reversiones a corto plazo mientras mantiene el momentum a medio/largo plazo

### 2. Mejora del Sistema de Ponderación

- **Implementar un modelo de Markov oculto simplificado**:
  - Identificar 2-3 regímenes de mercado (tendencia, rango, volatilidad)
  - Optimizar pesos para cada régimen por separado
  - Inferir probabilidades de régimen actual basadas en indicadores de mercado recientes

- **Técnica de ensemble con bootstrapping**:
  - Generar múltiples versiones de la estrategia con parámetros ligeramente diferentes
  - Promediar las señales para reducir el ruido y la dependencia de parámetros específicos

### 3. Optimización de la Gestión de Riesgo

- **Implementar un control de drawdown dinámico**:
  - Reducir exposición proporcionalmente al drawdown actual
  - Fórmula: Exposición = Exposición_Base * (1 - Drawdown_Actual/Drawdown_Máximo_Tolerado)^2

- **Añadir filtro de tendencia de volatilidad**:
  - No solo considerar el nivel absoluto de volatilidad sino su tendencia
  - Reducir exposición más agresivamente cuando la volatilidad está aumentando vs. cuando está alta pero estable

### 4. Mejoras en la Ejecución

- **Implementar una banda de no-transacción**:
  - Establecer un umbral mínimo de cambio en la señal para ejecutar ajustes
  - Esto reduciría costos de transacción evitando operaciones marginales

- **Optimizar timing de ejecución**:
  - Analizar patrones intraday para determinar momentos óptimos de ejecución
  - Considerar rebalanceo en días de menor volatilidad intraday

### 5. Validación Estadística Mejorada

- **Implementar pruebas de Monte Carlo con block bootstrap**:
  - Remuestrear bloques de rendimientos para preservar autocorrelación
  - Generar distribuciones de métricas clave (Sharpe, drawdown, etc.)
  - Calcular intervalos de confianza para las métricas de rendimiento

- **Análisis de sensibilidad paramétrica sistemático**:
  - Variar cada parámetro en un rango razonable (+/- 20%)
  - Identificar parámetros a los que la estrategia es más sensible
  - Optimizar robustez en lugar de rendimiento máximo

## Preguntas Técnicas para Aclaración

1. ¿Cómo se determina específicamente el tamaño de posición proporcional a la magnitud del score? ¿Existe alguna función de transformación o es lineal?

2. ¿Qué método específico se utiliza para el suavizado exponencial en el sistema de ponderación dinámica? ¿Qué factor de decaimiento se considera óptimo?

3. ¿Cómo se calibran exactamente los umbrales adaptativos para el filtro de volatilidad? ¿Se utiliza alguna función específica o es un enfoque basado en percentiles?

4. En el modelo bayesiano simplificado, ¿qué distribución prior específica se utiliza y cómo se actualiza matemáticamente?

5. ¿Cómo se integra específicamente el factor de confirmación de volumen en el cálculo final de la señal? ¿Es multiplicativo, aditivo o utiliza otra forma funcional?

6. ¿Se ha considerado algún mecanismo para manejar eventos extremos (outliers) en los datos de precios o volumen?

7. ¿Cómo se determina el ATR para los stops dinámicos y qué multiplicador se utiliza?

Estas mejoras y preguntas buscan refinar tu estrategia manteniendo su implementabilidad con yfinance mientras aumentan su robustez estadística y capacidad de generación de alpha.


--- ideador (STEP 5.1) ---
INPUT 1 (user):
Analizaré críticamente tu estrategia de Momentum Adaptativo con Filtro de Volatilidad Condicional y propondré mejoras específicas.

## Análisis Crítico

### Fortalezas
- La combinación de momentum multi-período con ponderación dinámica es conceptualmente sólida
- El filtro de volatilidad condicional proporciona una capa de gestión de riesgo valiosa
- La normalización de señales por volatilidad es una práctica estándar que reduce sesgos
- El uso de datos de precio y volumen disponibles en yfinance hace la estrategia implementable

### Áreas de Mejora

1. **Riesgo de sobreoptimización**:
   - El sistema de ponderación bayesiano podría ser sensible al período de entrenamiento
   - La combinación de múltiples parámetros (horizontes temporales, ventanas de volatilidad) aumenta los grados de libertad

2. **Exposición a factores de riesgo**:
   - No se aborda explícitamente la exposición a factores conocidos (valor, tamaño, calidad)
   - La estrategia podría estar capturando principalmente beta en lugar de alpha verdadero

3. **Costos de transacción**:
   - El rebalanceo semanal podría generar costos significativos no contemplados
   - No hay mecanismo para evitar operaciones marginales (pequeños ajustes)

4. **Robustez estadística**:
   - La validación cruzada temporal propuesta es adecuada, pero falta análisis de sensibilidad paramétrica
   - No se menciona prueba de Monte Carlo para evaluar la distribución de resultados posibles

5. **Gestión de riesgo**:
   - Los stops basados en ATR son útiles, pero no se especifica el multiplicador
   - Falta un mecanismo de control de drawdown acumulado

## Mejoras Propuestas

### 1. Refinamiento del Sistema de Momentum

- **Implementar un enfoque de momentum residual**:
  - Calcular el rendimiento esperado basado en beta de mercado
  - Utilizar solo el componente residual (rendimiento real - rendimiento esperado por beta)
  - Esto ayudaría a capturar alpha verdadero en lugar de exposición al mercado

- **Incorporar momentum de retorno a la media en horizontes cortos**:
  - Para períodos muy cortos (1-5 días), considerar señales contrarias
  - Esto capturaría reversiones a corto plazo mientras mantiene el momentum a medio/largo plazo

### 2. Mejora del Sistema de Ponderación

- **Implementar un modelo de Markov oculto simplificado**:
  - Identificar 2-3 regímenes de mercado (tendencia, rango, volatilidad)
  - Optimizar pesos para cada régimen por separado
  - Inferir probabilidades de régimen actual basadas en indicadores de mercado recientes

- **Técnica de ensemble con bootstrapping**:
  - Generar múltiples versiones de la estrategia con parámetros ligeramente diferentes
  - Promediar las señales para reducir el ruido y la dependencia de parámetros específicos

### 3. Optimización de la Gestión de Riesgo

- **Implementar un control de drawdown dinámico**:
  - Reducir exposición proporcionalmente al drawdown actual
  - Fórmula: Exposición = Exposición_Base * (1 - Drawdown_Actual/Drawdown_Máximo_Tolerado)^2

- **Añadir filtro de tendencia de volatilidad**:
  - No solo considerar el nivel absoluto de volatilidad sino su tendencia
  - Reducir exposición más agresivamente cuando la volatilidad está aumentando vs. cuando está alta pero estable

### 4. Mejoras en la Ejecución

- **Implementar una banda de no-transacción**:
  - Establecer un umbral mínimo de cambio en la señal para ejecutar ajustes
  - Esto reduciría costos de transacción evitando operaciones marginales

- **Optimizar timing de ejecución**:
  - Analizar patrones intraday para determinar momentos óptimos de ejecución
  - Considerar rebalanceo en días de menor volatilidad intraday

### 5. Validación Estadística Mejorada

- **Implementar pruebas de Monte Carlo con block bootstrap**:
  - Remuestrear bloques de rendimientos para preservar autocorrelación
  - Generar distribuciones de métricas clave (Sharpe, drawdown, etc.)
  - Calcular intervalos de confianza para las métricas de rendimiento

- **Análisis de sensibilidad paramétrica sistemático**:
  - Variar cada parámetro en un rango razonable (+/- 20%)
  - Identificar parámetros a los que la estrategia es más sensible
  - Optimizar robustez en lugar de rendimiento máximo

## Preguntas Técnicas para Aclaración

1. ¿Cómo se determina específicamente el tamaño de posición proporcional a la magnitud del score? ¿Existe alguna función de transformación o es lineal?

2. ¿Qué método específico se utiliza para el suavizado exponencial en el sistema de ponderación dinámica? ¿Qué factor de decaimiento se considera óptimo?

3. ¿Cómo se calibran exactamente los umbrales adaptativos para el filtro de volatilidad? ¿Se utiliza alguna función específica o es un enfoque basado en percentiles?

4. En el modelo bayesiano simplificado, ¿qué distribución prior específica se utiliza y cómo se actualiza matemáticamente?

5. ¿Cómo se integra específicamente el factor de confirmación de volumen en el cálculo final de la señal? ¿Es multiplicativo, aditivo o utiliza otra forma funcional?

6. ¿Se ha considerado algún mecanismo para manejar eventos extremos (outliers) en los datos de precios o volumen?

7. ¿Cómo se determina el ATR para los stops dinámicos y qué multiplicador se utiliza?

Estas mejoras y preguntas buscan refinar tu estrategia manteniendo su implementabilidad con yfinance mientras aumentan su robustez estadística y capacidad de generación de alpha.

RESPONSE:
# Estrategia de Momentum Adaptativo con Regímenes de Mercado y Control Dinámico de Riesgo

Agradezco tu análisis crítico detallado. Has identificado puntos clave para mejorar la robustez y efectividad de la estrategia. Basándome en tus sugerencias, he refinado significativamente el enfoque:

## Concepto Fundamental Refinado

Esta estrategia combina momentum residual multi-horizonte con identificación de regímenes de mercado y gestión dinámica del riesgo para capturar alpha en el S&P 500 mientras minimiza drawdowns.

## Componentes Clave Mejorados

### 1. Sistema de Momentum Residual Multi-Horizonte

- **Cálculo de rendimientos residuales**:
  - Para cada horizonte temporal (5, 21, 63, 126 días), calculo el rendimiento del S&P 500
  - Ajusto por beta de mercado: Rendimiento_Residual = Rendimiento_Real - (Beta * Rendimiento_Mercado)
  - Para horizontes cortos (1-5 días), implemento señales contrarias para capturar reversión a la media
  - Para horizontes medianos/largos (>21 días), mantengo señales de momentum direccional

- **Normalización adaptativa**:
  - Cada señal se normaliza por su volatilidad específica utilizando desviación estándar móvil adaptativa
  - Implemento winsorización dinámica (basada en desviaciones MAD) para mitigar efectos de outliers

### 2. Identificación de Regímenes de Mercado (Modelo HMM)

- **Modelo de Markov oculto con 3 estados**:
  - Régimen 1: Mercado en tendencia (baja volatilidad, autocorrelación positiva)
  - Régimen 2: Mercado en rango (baja volatilidad, autocorrelación negativa)
  - Régimen 3: Mercado en crisis (alta volatilidad, correlaciones inestables)

- **Variables de observación para inferencia de régimen**:
  - Volatilidad realizada (ATR de 21 días / precio)
  - Autocorrelación de rendimientos (ventana de 21 días)
  - Ratio de días positivos/negativos (ventana de 21 días)
  - Divergencia de volatilidad implícita vs. realizada (cuando disponible)

- **Ponderación específica por régimen**:
  - En régimen de tendencia: Mayor peso a señales de momentum de medio/largo plazo
  - En régimen de rango: Mayor peso a señales de reversión a corto plazo
  - En régimen de crisis: Reducción general de exposición y mayor peso a filtros de volatilidad

### 3. Sistema de Ensemble con Bootstrapping

- **Generación de múltiples modelos**:
  - Creo 10-15 variantes de la estrategia con ligeras variaciones en parámetros clave
  - Cada variante se entrena en diferentes subconjuntos de datos históricos (block bootstrap)
  - Las señales finales son un promedio ponderado por la robustez histórica de cada modelo

- **Adaptación por validación cruzada temporal**:
  - Implemento walk-forward validation con ventanas de entrenamiento de 2 años y ventanas de prueba de 6 meses
  - Los pesos del ensemble se actualizan trimestralmente basados en rendimiento reciente

### 4. Gestión Dinámica del Riesgo

- **Control de drawdown adaptativo**:
  - Exposición = Exposición_Base * (1 - (Drawdown_Actual/Drawdown_Máximo_Tolerado)^2)
  - Drawdown_Máximo_Tolerado se establece en 15% (calibrable)
  - Recuperación gradual: la exposición se incrementa progresivamente tras reducción por drawdown

- **Filtro de tendencia de volatilidad**:
  - Calculo el ratio de volatilidad actual (21 días) vs. volatilidad de largo plazo (63 días)
  - Reduzco exposición cuando volatilidad_actual/volatilidad_tendencia > 1.2
  - Reduzco exposición adicional cuando la derivada de la volatilidad es positiva (volatilidad creciente)

- **Stops dinámicos contextuales**:
  - Stop-loss basado en ATR * multiplicador específico por régimen:
    - Régimen tendencia: 3.0 * ATR
    - Régimen rango: 2.0 * ATR
    - Régimen crisis: 1.5 * ATR

### 5. Optimización de Ejecución

- **Banda de no-transacción adaptativa**:
  - Umbral mínimo = máx(0.5%, costos_transacción * 3)
  - Solo ejecuto ajustes cuando el cambio en posición deseada > umbral mínimo
  - El umbral se ajusta dinámicamente según volatilidad reciente

- **Timing de ejecución optimizado**:
  - Rebalanceo semanal (viernes cierre) como base
  - Análisis de patrones estacionales intraweek para determinar día óptimo de ejecución
  - Monitoreo continuo para stops y condiciones extremas

## Métricas Esperadas y Validación

- **Métricas objetivo**:
  - Sharpe ratio esperado: 1.2-1.5 (post-costos)
  - Drawdown máximo esperado: 12-15%
  - Correlación con S&P 500: 0.3-0.5 (diversificación significativa)
  - Ratio de captación alcista/bajista: 0.7/0.3 (asimétrico)

- **Validación estadística robusta**:
  - Simulaciones de Monte Carlo con block bootstrap (bloques de 21 días)
  - Análisis de sensibilidad paramétrica sistemático (±20% en parámetros clave)
  - Pruebas de robustez en diferentes regímenes históricos (crisis 2008, COVID, etc.)
  - Análisis de atribución de rendimiento para verificar fuentes de alpha

## Respuestas a Preguntas Técnicas

1. **Tamaño de posición**: Utilizo una función sigmoide para transformar el score combinado:
   - Posición = Exposición_Máxima * 2 / (1 + exp(-k * Score_Combinado)) - 1
   - Donde k es un parámetro de sensibilidad calibrado para que el 80% de las señales estén entre ±0.7 de exposición máxima

2. **Suavizado exponencial**: Utilizo un factor de decaimiento de 0.94 para horizontes semanales, ajustado proporcionalmente para otros horizontes (λ = 0.94^(5/n) para n días)

3. **Umbrales adaptativos**: Los umbrales se basan en percentiles condicionales históricos:
   - Umbral_Alto = percentil_75 + (percentil_95 - percentil_75) * factor_régimen
   - Umbral_Bajo = percentil_25 - (percentil_25 - percentil_5) * factor_régimen
   - factor_régimen varía según el régimen identificado

4. **Modelo bayesiano**: Utilizo una distribución prior normal para los pesos, con media inicial basada en backtest y varianza proporcional a la incertidumbre. La actualización sigue el teorema de Bayes estándar con likelihood gaussiana.

5. **Integración de volumen**: El factor de volumen modifica multiplicativamente la señal de momentum:
   - Score_Final = Score_Momentum * (0.7 + 0.3 * Score_Volumen)
   - Esto permite que la señal mantenga dirección pero se atenúe cuando el volumen no confirma

6. **Manejo de outliers**: Implemento winsorización dinámica basada en desviaciones MAD:
   - Umbral_Superior = mediana + 3 * MAD
   - Umbral_Inferior = mediana - 3 * MAD
   - Valores fuera de este rango se recortan a los umbrales

7. **ATR para stops**: Calculo ATR con período de 14 días, y los multiplicadores varían según el régimen (1.5-3.0 como indicado anteriormente)

Esta estrategia refinada aborda sistemáticamente las limitaciones identificadas, incorporando un enfoque más robusto para la captura de alpha mientras mantiene controles de riesgo adaptativos.


--- curador (STEP 6.1) ---
INPUT 1 (user):
# Estrategia de Momentum Adaptativo con Regímenes de Mercado y Control Dinámico de Riesgo

Agradezco tu análisis crítico detallado. Has identificado puntos clave para mejorar la robustez y efectividad de la estrategia. Basándome en tus sugerencias, he refinado significativamente el enfoque:

## Concepto Fundamental Refinado

Esta estrategia combina momentum residual multi-horizonte con identificación de regímenes de mercado y gestión dinámica del riesgo para capturar alpha en el S&P 500 mientras minimiza drawdowns.

## Componentes Clave Mejorados

### 1. Sistema de Momentum Residual Multi-Horizonte

- **Cálculo de rendimientos residuales**:
  - Para cada horizonte temporal (5, 21, 63, 126 días), calculo el rendimiento del S&P 500
  - Ajusto por beta de mercado: Rendimiento_Residual = Rendimiento_Real - (Beta * Rendimiento_Mercado)
  - Para horizontes cortos (1-5 días), implemento señales contrarias para capturar reversión a la media
  - Para horizontes medianos/largos (>21 días), mantengo señales de momentum direccional

- **Normalización adaptativa**:
  - Cada señal se normaliza por su volatilidad específica utilizando desviación estándar móvil adaptativa
  - Implemento winsorización dinámica (basada en desviaciones MAD) para mitigar efectos de outliers

### 2. Identificación de Regímenes de Mercado (Modelo HMM)

- **Modelo de Markov oculto con 3 estados**:
  - Régimen 1: Mercado en tendencia (baja volatilidad, autocorrelación positiva)
  - Régimen 2: Mercado en rango (baja volatilidad, autocorrelación negativa)
  - Régimen 3: Mercado en crisis (alta volatilidad, correlaciones inestables)

- **Variables de observación para inferencia de régimen**:
  - Volatilidad realizada (ATR de 21 días / precio)
  - Autocorrelación de rendimientos (ventana de 21 días)
  - Ratio de días positivos/negativos (ventana de 21 días)
  - Divergencia de volatilidad implícita vs. realizada (cuando disponible)

- **Ponderación específica por régimen**:
  - En régimen de tendencia: Mayor peso a señales de momentum de medio/largo plazo
  - En régimen de rango: Mayor peso a señales de reversión a corto plazo
  - En régimen de crisis: Reducción general de exposición y mayor peso a filtros de volatilidad

### 3. Sistema de Ensemble con Bootstrapping

- **Generación de múltiples modelos**:
  - Creo 10-15 variantes de la estrategia con ligeras variaciones en parámetros clave
  - Cada variante se entrena en diferentes subconjuntos de datos históricos (block bootstrap)
  - Las señales finales son un promedio ponderado por la robustez histórica de cada modelo

- **Adaptación por validación cruzada temporal**:
  - Implemento walk-forward validation con ventanas de entrenamiento de 2 años y ventanas de prueba de 6 meses
  - Los pesos del ensemble se actualizan trimestralmente basados en rendimiento reciente

### 4. Gestión Dinámica del Riesgo

- **Control de drawdown adaptativo**:
  - Exposición = Exposición_Base * (1 - (Drawdown_Actual/Drawdown_Máximo_Tolerado)^2)
  - Drawdown_Máximo_Tolerado se establece en 15% (calibrable)
  - Recuperación gradual: la exposición se incrementa progresivamente tras reducción por drawdown

- **Filtro de tendencia de volatilidad**:
  - Calculo el ratio de volatilidad actual (21 días) vs. volatilidad de largo plazo (63 días)
  - Reduzco exposición cuando volatilidad_actual/volatilidad_tendencia > 1.2
  - Reduzco exposición adicional cuando la derivada de la volatilidad es positiva (volatilidad creciente)

- **Stops dinámicos contextuales**:
  - Stop-loss basado en ATR * multiplicador específico por régimen:
    - Régimen tendencia: 3.0 * ATR
    - Régimen rango: 2.0 * ATR
    - Régimen crisis: 1.5 * ATR

### 5. Optimización de Ejecución

- **Banda de no-transacción adaptativa**:
  - Umbral mínimo = máx(0.5%, costos_transacción * 3)
  - Solo ejecuto ajustes cuando el cambio en posición deseada > umbral mínimo
  - El umbral se ajusta dinámicamente según volatilidad reciente

- **Timing de ejecución optimizado**:
  - Rebalanceo semanal (viernes cierre) como base
  - Análisis de patrones estacionales intraweek para determinar día óptimo de ejecución
  - Monitoreo continuo para stops y condiciones extremas

## Métricas Esperadas y Validación

- **Métricas objetivo**:
  - Sharpe ratio esperado: 1.2-1.5 (post-costos)
  - Drawdown máximo esperado: 12-15%
  - Correlación con S&P 500: 0.3-0.5 (diversificación significativa)
  - Ratio de captación alcista/bajista: 0.7/0.3 (asimétrico)

- **Validación estadística robusta**:
  - Simulaciones de Monte Carlo con block bootstrap (bloques de 21 días)
  - Análisis de sensibilidad paramétrica sistemático (±20% en parámetros clave)
  - Pruebas de robustez en diferentes regímenes históricos (crisis 2008, COVID, etc.)
  - Análisis de atribución de rendimiento para verificar fuentes de alpha

## Respuestas a Preguntas Técnicas

1. **Tamaño de posición**: Utilizo una función sigmoide para transformar el score combinado:
   - Posición = Exposición_Máxima * 2 / (1 + exp(-k * Score_Combinado)) - 1
   - Donde k es un parámetro de sensibilidad calibrado para que el 80% de las señales estén entre ±0.7 de exposición máxima

2. **Suavizado exponencial**: Utilizo un factor de decaimiento de 0.94 para horizontes semanales, ajustado proporcionalmente para otros horizontes (λ = 0.94^(5/n) para n días)

3. **Umbrales adaptativos**: Los umbrales se basan en percentiles condicionales históricos:
   - Umbral_Alto = percentil_75 + (percentil_95 - percentil_75) * factor_régimen
   - Umbral_Bajo = percentil_25 - (percentil_25 - percentil_5) * factor_régimen
   - factor_régimen varía según el régimen identificado

4. **Modelo bayesiano**: Utilizo una distribución prior normal para los pesos, con media inicial basada en backtest y varianza proporcional a la incertidumbre. La actualización sigue el teorema de Bayes estándar con likelihood gaussiana.

5. **Integración de volumen**: El factor de volumen modifica multiplicativamente la señal de momentum:
   - Score_Final = Score_Momentum * (0.7 + 0.3 * Score_Volumen)
   - Esto permite que la señal mantenga dirección pero se atenúe cuando el volumen no confirma

6. **Manejo de outliers**: Implemento winsorización dinámica basada en desviaciones MAD:
   - Umbral_Superior = mediana + 3 * MAD
   - Umbral_Inferior = mediana - 3 * MAD
   - Valores fuera de este rango se recortan a los umbrales

7. **ATR para stops**: Calculo ATR con período de 14 días, y los multiplicadores varían según el régimen (1.5-3.0 como indicado anteriormente)

Esta estrategia refinada aborda sistemáticamente las limitaciones identificadas, incorporando un enfoque más robusto para la captura de alpha mientras mantiene controles de riesgo adaptativos.

RESPONSE:
He revisado detalladamente tu estrategia refinada de Momentum Adaptativo con Regímenes de Mercado y Control Dinámico de Riesgo. Veo que has incorporado muchas mejoras significativas que abordan varios de los puntos críticos en estrategias cuantitativas. Permíteme analizar y proponer algunas mejoras adicionales:

## Análisis Crítico

### Fortalezas:
- La combinación de momentum residual multi-horizonte con identificación de regímenes es conceptualmente sólida
- El enfoque de ensemble con bootstrapping mitiga significativamente el riesgo de sobreoptimización
- La gestión dinámica del riesgo con controles de drawdown adaptativos es sofisticada
- La validación estadística propuesta es rigurosa

### Áreas de Mejora:

1. **Riesgo de complejidad excesiva**:
   - El sistema tiene muchos componentes móviles que podrían generar inestabilidad
   - La complejidad aumenta el riesgo de errores de implementación y dificulta la atribución de rendimiento

2. **Calibración del HMM**:
   - Los modelos HMM son notoriamente sensibles a la inicialización y pueden converger a óptimos locales
   - No especificas el método de entrenamiento y actualización del modelo HMM

3. **Gestión de transiciones entre regímenes**:
   - Las transiciones abruptas entre regímenes podrían generar rotación excesiva de la cartera
   - No hay mecanismo explícito para suavizar estas transiciones

4. **Validación de la señal de momentum residual**:
   - El ajuste por beta podría no capturar adecuadamente la exposición a factores sistemáticos
   - Considerar exposición a otros factores (valor, tamaño, calidad) además del beta de mercado

5. **Costos de implementación**:
   - La estrategia implica rebalanceo frecuente y potencialmente alta rotación
   - Los costos de transacción podrían erosionar significativamente el rendimiento

## Mejoras Propuestas

### 1. Refinamiento del Modelo de Regímenes

- **Implementar un enfoque de "régimen suavizado"**:
  - En lugar de clasificación discreta, utilizar probabilidades de pertenencia a cada régimen
  - Calcular señales como: Señal_Final = Σ(Prob_Régimen_i * Señal_Régimen_i)
  - Esto reduce la rotación en transiciones y mejora la robustez

- **Validación cruzada para selección de estados**:
  - Evaluar sistemáticamente modelos con 2-5 estados para determinar el número óptimo
  - Utilizar criterios de información (AIC/BIC) junto con métricas de rendimiento

- **Incorporar variables macroeconómicas como features auxiliares**:
  - Pendiente de la curva de rendimiento (10Y-2Y)
  - Spreads de crédito (BBB-Treasury)
  - Estas variables pueden mejorar la identificación temprana de cambios de régimen

### 2. Mejora del Sistema de Momentum Residual

- **Implementar un modelo multifactorial completo**:
  - Ajustar por exposición a factores Fama-French (Mercado, Tamaño, Valor, Momentum, Calidad)
  - Esto proporciona una medida más precisa del alpha verdadero

- **Incorporar asimetría en la señal de momentum**:
  - Evidencia empírica sugiere que el momentum negativo (drawdowns) tiene diferentes características que el momentum positivo
  - Calibrar separadamente los parámetros para señales positivas y negativas

- **Añadir filtro de tendencia de largo plazo**:
  - Comparar precio actual con media móvil de 200 días
  - Ajustar la exposición al momentum de corto plazo según la dirección de la tendencia de largo plazo

### 3. Optimización del Sistema de Ensemble

- **Implementar stacking en lugar de promedio simple**:
  - Entrenar un meta-modelo (regresión regularizada) que combine las predicciones de los modelos base
  - Esto permite capturar interacciones entre modelos y mejorar la precisión

- **Diversificar los métodos de ensemble**:
  - Incluir modelos basados en diferentes técnicas (no solo variaciones paramétricas)
  - Por ejemplo: modelos basados en momentum, reversión, volatilidad, etc.

- **Implementar pruning dinámico del ensemble**:
  - Monitorear continuamente el rendimiento de cada modelo componente
  - Eliminar temporalmente modelos con rendimiento persistentemente pobre

### 4. Refinamiento de la Gestión de Riesgo

- **Incorporar VaR condicional (CVaR/Expected Shortfall)**:
  - Limitar la exposición basándose en el CVaR estimado bajo diferentes escenarios
  - Esto proporciona mejor protección contra eventos extremos que el control de drawdown simple

- **Implementar stops basados en tiempo además de precio**:
  - Reducir posiciones que no muestran el comportamiento esperado después de n días
  - Esto mitiga el riesgo de "posiciones zombi" que consumen capital sin generar retorno

- **Añadir filtro de liquidez de mercado**:
  - Monitorear spreads bid-ask y profundidad de mercado (si disponible)
  - Reducir exposición cuando la liquidez se deteriora

### 5. Mejoras en la Implementación y Backtesting

- **Implementar simulación de órdenes realista**:
  - Modelar slippage basado en volatilidad y volumen
  - Incorporar costos de financiamiento para posiciones apalancadas

- **Realizar backtesting con bootstrapping de bloques**:
  - Generar múltiples trayectorias sintéticas preservando la estructura de autocorrelación
  - Esto proporciona una distribución más realista de resultados posibles

- **Implementar análisis de robustez paramétrica sistemático**:
  - Evaluar rendimiento en una cuadrícula de parámetros (no solo ±20%)
  - Identificar regiones de estabilidad paramétrica vs. regiones sensibles

## Preguntas Técnicas Específicas

1. ¿Cómo determinas específicamente los parámetros del modelo HMM y con qué frecuencia los recalibras?

2. ¿Has considerado el impacto de eventos corporativos (splits, dividendos) en el cálculo de las señales de momentum?

3. ¿Qué método específico utilizas para estimar el beta en el cálculo del rendimiento residual? ¿Consideras beta dinámico?

4. ¿Has evaluado el rendimiento de la estrategia en diferentes entornos de volatilidad y liquidez?

5. ¿Cómo manejas específicamente los períodos de datos faltantes o de baja calidad en yfinance?

6. ¿Has considerado incorporar señales basadas en volumen o flujo de órdenes para complementar las señales de precio?

7. ¿Qué horizonte temporal utilizas para la optimización de parámetros y cómo evitas el sesgo de optimización?

Esta estrategia tiene potencial significativo, pero recomendaría simplificar algunos componentes para mejorar la robustez y facilitar la implementación y monitoreo. Recuerda que en estrategias cuantitativas, la simplicidad y robustez suelen superar a la complejidad excesiva en el largo plazo.


--- sintetizador (STEP 7.1) ---
INPUT 1 (user):
=== SÍNTESIS DE IDEA ===

PROPUESTA FINAL DEL IDEADOR:
# Estrategia de Momentum Adaptativo con Regímenes de Mercado y Control Dinámico de Riesgo

Agradezco tu análisis crítico detallado. Has identificado puntos clave para mejorar la robustez y efectividad de la estrategia. Basándome en tus sugerencias, he refinado significativamente el enfoque:

## Concepto Fundamental Refinado

Esta estrategia combina momentum residual multi-horizonte con identificación de regímenes de mercado y gestión dinámica del riesgo para capturar alpha en el S&P 500 mientras minimiza drawdowns.

## Componentes Clave Mejorados

### 1. Sistema de Momentum Residual Multi-Horizonte

- **Cálculo de rendimientos residuales**:
  - Para cada horizonte temporal (5, 21, 63, 126 días), calculo el rendimiento del S&P 500
  - Ajusto por beta de mercado: Rendimiento_Residual = Rendimiento_Real - (Beta * Rendimiento_Mercado)
  - Para horizontes cortos (1-5 días), implemento señales contrarias para capturar reversión a la media
  - Para horizontes medianos/largos (>21 días), mantengo señales de momentum direccional

- **Normalización adaptativa**:
  - Cada señal se normaliza por su volatilidad específica utilizando desviación estándar móvil adaptativa
  - Implemento winsorización dinámica (basada en desviaciones MAD) para mitigar efectos de outliers

### 2. Identificación de Regímenes de Mercado (Modelo HMM)

- **Modelo de Markov oculto con 3 estados**:
  - Régimen 1: Mercado en tendencia (baja volatilidad, autocorrelación positiva)
  - Régimen 2: Mercado en rango (baja volatilidad, autocorrelación negativa)
  - Régimen 3: Mercado en crisis (alta volatilidad, correlaciones inestables)

- **Variables de observación para inferencia de régimen**:
  - Volatilidad realizada (ATR de 21 días / precio)
  - Autocorrelación de rendimientos (ventana de 21 días)
  - Ratio de días positivos/negativos (ventana de 21 días)
  - Divergencia de volatilidad implícita vs. realizada (cuando disponible)

- **Ponderación específica por régimen**:
  - En régimen de tendencia: Mayor peso a señales de momentum de medio/largo plazo
  - En régimen de rango: Mayor peso a señales de reversión a corto plazo
  - En régimen de crisis: Reducción general de exposición y mayor peso a filtros de volatilidad

### 3. Sistema de Ensemble con Bootstrapping

- **Generación de múltiples modelos**:
  - Creo 10-15 variantes de la estrategia con ligeras variaciones en parámetros clave
  - Cada variante se entrena en diferentes subconjuntos de datos históricos (block bootstrap)
  - Las señales finales son un promedio ponderado por la robustez histórica de cada modelo

- **Adaptación por validación cruzada temporal**:
  - Implemento walk-forward validation con ventanas de entrenamiento de 2 años y ventanas de prueba de 6 meses
  - Los pesos del ensemble se actualizan trimestralmente basados en rendimiento reciente

### 4. Gestión Dinámica del Riesgo

- **Control de drawdown adaptativo**:
  - Exposición = Exposición_Base * (1 - (Drawdown_Actual/Drawdown_Máximo_Tolerado)^2)
  - Drawdown_Máximo_Tolerado se establece en 15% (calibrable)
  - Recuperación gradual: la exposición se incrementa progresivamente tras reducción por drawdown

- **Filtro de tendencia de volatilidad**:
  - Calculo el ratio de volatilidad actual (21 días) vs. volatilidad de largo plazo (63 días)
  - Reduzco exposición cuando volatilidad_actual/volatilidad_tendencia > 1.2
  - Reduzco exposición adicional cuando la derivada de la volatilidad es positiva (volatilidad creciente)

- **Stops dinámicos contextuales**:
  - Stop-loss basado en ATR * multiplicador específico por régimen:
    - Régimen tendencia: 3.0 * ATR
    - Régimen rango: 2.0 * ATR
    - Régimen crisis: 1.5 * ATR

### 5. Optimización de Ejecución

- **Banda de no-transacción adaptativa**:
  - Umbral mínimo = máx(0.5%, costos_transacción * 3)
  - Solo ejecuto ajustes cuando el cambio en posición deseada > umbral mínimo
  - El umbral se ajusta dinámicamente según volatilidad reciente

- **Timing de ejecución optimizado**:
  - Rebalanceo semanal (viernes cierre) como base
  - Análisis de patrones estacionales intraweek para determinar día óptimo de ejecución
  - Monitoreo continuo para stops y condiciones extremas

## Métricas Esperadas y Validación

- **Métricas objetivo**:
  - Sharpe ratio esperado: 1.2-1.5 (post-costos)
  - Drawdown máximo esperado: 12-15%
  - Correlación con S&P 500: 0.3-0.5 (diversificación significativa)
  - Ratio de captación alcista/bajista: 0.7/0.3 (asimétrico)

- **Validación estadística robusta**:
  - Simulaciones de Monte Carlo con block bootstrap (bloques de 21 días)
  - Análisis de sensibilidad paramétrica sistemático (±20% en parámetros clave)
  - Pruebas de robustez en diferentes regímenes históricos (crisis 2008, COVID, etc.)
  - Análisis de atribución de rendimiento para verificar fuentes de alpha

## Respuestas a Preguntas Técnicas

1. **Tamaño de posición**: Utilizo una función sigmoide para transformar el score combinado:
   - Posición = Exposición_Máxima * 2 / (1 + exp(-k * Score_Combinado)) - 1
   - Donde k es un parámetro de sensibilidad calibrado para que el 80% de las señales estén entre ±0.7 de exposición máxima

2. **Suavizado exponencial**: Utilizo un factor de decaimiento de 0.94 para horizontes semanales, ajustado proporcionalmente para otros horizontes (λ = 0.94^(5/n) para n días)

3. **Umbrales adaptativos**: Los umbrales se basan en percentiles condicionales históricos:
   - Umbral_Alto = percentil_75 + (percentil_95 - percentil_75) * factor_régimen
   - Umbral_Bajo = percentil_25 - (percentil_25 - percentil_5) * factor_régimen
   - factor_régimen varía según el régimen identificado

4. **Modelo bayesiano**: Utilizo una distribución prior normal para los pesos, con media inicial basada en backtest y varianza proporcional a la incertidumbre. La actualización sigue el teorema de Bayes estándar con likelihood gaussiana.

5. **Integración de volumen**: El factor de volumen modifica multiplicativamente la señal de momentum:
   - Score_Final = Score_Momentum * (0.7 + 0.3 * Score_Volumen)
   - Esto permite que la señal mantenga dirección pero se atenúe cuando el volumen no confirma

6. **Manejo de outliers**: Implemento winsorización dinámica basada en desviaciones MAD:
   - Umbral_Superior = mediana + 3 * MAD
   - Umbral_Inferior = mediana - 3 * MAD
   - Valores fuera de este rango se recortan a los umbrales

7. **ATR para stops**: Calculo ATR con período de 14 días, y los multiplicadores varían según el régimen (1.5-3.0 como indicado anteriormente)

Esta estrategia refinada aborda sistemáticamente las limitaciones identificadas, incorporando un enfoque más robusto para la captura de alpha mientras mantiene controles de riesgo adaptativos.

FEEDBACK FINAL DEL CURADOR:
He revisado detalladamente tu estrategia refinada de Momentum Adaptativo con Regímenes de Mercado y Control Dinámico de Riesgo. Veo que has incorporado muchas mejoras significativas que abordan varios de los puntos críticos en estrategias cuantitativas. Permíteme analizar y proponer algunas mejoras adicionales:

## Análisis Crítico

### Fortalezas:
- La combinación de momentum residual multi-horizonte con identificación de regímenes es conceptualmente sólida
- El enfoque de ensemble con bootstrapping mitiga significativamente el riesgo de sobreoptimización
- La gestión dinámica del riesgo con controles de drawdown adaptativos es sofisticada
- La validación estadística propuesta es rigurosa

### Áreas de Mejora:

1. **Riesgo de complejidad excesiva**:
   - El sistema tiene muchos componentes móviles que podrían generar inestabilidad
   - La complejidad aumenta el riesgo de errores de implementación y dificulta la atribución de rendimiento

2. **Calibración del HMM**:
   - Los modelos HMM son notoriamente sensibles a la inicialización y pueden converger a óptimos locales
   - No especificas el método de entrenamiento y actualización del modelo HMM

3. **Gestión de transiciones entre regímenes**:
   - Las transiciones abruptas entre regímenes podrían generar rotación excesiva de la cartera
   - No hay mecanismo explícito para suavizar estas transiciones

4. **Validación de la señal de momentum residual**:
   - El ajuste por beta podría no capturar adecuadamente la exposición a factores sistemáticos
   - Considerar exposición a otros factores (valor, tamaño, calidad) además del beta de mercado

5. **Costos de implementación**:
   - La estrategia implica rebalanceo frecuente y potencialmente alta rotación
   - Los costos de transacción podrían erosionar significativamente el rendimiento

## Mejoras Propuestas

### 1. Refinamiento del Modelo de Regímenes

- **Implementar un enfoque de "régimen suavizado"**:
  - En lugar de clasificación discreta, utilizar probabilidades de pertenencia a cada régimen
  - Calcular señales como: Señal_Final = Σ(Prob_Régimen_i * Señal_Régimen_i)
  - Esto reduce la rotación en transiciones y mejora la robustez

- **Validación cruzada para selección de estados**:
  - Evaluar sistemáticamente modelos con 2-5 estados para determinar el número óptimo
  - Utilizar criterios de información (AIC/BIC) junto con métricas de rendimiento

- **Incorporar variables macroeconómicas como features auxiliares**:
  - Pendiente de la curva de rendimiento (10Y-2Y)
  - Spreads de crédito (BBB-Treasury)
  - Estas variables pueden mejorar la identificación temprana de cambios de régimen

### 2. Mejora del Sistema de Momentum Residual

- **Implementar un modelo multifactorial completo**:
  - Ajustar por exposición a factores Fama-French (Mercado, Tamaño, Valor, Momentum, Calidad)
  - Esto proporciona una medida más precisa del alpha verdadero

- **Incorporar asimetría en la señal de momentum**:
  - Evidencia empírica sugiere que el momentum negativo (drawdowns) tiene diferentes características que el momentum positivo
  - Calibrar separadamente los parámetros para señales positivas y negativas

- **Añadir filtro de tendencia de largo plazo**:
  - Comparar precio actual con media móvil de 200 días
  - Ajustar la exposición al momentum de corto plazo según la dirección de la tendencia de largo plazo

### 3. Optimización del Sistema de Ensemble

- **Implementar stacking en lugar de promedio simple**:
  - Entrenar un meta-modelo (regresión regularizada) que combine las predicciones de los modelos base
  - Esto permite capturar interacciones entre modelos y mejorar la precisión

- **Diversificar los métodos de ensemble**:
  - Incluir modelos basados en diferentes técnicas (no solo variaciones paramétricas)
  - Por ejemplo: modelos basados en momentum, reversión, volatilidad, etc.

- **Implementar pruning dinámico del ensemble**:
  - Monitorear continuamente el rendimiento de cada modelo componente
  - Eliminar temporalmente modelos con rendimiento persistentemente pobre

### 4. Refinamiento de la Gestión de Riesgo

- **Incorporar VaR condicional (CVaR/Expected Shortfall)**:
  - Limitar la exposición basándose en el CVaR estimado bajo diferentes escenarios
  - Esto proporciona mejor protección contra eventos extremos que el control de drawdown simple

- **Implementar stops basados en tiempo además de precio**:
  - Reducir posiciones que no muestran el comportamiento esperado después de n días
  - Esto mitiga el riesgo de "posiciones zombi" que consumen capital sin generar retorno

- **Añadir filtro de liquidez de mercado**:
  - Monitorear spreads bid-ask y profundidad de mercado (si disponible)
  - Reducir exposición cuando la liquidez se deteriora

### 5. Mejoras en la Implementación y Backtesting

- **Implementar simulación de órdenes realista**:
  - Modelar slippage basado en volatilidad y volumen
  - Incorporar costos de financiamiento para posiciones apalancadas

- **Realizar backtesting con bootstrapping de bloques**:
  - Generar múltiples trayectorias sintéticas preservando la estructura de autocorrelación
  - Esto proporciona una distribución más realista de resultados posibles

- **Implementar análisis de robustez paramétrica sistemático**:
  - Evaluar rendimiento en una cuadrícula de parámetros (no solo ±20%)
  - Identificar regiones de estabilidad paramétrica vs. regiones sensibles

## Preguntas Técnicas Específicas

1. ¿Cómo determinas específicamente los parámetros del modelo HMM y con qué frecuencia los recalibras?

2. ¿Has considerado el impacto de eventos corporativos (splits, dividendos) en el cálculo de las señales de momentum?

3. ¿Qué método específico utilizas para estimar el beta en el cálculo del rendimiento residual? ¿Consideras beta dinámico?

4. ¿Has evaluado el rendimiento de la estrategia en diferentes entornos de volatilidad y liquidez?

5. ¿Cómo manejas específicamente los períodos de datos faltantes o de baja calidad en yfinance?

6. ¿Has considerado incorporar señales basadas en volumen o flujo de órdenes para complementar las señales de precio?

7. ¿Qué horizonte temporal utilizas para la optimización de parámetros y cómo evitas el sesgo de optimización?

Esta estrategia tiene potencial significativo, pero recomendaría simplificar algunos componentes para mejorar la robustez y facilitar la implementación y monitoreo. Recuerda que en estrategias cuantitativas, la simplicidad y robustez suelen superar a la complejidad excesiva en el largo plazo.

Tu tarea es sintetizar ambas contribuciones en una IDEA FINAL coherente y completa.


RESPONSE:
# IDEA FINAL: ESTRATEGIA DE MOMENTUM ADAPTATIVO CON REGÍMENES DE MERCADO Y CONTROL DINÁMICO DE RIESGO

## Descripción

Esta estrategia combina momentum residual multi-horizonte con identificación de regímenes de mercado y gestión dinámica del riesgo para capturar alpha en el S&P 500 mientras minimiza drawdowns. El enfoque integra análisis cuantitativo avanzado con controles de riesgo adaptativos para crear un sistema de trading robusto que se ajusta a diferentes condiciones de mercado.

## Características principales

### 1. Sistema de Momentum Residual Multi-Horizonte

- **Cálculo de rendimientos residuales**:
  - Análisis de múltiples horizontes temporales (5, 21, 63, 126 días)
  - Ajuste por modelo multifactorial: Rendimiento_Residual = Rendimiento_Real - (Exposición_Factores * Rendimiento_Factores)
  - Factores incluidos: Mercado, Tamaño, Valor, Momentum, Calidad
  - Señales contrarias para horizontes cortos (1-5 días) para capturar reversión a la media
  - Señales de momentum direccional para horizontes medianos/largos (>21 días)

- **Normalización adaptativa**:
  - Normalización por volatilidad específica usando desviación estándar móvil adaptativa
  - Winsorización dinámica basada en desviaciones MAD para mitigar outliers:
    - Umbral_Superior = mediana + 3 * MAD
    - Umbral_Inferior = mediana - 3 * MAD

- **Filtro de tendencia de largo plazo**:
  - Comparación del precio actual con media móvil de 200 días
  - Ajuste de exposición al momentum de corto plazo según dirección de tendencia de largo plazo

### 2. Identificación de Regímenes de Mercado (Modelo HMM Suavizado)

- **Modelo de Markov oculto con 3 estados**:
  - Régimen 1: Mercado en tendencia (baja volatilidad, autocorrelación positiva)
  - Régimen 2: Mercado en rango (baja volatilidad, autocorrelación negativa)
  - Régimen 3: Mercado en crisis (alta volatilidad, correlaciones inestables)

- **Variables de observación para inferencia de régimen**:
  - Volatilidad realizada (ATR de 21 días / precio)
  - Autocorrelación de rendimientos (ventana de 21 días)
  - Ratio de días positivos/negativos (ventana de 21 días)
  - Divergencia de volatilidad implícita vs. realizada (cuando disponible)
  - Variables macroeconómicas auxiliares:
    - Pendiente de la curva de rendimiento (10Y-2Y)
    - Spreads de crédito (BBB-Treasury)

- **Enfoque de régimen suavizado**:
  - Utilización de probabilidades de pertenencia a cada régimen en lugar de clasificación discreta
  - Señal_Final = Σ(Prob_Régimen_i * Señal_Régimen_i)
  - Recalibración trimestral del modelo HMM con validación cruzada para selección óptima de estados

### 3. Sistema de Ensemble con Stacking

- **Generación de múltiples modelos**:
  - 10-15 variantes de la estrategia con diferentes técnicas y parámetros
  - Entrenamiento en diferentes subconjuntos de datos históricos (block bootstrap)
  - Diversificación de métodos: modelos basados en momentum, reversión, volatilidad, etc.

- **Implementación de stacking**:
  - Meta-modelo (regresión regularizada) para combinar predicciones de modelos base
  - Captura de interacciones entre modelos para mejorar precisión
  - Pruning dinámico: eliminación temporal de modelos con rendimiento persistentemente pobre

- **Adaptación por validación cruzada temporal**:
  - Walk-forward validation con ventanas de entrenamiento de 2 años y prueba de 6 meses
  - Actualización trimestral de pesos del ensemble basada en rendimiento reciente

### 4. Gestión Dinámica del Riesgo

- **Control de drawdown adaptativo**:
  - Exposición = Exposición_Base * (1 - (Drawdown_Actual/Drawdown_Máximo_Tolerado)^2)
  - Drawdown_Máximo_Tolerado = 15% (calibrable)
  - Recuperación gradual post-reducción por drawdown

- **VaR condicional (CVaR/Expected Shortfall)**:
  - Limitación de exposición basada en CVaR estimado bajo diferentes escenarios
  - Protección mejorada contra eventos extremos

- **Filtro de tendencia de volatilidad**:
  - Ratio de volatilidad actual (21 días) vs. volatilidad de largo plazo (63 días)
  - Reducción de exposición cuando volatilidad_actual/volatilidad_tendencia > 1.2
  - Reducción adicional cuando la derivada de la volatilidad es positiva

- **Stops dinámicos contextuales**:
  - Stop-loss basado en ATR * multiplicador específico por régimen:
    - Régimen tendencia: 3.0 * ATR
    - Régimen rango: 2.0 * ATR
    - Régimen crisis: 1.5 * ATR
  - Stops basados en tiempo: reducción de posiciones sin comportamiento esperado después de n días

### 5. Optimización de Ejecución

- **Banda de no-transacción adaptativa**:
  - Umbral mínimo = máx(0.5%, costos_transacción * 3)
  - Ejecución solo cuando el cambio en posición deseada > umbral mínimo
  - Ajuste dinámico según volatilidad reciente

- **Timing de ejecución optimizado**:
  - Rebalanceo semanal (viernes cierre) como base
  - Análisis de patrones estacionales intraweek para día óptimo de ejecución
  - Monitoreo continuo para stops y condiciones extremas

- **Simulación de órdenes realista**:
  - Modelado de slippage basado en volatilidad y volumen
  - Incorporación de costos de financiamiento para posiciones apalancadas

## Implementación

### 1. Cálculo de Señales

```python
# Pseudocódigo para cálculo de señales
def calcular_senales(precios, factores, parametros):
    # Calcular rendimientos para diferentes horizontes
    rendimientos = {}
    for horizonte in [5, 21, 63, 126]:
        rendimientos[horizonte] = calcular_rendimiento(precios, horizonte)
    
    # Calcular exposición a factores y rendimientos residuales
    rendimientos_residuales = {}
    for horizonte, rend in rendimientos.items():
        exposicion = calcular_exposicion_factores(rend, factores, horizonte)
        rendimientos_residuales[horizonte] = rend - np.dot(exposicion, factores)
    
    # Normalizar señales
    senales_normalizadas = {}
    for horizonte, rend_res in rendimientos_residuales.items():
        vol = calcular_volatilidad_adaptativa(rend_res, horizonte)
        senales_normalizadas[horizonte] = rend_res / vol
        
        # Aplicar winsorización
        mad = calcular_mad(senales_normalizadas[horizonte])
        mediana = np.median(senales_normalizadas[horizonte])
        umbral_sup = mediana + 3 * mad
        umbral_inf = mediana - 3 * mad
        senales_normalizadas[horizonte] = np.clip(senales_normalizadas[horizonte], umbral_inf, umbral_sup)
    
    # Invertir señal para horizontes cortos (reversión a la media)
    if horizonte <= 5:
        senales_normalizadas[horizonte] = -senales_normalizadas[horizonte]
    
    return senales_normalizadas
```

### 2. Identificación de Regímenes

```python
# Pseudocódigo para identificación de regímenes
def identificar_regimenes(precios, datos_macro, parametros):
    # Calcular features para el HMM
    features = []
    
    # Volatilidad realizada
    atr = calcular_atr(precios, 21)
    features.append(atr / precios)
    
    # Autocorrelación
    autocorr = calcular_autocorrelacion(precios, 21)
    features.append(autocorr)
    
    # Ratio días positivos/negativos
    ratio_dias = calcular_ratio_dias_pos_neg(precios, 21)
    features.append(ratio_dias)
    
    # Variables macro
    pendiente_curva = datos_macro['10Y'] - datos_macro['2Y']
    features.append(pendiente_curva)
    spread_credito = datos_macro['BBB'] - datos_macro['Treasury']
    features.append(spread_credito)
    
    # Entrenar/actualizar modelo HMM
    if necesita_recalibracion():
        modelo_hmm = entrenar_hmm(features_historicos, n_estados=3)
    
    # Obtener probabilidades de régimen
    prob_regimenes = modelo_hmm.predict_proba(features)
    
    return prob_regimenes
```

### 3. Combinación de Señales y Gestión de Riesgo

```python
# Pseudocódigo para combinación de señales y gestión de riesgo
def combinar_senales(senales, prob_regimenes, estado_portfolio, parametros):
    # Pesos base por horizonte según régimen
    pesos_horizonte = {}
    for horizonte in senales.keys():
        pesos_horizonte[horizonte] = 0
        for i, prob in enumerate(prob_regimenes):
            if i == 0:  # Régimen tendencia
                if horizonte >= 21:
                    pesos_horizonte[horizonte] += prob * 0.4  # Mayor peso a medio/largo plazo
                else:
                    pesos_horizonte[horizonte] += prob * 0.1
            elif i == 1:  # Régimen rango
                if horizonte <= 5:
                    pesos_horizonte[horizonte] += prob * 0.5  # Mayor peso a corto plazo (reversión)
                else:
                    pesos_horizonte[horizonte] += prob * 0.1
            else:  # Régimen crisis
                pesos_horizonte[horizonte] += prob * 0.2  # Reducción general
    
    # Combinar señales ponderadas
    senal_combinada = 0
    for horizonte, senal in senales.items():
        senal_combinada += senal * pesos_horizonte[horizonte]
    
    # Aplicar control de drawdown
    drawdown_actual = calcular_drawdown(estado_portfolio)
    drawdown_max = parametros['drawdown_maximo']
    factor_drawdown = 1 - (drawdown_actual/drawdown_max)**2
    factor_drawdown = max(0, min(1, factor_drawdown))
    
    # Aplicar filtro de volatilidad
    vol_actual = calcular_volatilidad(precios, 21)
    vol_tendencia = calcular_volatilidad(precios, 63)
    ratio_vol = vol_actual / vol_tendencia
    factor_vol = 1.0
    if ratio_vol > 1.2:
        factor_vol = 1.2 / ratio_vol
    
    # Calcular exposición final
    exposicion_base = parametros['exposicion_maxima']
    exposicion = exposicion_base * factor_drawdown * factor_vol
    
    # Transformar señal en posición usando función sigmoide
    k = parametros['sensibilidad_sigmoide']
    posicion = exposicion * (2 / (1 + np.exp(-k * senal_combinada)) - 1)
    
    return posicion
```

### 4. Backtesting y Validación

```python
# Pseudocódigo para backtesting y validación
def backtest(datos, parametros):
    resultados = []
    
    # Walk-forward validation
    for inicio_test, fin_test in generar_ventanas_test(datos):
        # Datos de entrenamiento hasta inicio_test
        datos_train = datos[:inicio_test]
        
        # Entrenar ensemble de modelos
        modelos = entrenar_ensemble(datos_train, parametros)
        
        # Evaluar en ventana de test
        for fecha in range(inicio_test, fin_test):
            # Obtener señales de cada modelo
            senales_modelos = []
            for modelo in modelos:
                senal = modelo.generar_senal(datos, fecha)
                senales_modelos.append(senal)
            
            # Combinar señales con meta-modelo
            senal_combinada = meta_modelo.predecir(senales_modelos)
            
            # Aplicar gestión de riesgo
            posicion = aplicar_gestion_riesgo(senal_combinada, estado_portfolio)
            
            # Simular ejecución con costos
            rendimiento = simular_ejecucion(posicion, datos, fecha, costos)
            
            resultados.append({
                'fecha': fecha,
                'posicion': posicion,
                'rendimiento': rendimiento
            })
    
    # Calcular métricas
    sharpe = calcular_sharpe(resultados)
    drawdown_max = calcular_max_drawdown(resultados)
    correlacion_sp500 = calcular_correlacion(resultados, sp500)
    
    return {
        'resultados': resultados,
        'metricas': {
            'sharpe': sharpe,
            'drawdown_max': drawdown_max,
            'correlacion_sp500': correlacion_sp500
        }
    }
```

### 5. Métricas Esperadas

- **Sharpe ratio esperado**: 1.2-1.5 (post-costos)
- **Drawdown máximo esperado**: 12-15%
- **Correlación con S&P 500**: 0.3-0.5 (diversificación significativa)
- **Ratio de captación alcista/bajista**: 0.7/0.3 (asimétrico)

### 6. Consideraciones de Implementación

- Utilizar datos de yfinance para precios y volúmenes
- Implementar manejo de errores robusto para datos faltantes o erróneos
- Actualizar parámetros del modelo trimestralmente
- Monitorear continuamente el rendimiento y las métricas de riesgo
- Realizar análisis de atribución de rendimiento mensual para verificar fuentes de alpha
- Documentar todas las decisiones de diseño y parámetros para facilitar auditoría y mejora continua

Esta estrategia equilibra sofisticación analítica con robustez operativa, adaptándose a diferentes condiciones de mercado mientras mantiene un control riguroso del riesgo.


--- desarrollador (STEP 8.1) ---
INPUT 1 (user):
# IDEA FINAL: ESTRATEGIA DE MOMENTUM ADAPTATIVO CON REGÍMENES DE MERCADO Y CONTROL DINÁMICO DE RIESGO

## Descripción

Esta estrategia combina momentum residual multi-horizonte con identificación de regímenes de mercado y gestión dinámica del riesgo para capturar alpha en el S&P 500 mientras minimiza drawdowns. El enfoque integra análisis cuantitativo avanzado con controles de riesgo adaptativos para crear un sistema de trading robusto que se ajusta a diferentes condiciones de mercado.

## Características principales

### 1. Sistema de Momentum Residual Multi-Horizonte

- **Cálculo de rendimientos residuales**:
  - Análisis de múltiples horizontes temporales (5, 21, 63, 126 días)
  - Ajuste por modelo multifactorial: Rendimiento_Residual = Rendimiento_Real - (Exposición_Factores * Rendimiento_Factores)
  - Factores incluidos: Mercado, Tamaño, Valor, Momentum, Calidad
  - Señales contrarias para horizontes cortos (1-5 días) para capturar reversión a la media
  - Señales de momentum direccional para horizontes medianos/largos (>21 días)

- **Normalización adaptativa**:
  - Normalización por volatilidad específica usando desviación estándar móvil adaptativa
  - Winsorización dinámica basada en desviaciones MAD para mitigar outliers:
    - Umbral_Superior = mediana + 3 * MAD
    - Umbral_Inferior = mediana - 3 * MAD

- **Filtro de tendencia de largo plazo**:
  - Comparación del precio actual con media móvil de 200 días
  - Ajuste de exposición al momentum de corto plazo según dirección de tendencia de largo plazo

### 2. Identificación de Regímenes de Mercado (Modelo HMM Suavizado)

- **Modelo de Markov oculto con 3 estados**:
  - Régimen 1: Mercado en tendencia (baja volatilidad, autocorrelación positiva)
  - Régimen 2: Mercado en rango (baja volatilidad, autocorrelación negativa)
  - Régimen 3: Mercado en crisis (alta volatilidad, correlaciones inestables)

- **Variables de observación para inferencia de régimen**:
  - Volatilidad realizada (ATR de 21 días / precio)
  - Autocorrelación de rendimientos (ventana de 21 días)
  - Ratio de días positivos/negativos (ventana de 21 días)
  - Divergencia de volatilidad implícita vs. realizada (cuando disponible)
  - Variables macroeconómicas auxiliares:
    - Pendiente de la curva de rendimiento (10Y-2Y)
    - Spreads de crédito (BBB-Treasury)

- **Enfoque de régimen suavizado**:
  - Utilización de probabilidades de pertenencia a cada régimen en lugar de clasificación discreta
  - Señal_Final = Σ(Prob_Régimen_i * Señal_Régimen_i)
  - Recalibración trimestral del modelo HMM con validación cruzada para selección óptima de estados

### 3. Sistema de Ensemble con Stacking

- **Generación de múltiples modelos**:
  - 10-15 variantes de la estrategia con diferentes técnicas y parámetros
  - Entrenamiento en diferentes subconjuntos de datos históricos (block bootstrap)
  - Diversificación de métodos: modelos basados en momentum, reversión, volatilidad, etc.

- **Implementación de stacking**:
  - Meta-modelo (regresión regularizada) para combinar predicciones de modelos base
  - Captura de interacciones entre modelos para mejorar precisión
  - Pruning dinámico: eliminación temporal de modelos con rendimiento persistentemente pobre

- **Adaptación por validación cruzada temporal**:
  - Walk-forward validation con ventanas de entrenamiento de 2 años y prueba de 6 meses
  - Actualización trimestral de pesos del ensemble basada en rendimiento reciente

### 4. Gestión Dinámica del Riesgo

- **Control de drawdown adaptativo**:
  - Exposición = Exposición_Base * (1 - (Drawdown_Actual/Drawdown_Máximo_Tolerado)^2)
  - Drawdown_Máximo_Tolerado = 15% (calibrable)
  - Recuperación gradual post-reducción por drawdown

- **VaR condicional (CVaR/Expected Shortfall)**:
  - Limitación de exposición basada en CVaR estimado bajo diferentes escenarios
  - Protección mejorada contra eventos extremos

- **Filtro de tendencia de volatilidad**:
  - Ratio de volatilidad actual (21 días) vs. volatilidad de largo plazo (63 días)
  - Reducción de exposición cuando volatilidad_actual/volatilidad_tendencia > 1.2
  - Reducción adicional cuando la derivada de la volatilidad es positiva

- **Stops dinámicos contextuales**:
  - Stop-loss basado en ATR * multiplicador específico por régimen:
    - Régimen tendencia: 3.0 * ATR
    - Régimen rango: 2.0 * ATR
    - Régimen crisis: 1.5 * ATR
  - Stops basados en tiempo: reducción de posiciones sin comportamiento esperado después de n días

### 5. Optimización de Ejecución

- **Banda de no-transacción adaptativa**:
  - Umbral mínimo = máx(0.5%, costos_transacción * 3)
  - Ejecución solo cuando el cambio en posición deseada > umbral mínimo
  - Ajuste dinámico según volatilidad reciente

- **Timing de ejecución optimizado**:
  - Rebalanceo semanal (viernes cierre) como base
  - Análisis de patrones estacionales intraweek para día óptimo de ejecución
  - Monitoreo continuo para stops y condiciones extremas

- **Simulación de órdenes realista**:
  - Modelado de slippage basado en volatilidad y volumen
  - Incorporación de costos de financiamiento para posiciones apalancadas

## Implementación

### 1. Cálculo de Señales

```python
# Pseudocódigo para cálculo de señales
def calcular_senales(precios, factores, parametros):
    # Calcular rendimientos para diferentes horizontes
    rendimientos = {}
    for horizonte in [5, 21, 63, 126]:
        rendimientos[horizonte] = calcular_rendimiento(precios, horizonte)
    
    # Calcular exposición a factores y rendimientos residuales
    rendimientos_residuales = {}
    for horizonte, rend in rendimientos.items():
        exposicion = calcular_exposicion_factores(rend, factores, horizonte)
        rendimientos_residuales[horizonte] = rend - np.dot(exposicion, factores)
    
    # Normalizar señales
    senales_normalizadas = {}
    for horizonte, rend_res in rendimientos_residuales.items():
        vol = calcular_volatilidad_adaptativa(rend_res, horizonte)
        senales_normalizadas[horizonte] = rend_res / vol
        
        # Aplicar winsorización
        mad = calcular_mad(senales_normalizadas[horizonte])
        mediana = np.median(senales_normalizadas[horizonte])
        umbral_sup = mediana + 3 * mad
        umbral_inf = mediana - 3 * mad
        senales_normalizadas[horizonte] = np.clip(senales_normalizadas[horizonte], umbral_inf, umbral_sup)
    
    # Invertir señal para horizontes cortos (reversión a la media)
    if horizonte <= 5:
        senales_normalizadas[horizonte] = -senales_normalizadas[horizonte]
    
    return senales_normalizadas
```

### 2. Identificación de Regímenes

```python
# Pseudocódigo para identificación de regímenes
def identificar_regimenes(precios, datos_macro, parametros):
    # Calcular features para el HMM
    features = []
    
    # Volatilidad realizada
    atr = calcular_atr(precios, 21)
    features.append(atr / precios)
    
    # Autocorrelación
    autocorr = calcular_autocorrelacion(precios, 21)
    features.append(autocorr)
    
    # Ratio días positivos/negativos
    ratio_dias = calcular_ratio_dias_pos_neg(precios, 21)
    features.append(ratio_dias)
    
    # Variables macro
    pendiente_curva = datos_macro['10Y'] - datos_macro['2Y']
    features.append(pendiente_curva)
    spread_credito = datos_macro['BBB'] - datos_macro['Treasury']
    features.append(spread_credito)
    
    # Entrenar/actualizar modelo HMM
    if necesita_recalibracion():
        modelo_hmm = entrenar_hmm(features_historicos, n_estados=3)
    
    # Obtener probabilidades de régimen
    prob_regimenes = modelo_hmm.predict_proba(features)
    
    return prob_regimenes
```

### 3. Combinación de Señales y Gestión de Riesgo

```python
# Pseudocódigo para combinación de señales y gestión de riesgo
def combinar_senales(senales, prob_regimenes, estado_portfolio, parametros):
    # Pesos base por horizonte según régimen
    pesos_horizonte = {}
    for horizonte in senales.keys():
        pesos_horizonte[horizonte] = 0
        for i, prob in enumerate(prob_regimenes):
            if i == 0:  # Régimen tendencia
                if horizonte >= 21:
                    pesos_horizonte[horizonte] += prob * 0.4  # Mayor peso a medio/largo plazo
                else:
                    pesos_horizonte[horizonte] += prob * 0.1
            elif i == 1:  # Régimen rango
                if horizonte <= 5:
                    pesos_horizonte[horizonte] += prob * 0.5  # Mayor peso a corto plazo (reversión)
                else:
                    pesos_horizonte[horizonte] += prob * 0.1
            else:  # Régimen crisis
                pesos_horizonte[horizonte] += prob * 0.2  # Reducción general
    
    # Combinar señales ponderadas
    senal_combinada = 0
    for horizonte, senal in senales.items():
        senal_combinada += senal * pesos_horizonte[horizonte]
    
    # Aplicar control de drawdown
    drawdown_actual = calcular_drawdown(estado_portfolio)
    drawdown_max = parametros['drawdown_maximo']
    factor_drawdown = 1 - (drawdown_actual/drawdown_max)**2
    factor_drawdown = max(0, min(1, factor_drawdown))
    
    # Aplicar filtro de volatilidad
    vol_actual = calcular_volatilidad(precios, 21)
    vol_tendencia = calcular_volatilidad(precios, 63)
    ratio_vol = vol_actual / vol_tendencia
    factor_vol = 1.0
    if ratio_vol > 1.2:
        factor_vol = 1.2 / ratio_vol
    
    # Calcular exposición final
    exposicion_base = parametros['exposicion_maxima']
    exposicion = exposicion_base * factor_drawdown * factor_vol
    
    # Transformar señal en posición usando función sigmoide
    k = parametros['sensibilidad_sigmoide']
    posicion = exposicion * (2 / (1 + np.exp(-k * senal_combinada)) - 1)
    
    return posicion
```

### 4. Backtesting y Validación

```python
# Pseudocódigo para backtesting y validación
def backtest(datos, parametros):
    resultados = []
    
    # Walk-forward validation
    for inicio_test, fin_test in generar_ventanas_test(datos):
        # Datos de entrenamiento hasta inicio_test
        datos_train = datos[:inicio_test]
        
        # Entrenar ensemble de modelos
        modelos = entrenar_ensemble(datos_train, parametros)
        
        # Evaluar en ventana de test
        for fecha in range(inicio_test, fin_test):
            # Obtener señales de cada modelo
            senales_modelos = []
            for modelo in modelos:
                senal = modelo.generar_senal(datos, fecha)
                senales_modelos.append(senal)
            
            # Combinar señales con meta-modelo
            senal_combinada = meta_modelo.predecir(senales_modelos)
            
            # Aplicar gestión de riesgo
            posicion = aplicar_gestion_riesgo(senal_combinada, estado_portfolio)
            
            # Simular ejecución con costos
            rendimiento = simular_ejecucion(posicion, datos, fecha, costos)
            
            resultados.append({
                'fecha': fecha,
                'posicion': posicion,
                'rendimiento': rendimiento
            })
    
    # Calcular métricas
    sharpe = calcular_sharpe(resultados)
    drawdown_max = calcular_max_drawdown(resultados)
    correlacion_sp500 = calcular_correlacion(resultados, sp500)
    
    return {
        'resultados': resultados,
        'metricas': {
            'sharpe': sharpe,
            'drawdown_max': drawdown_max,
            'correlacion_sp500': correlacion_sp500
        }
    }
```

### 5. Métricas Esperadas

- **Sharpe ratio esperado**: 1.2-1.5 (post-costos)
- **Drawdown máximo esperado**: 12-15%
- **Correlación con S&P 500**: 0.3-0.5 (diversificación significativa)
- **Ratio de captación alcista/bajista**: 0.7/0.3 (asimétrico)

### 6. Consideraciones de Implementación

- Utilizar datos de yfinance para precios y volúmenes
- Implementar manejo de errores robusto para datos faltantes o erróneos
- Actualizar parámetros del modelo trimestralmente
- Monitorear continuamente el rendimiento y las métricas de riesgo
- Realizar análisis de atribución de rendimiento mensual para verificar fuentes de alpha
- Documentar todas las decisiones de diseño y parámetros para facilitar auditoría y mejora continua

Esta estrategia equilibra sofisticación analítica con robustez operativa, adaptándose a diferentes condiciones de mercado mientras mantiene un control riguroso del riesgo.

RESPONSE:
```python
import os
import logging
import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.model_selection import TimeSeriesSplit
from hmmlearn import hmm
from scipy import stats
import warnings
from datetime import datetime, timedelta
import requests
from io import StringIO
from tqdm import tqdm

# Crear directorios para resultados
os.makedirs('./artifacts/results', exist_ok=True)
os.makedirs('./artifacts/results/figures', exist_ok=True)
os.makedirs('./artifacts/results/data', exist_ok=True)

# Configurar logging
logging.basicConfig(
    filename='./artifacts/errors.txt',
    level=logging.ERROR,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# Ignorar advertencias
warnings.filterwarnings('ignore')

class AdaptiveMomentumStrategy:
    def __init__(self, start_date='2010-01-01', end_date=None, lookback_years=5):
        """
        Inicializa la estrategia de momentum adaptativo con regímenes de mercado.
        
        Args:
            start_date: Fecha de inicio para el backtest
            end_date: Fecha de fin para el backtest (None = hoy)
            lookback_years: Años de datos históricos para entrenamiento inicial
        """
        self.start_date = start_date
        self.end_date = end_date if end_date else datetime.now().strftime('%Y-%m-%d')
        
        # Fecha de inicio para obtener datos (incluye periodo de lookback)
        lookback_start = (pd.to_datetime(start_date) - pd.DateOffset(years=lookback_years)).strftime('%Y-%m-%d')
        self.data_start_date = lookback_start
        
        # Parámetros de la estrategia
        self.momentum_windows = [5, 21, 63, 126]  # Ventanas de momentum en días
        self.regime_windows = [21, 63]  # Ventanas para detección de regímenes
        self.max_drawdown_limit = 0.15  # Límite de drawdown máximo tolerado
        self.max_exposure = 1.0  # Exposición máxima (1.0 = 100%)
        self.transaction_cost = 0.001  # Costo de transacción estimado (10bps)
        self.rebalance_frequency = 5  # Frecuencia de rebalanceo en días (5 = semanal)
        
        # Parámetros del modelo HMM
        self.n_regimes = 3  # Número de regímenes
        self.hmm_recalibration_days = 63  # Recalibración trimestral
        
        # Datos y resultados
        self.sp500_data = None
        self.sp500_symbols = None
        self.market_data = None
        self.factor_data = None
        self.regime_probs = None
        self.signals = None
        self.positions = None
        self.portfolio_values = None
        self.hmm_model = None
        
        # Inicializar
        try:
            self._load_data()
        except Exception as e:
            logging.error(f"Error al cargar datos: {str(e)}", exc_info=True)
            raise
    
    def _load_data(self):
        """Carga los datos necesarios para la estrategia"""
        # Obtener lista de símbolos del S&P 500 desde Wikipedia
        try:
            url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
            tables = pd.read_html(url)
            sp500_table = tables[0]
            self.sp500_symbols = sp500_table['Symbol'].tolist()
            
            # Guardar lista de símbolos
            sp500_table.to_csv('./artifacts/results/data/sp500_symbols.csv', index=False)
            
            print(f"Cargados {len(self.sp500_symbols)} símbolos del S&P 500")
        except Exception as e:
            logging.error(f"Error al obtener símbolos del S&P 500: {str(e)}", exc_info=True)
            # Usar un conjunto reducido de símbolos como fallback
            self.sp500_symbols = ['SPY']
        
        # Descargar datos del índice S&P 500 (SPY como proxy)
        try:
            self.sp500_data = yf.download('SPY', start=self.data_start_date, end=self.end_date)
            print(f"Datos del S&P 500 cargados: {len(self.sp500_data)} días")
            
            # Guardar datos del S&P 500
            self.sp500_data.to_csv('./artifacts/results/data/sp500_data.csv')
        except Exception as e:
            logging.error(f"Error al descargar datos del S&P 500: {str(e)}", exc_info=True)
            raise
        
        # Crear factores básicos
        self._create_factor_data()
    
    def _create_factor_data(self):
        """Crea datos de factores básicos para el modelo multifactorial"""
        try:
            # Usamos ETFs como proxies para los factores
            factor_etfs = {
                'Market': 'SPY',  # Mercado
                'Size': 'IWM',    # Tamaño (small cap)
                'Value': 'IWD',   # Valor
                'Momentum': 'MTUM', # Momentum
                'Quality': 'QUAL'  # Calidad
            }
            
            # Descargar datos de factores
            factor_data = {}
            for factor_name, ticker in factor_etfs.items():
                try:
                    data = yf.download(ticker, start=self.data_start_date, end=self.end_date)['Close']
                    factor_data[factor_name] = data
                except Exception as e:
                    logging.warning(f"Error al descargar factor {factor_name} ({ticker}): {str(e)}")
                    # Usar SPY como fallback para factores que no se pueden descargar
                    if factor_name != 'Market':  # Evitar duplicación para Market
                        factor_data[factor_name] = self.sp500_data['Close']
            
            # Convertir a DataFrame
            self.factor_data = pd.DataFrame(factor_data)
            
            # Calcular rendimientos de factores
            self.factor_returns = self.factor_data.pct_change().dropna()
            
            # Guardar datos de factores
            self.factor_returns.to_csv('./artifacts/results/data/factor_returns.csv')
            
            print(f"Datos de factores creados: {len(self.factor_returns)} días, {len(self.factor_returns.columns)} factores")
        except Exception as e:
            logging.error(f"Error al crear datos de factores: {str(e)}", exc_info=True)
            # Crear un factor básico de mercado como fallback
            self.factor_returns = pd.DataFrame({
                'Market': self.sp500_data['Close'].pct_change().dropna()
            })
    
    def _calculate_residual_returns(self, returns, factor_returns, window=252):
        """
        Calcula rendimientos residuales ajustados por factores
        
        Args:
            returns: Serie de rendimientos del activo
            factor_returns: DataFrame de rendimientos de factores
            window: Ventana para el cálculo de exposición a factores
        
        Returns:
            Serie de rendimientos residuales
        """
        # Asegurar alineación de índices
        aligned_data = pd.concat([returns, factor_returns], axis=1).dropna()
        if len(aligned_data) < window:
            window = max(30, len(aligned_data) // 2)  # Ajustar ventana si no hay suficientes datos
        
        # Inicializar serie de rendimientos residuales
        residual_returns = pd.Series(index=aligned_data.index, dtype=float)
        
        # Calcular rendimientos residuales con ventana móvil
        for i in range(window, len(aligned_data)):
            # Datos de entrenamiento
            train_data = aligned_data.iloc[i-window:i]
            y = train_data.iloc[:, 0]  # Rendimientos del activo
            X = train_data.iloc[:, 1:]  # Rendimientos de factores
            
            # Ajustar modelo de regresión
            model = Ridge(alpha=0.1)  # Ridge para estabilidad
            model.fit(X, y)
            
            # Calcular rendimiento esperado basado en factores
            current_factors = aligned_data.iloc[i, 1:].values.reshape(1, -1)
            expected_return = model.predict(current_factors)[0]
            
            # Rendimiento residual = rendimiento real - rendimiento esperado
            actual_return = aligned_data.iloc[i, 0]
            residual_returns.iloc[i] = actual_return - expected_return
        
        return residual_returns.dropna()
    
    def _calculate_adaptive_volatility(self, returns, window=21):
        """
        Calcula volatilidad adaptativa con ponderación exponencial
        
        Args:
            returns: Serie de rendimientos
            window: Ventana base para el cálculo
        
        Returns:
            Serie de volatilidad adaptativa
        """
        # Volatilidad base (desviación estándar móvil)
        rolling_std = returns.rolling(window=window).std()
        
        # Volatilidad con ponderación exponencial (más sensible a cambios recientes)
        ewma_vol = returns.ewm(span=window).std()
        
        # Combinar ambas medidas (70% EWMA, 30% rolling)
        adaptive_vol = 0.7 * ewma_vol + 0.3 * rolling_std
        
        # Asegurar valor mínimo para evitar divisiones por cero
        min_vol = returns.std() * 0.2  # 20% de la volatilidad total como mínimo
        adaptive_vol = adaptive_vol.clip(lower=min_vol)
        
        return adaptive_vol
    
    def _winsorize_signals(self, signals, n_mad=3):
        """
        Aplica winsorización basada en desviación absoluta mediana (MAD)
        
        Args:
            signals: Serie o DataFrame de señales
            n_mad: Número de MADs para el umbral
        
        Returns:
            Señales winsorizadas
        """
        if isinstance(signals, pd.DataFrame):
            return signals.apply(lambda x: self._winsorize_signals(x, n_mad))
        
        median = signals.median()
        mad = stats.median_abs_deviation(signals.dropna(), nan_policy='omit')
        
        upper_bound = median + n_mad * mad
        lower_bound = median - n_mad * mad
        
        return signals.clip(lower=lower_bound, upper=upper_bound)
    
    def _calculate_atr(self, data, window=14):
        """
        Calcula el Average True Range (ATR)
        
        Args:
            data: DataFrame con OHLC
            window: Ventana para el cálculo
        
        Returns:
            Serie con valores de ATR
        """
        high = data['High']
        low = data['Low']
        close = data['Close']
        
        # Calcular True Range
        tr1 = high - low
        tr2 = abs(high - close.shift())
        tr3 = abs(low - close.shift())
        
        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        
        # Calcular ATR
        atr = tr.rolling(window=window).mean()
        
        return atr
    
    def _calculate_regime_features(self):
        """
        Calcula características para la identificación de regímenes de mercado
        
        Returns:
            DataFrame con características para el modelo HMM
        """
        # Datos base
        market_data = self.sp500_data.copy()
        returns = market_data['Close'].pct_change().dropna()
        
        features = pd.DataFrame(index=market_data.index)
        
        # 1. Volatilidad realizada (ATR normalizado)
        atr = self._calculate_atr(market_data, window=21)
        features['volatility'] = atr / market_data['Close']
        
        # 2. Autocorrelación de rendimientos
        features['autocorr'] = returns.rolling(window=21).apply(
            lambda x: x.autocorr(lag=1) if len(x.dropna()) > 1 else 0
        )
        
        # 3. Ratio de días positivos/negativos
        features['pos_neg_ratio'] = returns.rolling(window=21).apply(
            lambda x: len(x[x > 0]) / max(1, len(x[x < 0]))
        )
        
        # 4. Tendencia (precio relativo a media móvil)
        features['trend'] = market_data['Close'] / market_data['Close'].rolling(window=63).mean() - 1
        
        # 5. Volatilidad relativa (corto vs largo plazo)
        vol_st = returns.rolling(window=21).std()
        vol_lt = returns.rolling(window=63).std()
        features['rel_vol'] = vol_st / vol_lt
        
        # Normalizar características
        scaler = StandardScaler()
        normalized_features = pd.DataFrame(
            scaler.fit_transform(features.dropna()),
            index=features.dropna().index,
            columns=features.columns
        )
        
        return normalized_features
    
    def _train_hmm_model(self, features):
        """
        Entrena un modelo HMM para identificación de regímenes
        
        Args:
            features: DataFrame con características para el modelo
        
        Returns:
            Modelo HMM entrenado
        """
        # Preparar datos
        X = features.dropna().values
        
        # Inicializar y entrenar modelo HMM
        model = hmm.GaussianHMM(
            n_components=self.n_regimes,
            covariance_type="full",
            n_iter=100,
            random_state=42
        )
        
        try:
            model.fit(X)
            return model
        except Exception as e:
            logging.error(f"Error al entrenar modelo HMM: {str(e)}", exc_info=True)
            # Fallback: modelo simple basado en K-means
            kmeans = KMeans(n_clusters=self.n_regimes, random_state=42)
            kmeans.fit(X)
            
            # Crear un pseudo-modelo HMM basado en K-means
            class KMeansHMM:
                def __init__(self, kmeans):
                    self.kmeans = kmeans
                
                def predict(self, X):
                    return self.kmeans.predict(X)
                
                def predict_proba(self, X):
                    # Distancias a centroides
                    distances = self.kmeans.transform(X)
                    # Convertir distancias a probabilidades (softmax)
                    exp_distances = np.exp(-distances)
                    return exp_distances / exp_distances.sum(axis=1, keepdims=True)
            
            return KMeansHMM(kmeans)
    
    def _identify_regimes(self, recalibrate=False):
        """
        Identifica regímenes de mercado usando HMM
        
        Args:
            recalibrate: Si es True, recalibra el modelo HMM
        
        Returns:
            DataFrame con probabilidades de regímenes
        """
        # Calcular características para regímenes
        regime_features = self._calculate_regime_features()
        
        # Entrenar o recalibrar modelo HMM si es necesario
        if self.hmm_model is None or recalibrate:
            self.hmm_model = self._train_hmm_model(regime_features)
        
        # Predecir probabilidades de regímenes
        try:
            regime_probs = self.hmm_model.predict_proba(regime_features.values)
            
            # Convertir a DataFrame
            regime_probs_df = pd.DataFrame(
                regime_probs,
                index=regime_features.index,
                columns=[f'Regime_{i+1}' for i in range(self.n_regimes)]
            )
            
            return regime_probs_df
        except Exception as e:
            logging.error(f"Error al predecir regímenes: {str(e)}", exc_info=True)
            # Fallback: asignar probabilidad uniforme a todos los regímenes
            uniform_probs = np.ones((len(regime_features), self.n_regimes)) / self.n_regimes
            return pd.DataFrame(
                uniform_probs,
                index=regime_features.index,
                columns=[f'Regime_{i+1}' for i in range(self.n_regimes)]
            )
    
    def _calculate_momentum_signals(self):
        """
        Calcula señales de momentum para diferentes horizontes temporales
        
        Returns:
            DataFrame con señales de momentum normalizadas
        """
        signals = pd.DataFrame(index=self.sp500_data.index)
        
        # Calcular rendimientos para diferentes horizontes
        for window in self.momentum_windows:
            # Rendimiento simple
            returns = self.sp500_data['Close'].pct_change(window).shift(1)
            
            # Calcular rendimiento residual (ajustado por factores)
            if window >= 21:  # Solo para horizontes medianos/largos
                try:
                    daily_returns = self.sp500_data['Close'].pct_change().dropna()
                    residual_returns = self._calculate_residual_returns(
                        daily_returns, 
                        self.factor_returns
                    )
                    # Acumular rendimientos residuales
                    cum_residual = (1 + residual_returns).rolling(window=window).apply(
                        lambda x: np.prod(x) - 1
                    ).shift(1)
                    
                    # Combinar señal residual con rendimiento simple (70% residual, 30% simple)
                    aligned_data = pd.concat([cum_residual, returns], axis=1).dropna()
                    combined_signal = 0.7 * aligned_data.iloc[:, 0] + 0.3 * aligned_data.iloc[:, 1]
                    signals[f'mom_{window}'] = combined_signal
                except Exception as e:
                    logging.warning(f"Error al calcular rendimientos residuales para ventana {window}: {str(e)}")
                    signals[f'mom_{window}'] = returns
            else:
                signals[f'mom_{window}'] = returns
        
        # Normalizar señales por volatilidad
        for col in signals.columns:
            window = int(col.split('_')[1])
            vol = self._calculate_adaptive_volatility(signals[col], window=min(window, 21))
            signals[col] = signals[col] / vol
        
        # Invertir señales para horizontes cortos (reversión a la media)
        for col in signals.columns:
            window = int(col.split('_')[1])
            if window <= 5:
                signals[col] = -signals[col]
        
        # Aplicar winsorización
        signals = self._winsorize_signals(signals)
        
        return signals
    
    def _combine_signals(self, signals, regime_probs):
        """
        Combina señales de momentum según regímenes de mercado
        
        Args:
            signals: DataFrame con señales de momentum
            regime_probs: DataFrame con probabilidades de regímenes
        
        Returns:
            Serie con señal combinada
        """
        # Definir pesos de señales por régimen
        # Régimen 1: Tendencia - Preferir momentum de medio/largo plazo
        # Régimen 2: Rango - Preferir reversión de corto plazo
        # Régimen 3: Crisis - Reducir exposición general
        
        regime_weights = {
            'Regime_1': {  # Tendencia
                'mom_5': 0.1,
                'mom_21': 0.3,
                'mom_63': 0.4,
                'mom_126': 0.2
            },
            'Regime_2': {  # Rango
                'mom_5': 0.5,
                'mom_21': 0.3,
                'mom_63': 0.1,
                'mom_126': 0.1
            },
            'Regime_3': {  # Crisis
                'mom_5': 0.2,
                'mom_21': 0.2,
                'mom_63': 0.3,
                'mom_126': 0.3
            }
        }
        
        # Inicializar señal combinada
        combined_signal = pd.Series(0, index=signals.index)
        
        # Combinar señales según pesos de regímenes
        for regime, weights in regime_weights.items():
            for signal_name, weight in weights.items():
                if signal_name in signals.columns and regime in regime_probs.columns:
                    # Multiplicar señal por peso y probabilidad de régimen
                    weighted_signal = signals[signal_name] * weight * regime_probs[regime]
                    combined_signal = combined_signal.add(weighted_signal, fill_value=0)
        
        return combined_signal
    
    def _apply_risk_management(self, combined_signal, portfolio_state=None):
        """
        Aplica gestión dinámica de riesgo a la señal combinada
        
        Args:
            combined_signal: Serie con señal combinada
            portfolio_state: Estado actual del portafolio (opcional)
        
        Returns:
            Serie con posiciones ajustadas por riesgo
        """
        # Inicializar posiciones con señal combinada
        positions = combined_signal.copy()
        
        # Calcular drawdown si hay estado de portafolio
        drawdown_factor = 1.0
        if portfolio_state is not None and 'equity_curve' in portfolio_state:
            equity = portfolio_state['equity_curve']
            rolling_max = equity.cummax()
            drawdown = (equity - rolling_max) / rolling_max
            current_drawdown = drawdown.iloc[-1]
            
            # Ajustar exposición según drawdown
            drawdown_factor = 1 - (current_drawdown / self.max_drawdown_limit)**2
            drawdown_factor = max(0, min(1, drawdown_factor))
        
        # Filtro de volatilidad
        vol_factor = 1.0
        returns = self.sp500_data['Close'].pct_change()
        vol_st = returns.rolling(window=21).std()
        vol_lt = returns.rolling(window=63).std()
        vol_ratio = vol_st / vol_lt
        
        # Reducir exposición cuando volatilidad a corto plazo > volatilidad a largo plazo
        vol_factor = pd.Series(1.0, index=positions.index)
        vol_factor[vol_ratio > 1.2] = 1.2 / vol_ratio[vol_ratio > 1.2]
        vol_factor = vol_factor.clip(0.3, 1.0)  # Limitar reducción de exposición
        
        # Aplicar factores de riesgo
        risk_adjusted_positions = positions * drawdown_factor * vol_factor
        
        # Limitar exposición máxima
        risk_adjusted_positions = risk_adjusted_positions.clip(-self.max_exposure, self.max_exposure)
        
        # Aplicar filtro de tendencia de largo plazo
        price = self.sp500_data['Close']
        sma200 = price.rolling(window=200).mean()
        trend_filter = (price > sma200).astype(int) * 2 - 1  # 1 si alcista, -1 si bajista
        
        # Reducir exposición en contra de la tendencia de largo plazo
        for i in range(len(risk_adjusted_positions)):
            if not np.isnan(trend_filter.iloc[i]) and not np.isnan(risk_adjusted_positions.iloc[i]):
                # Si posición y tendencia tienen signos opuestos, reducir exposición
                if risk_adjusted_positions.iloc[i] * trend_filter.iloc[i] < 0:
                    risk_adjusted_positions.iloc[i] *= 0.5
        
        return risk_adjusted_positions
    
    def _apply_execution_logic(self, target_positions):
        """
        Aplica lógica de ejecución con bandas de no-transacción
        
        Args:
            target_positions: Serie con posiciones objetivo
        
        Returns:
            Serie con posiciones ejecutadas
        """
        # Inicializar posiciones ejecutadas
        if not hasattr(self, 'current_positions'):
            self.current_positions = pd.Series(0, index=target_positions.index)
        
        executed_positions = self.current_positions.copy()
        
        # Calcular umbral mínimo adaptativo para transacciones
        returns = self.sp500_data['Close'].pct_change()
        vol = returns.rolling(window=21).std()
        min_threshold = np.maximum(0.005, self.transaction_cost * 3)  # Mínimo 0.5% o 3x costos
        adaptive_threshold = min_threshold * (1 + vol / vol.mean())
        
        # Aplicar lógica de ejecución
        for date in target_positions.index:
            if date in executed_positions.index:
                current_pos = executed_positions.loc[date]
                target_pos = target_positions.loc[date]
                threshold = adaptive_threshold.loc[date] if date in adaptive_threshold.index else min_threshold
                
                # Ejecutar solo si el cambio supera el umbral
                if abs(target_pos - current_pos) > threshold:
                    executed_positions.loc[date] = target_pos
                else:
                    executed_positions.loc[date] = current_pos
        
        # Actualizar posiciones actuales
        self.current_positions = executed_positions
        
        return executed_positions
    
    def _calculate_portfolio_returns(self, positions):
        """
        Calcula rendimientos del portafolio basados en posiciones
        
        Args:
            positions: Serie con posiciones
        
        Returns:
            Serie con rendimientos del portafolio
        """
        # Obtener rendimientos del mercado
        market_returns = self.sp500_data['Close'].pct_change()
        
        # Alinear datos
        aligned_data = pd.concat([positions.shift(1), market_returns], axis=1).dropna()
        
        # Calcular rendimientos del portafolio (posición * rendimiento del mercado)
        portfolio_returns = aligned_data.iloc[:, 0] * aligned_data.iloc[:, 1]
        
        # Aplicar costos de transacción
        position_changes = positions.diff().abs()
        transaction_costs = position_changes * self.transaction_cost
        
        # Rendimientos netos
        net_returns = portfolio_returns - transaction_costs
        
        return net_returns
    
    def _calculate_performance_metrics(self, returns):
        """
        Calcula métricas de rendimiento del portafolio
        
        Args:
            returns: Serie con rendimientos del portafolio
        
        Returns:
            Diccionario con métricas de rendimiento
        """
        # Rendimiento acumulado
        cum_returns = (1 + returns).cumprod()
        total_return = cum_returns.iloc[-1] - 1
        
        # Volatilidad anualizada
        annual_vol = returns.std() * np.sqrt(252)
        
        # Sharpe ratio
        risk_free_rate = 0.02 / 252  # Tasa libre de riesgo diaria (2% anual)
        sharpe_ratio = (returns.mean() - risk_free_rate) / returns.std() * np.sqrt(252)
        
        # Drawdown
        rolling_max = cum_returns.cummax()
        drawdown = (cum_returns - rolling_max) / rolling_max
        max_drawdown = drawdown.min()
        
        # Ratio de captación (upside/downside capture)
        market_returns = self.sp500_data['Close'].pct_change().loc[returns.index]
        up_market = market_returns > 0
        down_market = market_returns < 0
        
        if up_market.sum() > 0:
            upside_capture = (returns[up_market].mean() / market_returns[up_market].mean())
        else:
            upside_capture = np.nan
            
        if down_market.sum() > 0:
            downside_capture = (returns[down_market].mean() / market_returns[down_market].mean())
        else:
            downside_capture = np.nan
        
        # Métricas adicionales
        win_rate = (returns > 0).mean()
        profit_loss_ratio = abs(returns[returns > 0].mean() / returns[returns < 0].mean()) if (returns < 0).sum() > 0 else np.inf
        
        return {
            'total_return': total_return,
            'annual_return': (1 + total_return) ** (252 / len(returns)) - 1,
            'annual_volatility': annual_vol,
            'sharpe_ratio': sharpe_ratio,
            'max_drawdown': max_drawdown,
            'win_rate': win_rate,
            'profit_loss_ratio': profit_loss_ratio,
            'upside_capture': upside_capture,
            'downside_capture': downside_capture,
            'cum_returns': cum_returns
        }
    
    def backtest(self):
        """
        Ejecuta backtest de la estrategia
        
        Returns:
            Diccionario con resultados del backtest
        """
        try:
            # Filtrar datos para el período de backtest
            backtest_data = self.sp500_data[self.start_date:self.end_date].copy()
            
            # Inicializar resultados
            results = {
                'signals': pd.DataFrame(index=backtest_data.index),
                'positions': pd.Series(index=backtest_data.index),
                'returns': pd.Series(index=backtest_data.index),
                'equity_curve': pd.Series(index=backtest_data.index),
                'regime_probs': pd.DataFrame(index=backtest_data.index)
            }
            
            # Inicializar portafolio
            portfolio_state = {
                'equity_curve': pd.Series(1.0, index=backtest_data.index)
            }
            
            # Dividir en ventanas para walk-forward testing
            test_windows = []
            window_size = 126  # ~6 meses
            for i in range(0, len(backtest_data), window_size):
                start_idx = i
                end_idx = min(i + window_size, len(backtest_data))
                test_windows.append((start_idx, end_idx))
            
            # Ejecutar backtest por ventanas
            for start_idx, end_idx in tqdm(test_windows, desc="Ejecutando backtest"):
                window_dates = backtest_data.index[start_idx:end_idx]
                
                # Identificar regímenes (recalibrar al inicio de cada ventana)
                recalibrate = start_idx == 0 or (start_idx // window_size) % 2 == 0  # Recalibrar cada 2 ventanas
                regime_probs = self._identify_regimes(recalibrate=recalibrate)
                
                # Calcular señales de momentum
                momentum_signals = self._calculate_momentum_signals()
                
                # Combinar señales según regímenes
                combined_signal = self._combine_signals(momentum_signals, regime_probs)
                
                # Aplicar gestión de riesgo
                risk_adjusted_positions = self._apply_risk_management(combined_signal, portfolio_state)
                
                # Aplicar lógica de ejecución
                executed_positions = self._apply_execution_logic(risk_adjusted_positions)
                
                # Guardar resultados para esta ventana
                for date in window_dates:
                    if date in regime_probs.index:
                        results['regime_probs'].loc[date] = regime_probs.loc[date]
                    
                    if date in momentum_signals.index:
                        for col in momentum_signals.columns:
                            results['signals'].loc[date, col] = momentum_signals.loc[date, col]
                    
                    if date in combined_signal.index:
                        results['signals'].loc[date, 'combined'] = combined_signal.loc[date]
                    
                    if date in risk_adjusted_positions.index:
                        results['signals'].loc[date, 'risk_adjusted'] = risk_adjusted_positions.loc[date]
                    
                    if date in executed_positions.index:
                        results['positions'].loc[date] = executed_positions.loc[date]
            
            # Calcular rendimientos del portafolio
            portfolio_returns = self._calculate_portfolio_returns(results['positions'])
            results['returns'] = portfolio_returns
            
            # Calcular curva de equity
            results['equity_curve'] = (1 + portfolio_returns).cumprod()
            
            # Calcular métricas de rendimiento
            results['metrics'] = self._calculate_performance_metrics(portfolio_returns)
            
            # Guardar resultados
            self._save_results(results)
            
            return results
        
        except Exception as e:
            logging.error(f"Error en backtest: {str(e)}", exc_info=True)
            raise
    
    def _save_results(self, results):
        """
        Guarda resultados del backtest
        
        Args:
            results: Diccionario con resultados del backtest
        """
        try:
            # Guardar señales
            results['signals'].to_csv('./artifacts/results/data/signals.csv')
            
            # Guardar posiciones
            results['positions'].to_csv('./artifacts/results/data/positions.csv')
            
            # Guardar rendimientos
            results['returns'].to_csv('./artifacts/results/data/returns.csv')
            
            # Guardar curva de equity
            results['equity_curve'].to_csv('./artifacts/results/data/equity_curve.csv')
            
            # Guardar probabilidades de regímenes
            results['regime_probs'].to_csv('./artifacts/results/data/regime_probs.csv')
            
            # Guardar métricas
            metrics_df = pd.DataFrame({k: [v] for k, v in results['metrics'].items() 
                                      if k != 'cum_returns'})
            metrics_df.to_csv('./artifacts/results/data/performance_metrics.csv', index=False)
            
            # Generar gráficos
            self._generate_plots(results)
            
        except Exception as e:
            logging.error(f"Error al guardar resultados: {str(e)}", exc_info=True)
    
    def _generate_plots(self, results):
        """
        Genera gráficos de resultados
        
        Args:
            results: Diccionario con resultados del backtest
        """
        try:
            # Configuración de estilo
            plt.style.use('seaborn-v0_8-darkgrid')
            
            # 1. Curva de equity
            plt.figure(figsize=(12, 6))
            results['equity_curve'].plot(label='Estrategia')
            
            # Comparar con S&P 500
            sp500_returns = self.sp500_data['Close'].pct_change().loc[results['returns'].index]
            sp500_equity = (1 + sp500_returns).cumprod()
            sp500_equity.plot(label='S&P 500')
            
            plt.title('Curva de Equity')
            plt.xlabel('Fecha')
            plt.ylabel('Valor (normalizado)')
            plt.legend()
            plt.grid(True)
            plt.savefig('./artifacts/results/figures/equity_curve.png')
            plt.close()
            
            # 2. Drawdown
            plt.figure(figsize=(12, 6))
            equity = results['equity_curve']
            rolling_max = equity.cummax()
            drawdown = (equity - rolling_max) / rolling_max
            drawdown.plot(label='Drawdown')
            
            plt.title('Drawdown')
            plt.xlabel('Fecha')
            plt.ylabel('Drawdown')
            plt.legend()
            plt.grid(True)
            plt.savefig('./artifacts/results/figures/drawdown.png')
            plt.close()
            
            # 3. Posiciones a lo largo del tiempo
            plt.figure(figsize=(12, 6))
            results['positions'].plot(label='Posición')
            
            plt.title('Posiciones a lo largo del tiempo')
            plt.xlabel('Fecha')
            plt.ylabel('Posición')
            plt.legend()
            plt.grid(True)
            plt.savefig('./artifacts/results/figures/positions.png')
            plt.close()
            
            # 4. Regímenes de mercado
            plt.figure(figsize=(12, 6))
            regime_probs = results['regime_probs']
            
            for col in regime_probs.columns:
                regime_probs[col].plot(label=col)
            
            plt.title('Probabilidades de Regímenes de Mercado')
            plt.xlabel('Fecha')
            plt.ylabel('Probabilidad')
            plt.legend()
            plt.grid(True)
            plt.savefig('./artifacts/results/figures/regimes.png')
            plt.close()
            
            # 5. Señales de momentum
            plt.figure(figsize=(12, 6))
            signals = results['signals']
            
            for col in signals.columns:
                if col.startswith('mom_'):
                    signals[col].plot(label=col, alpha=0.7)
            
            plt.title('Señales de Momentum')
            plt.xlabel('Fecha')
            plt.ylabel('Señal')
            plt.legend()
            plt.grid(True)
            plt.savefig('./artifacts/results/figures/momentum_signals.png')
            plt.close()
            
            # 6. Señal combinada vs. posición
            plt.figure(figsize=(12, 6))
            if 'combined' in signals.columns:
                signals['combined'].plot(label='Señal Combinada', alpha=0.7)
            if 'risk_adjusted' in signals.columns:
                signals['risk_adjusted'].plot(label='Señal Ajustada por Riesgo', alpha=0.7)
            results['positions'].plot(label='Posición Final', linewidth=2)
            
            plt.title('Señal Combinada vs. Posición Final')
            plt.xlabel('Fecha')
            plt.ylabel('Valor')
            plt.legend()
            plt.grid(True)
            plt.savefig('./artifacts/results/figures/combined_signal_vs_position.png')
            plt.close()
            
            # 7. Rendimientos mensuales
            plt.figure(figsize=(12, 8))
            monthly_returns = results['returns'].resample('M').apply(
                lambda x: (1 + x).prod() - 1
            )
            
            monthly_returns_heatmap = monthly_returns.groupby(
                [monthly_returns.index.year, monthly_returns.index.month]
            ).first().unstack()
            
            sns.heatmap(
                monthly_returns_heatmap, 
                annot=True, 
                fmt=".1%", 
                cmap="RdYlGn",
                center=0,
                linewidths=1
            )
            
            plt.title('Rendimientos Mensuales')
            plt.xlabel('Mes')
            plt.ylabel('Año')
            plt.savefig('./artifacts/results/figures/monthly_returns.png')
            plt.close()
            
        except Exception as e:
            logging.error(f"Error al generar gráficos: {str(e)}", exc_info=True)
    
    def walk_forward_optimization(self, param_grid, n_splits=5):
        """
        Realiza optimización walk-forward de parámetros
        
        Args:
            param_grid: Diccionario con parámetros a optimizar
            n_splits: Número de divisiones para validación cruzada
        
        Returns:
            Diccionario con mejores parámetros y resultados
        """
        try:
            # Filtrar datos para el período de backtest
            backtest_data = self.sp500_data[self.start_date:self.end_date].copy()
            
            # Crear divisiones temporales
            tscv = TimeSeriesSplit(n_splits=n_splits)
            splits = list(tscv.split(backtest_data))
            
            # Inicializar resultados
            optimization_results = []
            
            # Generar combinaciones de parámetros
            param_combinations = self._generate_param_combinations(param_grid)
            
            # Ejecutar optimización
            for params in tqdm(param_combinations, desc="Optimizando parámetros"):
                fold_results = []
                
                for train_idx, test_idx in splits:
                    # Dividir datos
                    train_data = backtest_data.iloc[train_idx]
                    test_data = backtest_data.iloc[test_idx]
                    
                    # Aplicar parámetros
                    for param, value in params.items():
                        setattr(self, param, value)
                    
                    # Ejecutar backtest en datos de prueba
                    test_start = test_data.index[0].strftime('%Y-%m-%d')
                    test_end = test_data.index[-1].strftime('%Y-%m-%d')
                    
                    # Guardar configuración actual
                    original_start = self.start_date
                    original_end = self.end_date
                    
                    # Configurar para este fold
                    self.start_date = test_start
                    self.end_date = test_end
                    
                    # Ejecutar backtest
                    results = self.backtest()
                    
                    # Restaurar configuración
                    self.start_date = original_start
                    self.end_date = original_end
                    
                    # Guardar métricas
                    fold_results.append({
                        'sharpe_ratio': results['metrics']['sharpe_ratio'],
                        'max_drawdown': results['metrics']['max_drawdown'],
                        'total_return': results['metrics']['total_return']
                    })
                
                # Calcular métricas promedio
                avg_metrics = {
                    'sharpe_ratio': np.mean([r['sharpe_ratio'] for r in fold_results]),
                    'max_drawdown': np.mean([r['max_drawdown'] for r in fold_results]),
                    'total_return': np.mean([r['total_return'] for r in fold_results])
                }
                
                # Guardar resultados
                optimization_results.append({
                    'params': params,
                    'metrics': avg_metrics,
                    'fold_results': fold_results
                })
            
            # Ordenar por Sharpe ratio
            optimization_results.sort(key=lambda x: x['metrics']['sharpe_ratio'], reverse=True)
            
            # Guardar resultados de optimización
            self._save_optimization_results(optimization_results)
            
            # Aplicar mejores parámetros
            best_params = optimization_results[0]['params']
            for param, value in best_params.items():
                setattr(self, param, value)
            
            return {
                'best_params': best_params,
                'best_metrics': optimization_results[0]['metrics'],
                'all_results': optimization_results
            }
            
        except Exception as e:
            logging.error(f"Error en optimización walk-forward: {str(e)}", exc_info=True)
            raise
    
    def _generate_param_combinations(self, param_grid):
        """
        Genera todas las combinaciones de parámetros
        
        Args:
            param_grid: Diccionario con parámetros a optimizar
        
        Returns:
            Lista de diccionarios con combinaciones de parámetros
        """
        # Función recursiva para generar combinaciones
        def generate_combinations(param_grid, current_idx=0, current_params={}):
            if current_idx == len(param_names):
                return [current_params.copy()]
            
            param_name = param_names[current_idx]
            param_values = param_grid[param_name]
            
            combinations = []
            for value in param_values:
                current_params[param_name] = value
                combinations.extend(generate_combinations(param_grid, current_idx + 1, current_params))
            
            return combinations
        
        param_names = list(param_grid.keys())
        return generate_combinations(param_grid, 0, {})
    
    def _save_optimization_results(self, optimization_results):
        """
        Guarda resultados de optimización
        
        Args:
            optimization_results: Lista con resultados de optimización
        """
        try:
            # Crear DataFrame con resultados
            results_data = []
            
            for result in optimization_results:
                row = {}
                
                # Añadir parámetros
                for param, value in result['params'].items():
                    row[param] = value
                
                # Añadir métricas
                for metric, value in result['metrics'].items():
                    row[metric] = value
                
                results_data.append(row)
            
            # Convertir a DataFrame
            results_df = pd.DataFrame(results_data)
            
            # Guardar resultados
            results_df.to_csv('./artifacts/results/data/optimization_results.csv', index=False)
            
            # Generar gráfico de parámetros vs. métricas
            self._plot_optimization_results(results_df)
            
        except Exception as e:
            logging.error(f"Error al guardar resultados de optimización: {str(e)}", exc_info=True)
    
    def _plot_optimization_results(self, results_df):
        """
        Genera gráficos de resultados de optimización
        
        Args:
            results_df: DataFrame con resultados de optimización
        """
        try:
            # Configuración de estilo
            plt.style.use('seaborn-v0_8-darkgrid')
            
            # Identificar parámetros y métricas
            param_cols = [col for col in results_df.columns if col not in ['sharpe_ratio', 'max_drawdown', 'total_return']]
            metric_cols = ['sharpe_ratio', 'max_drawdown', 'total_return']
            
            # Generar gráficos para cada parámetro vs. métricas
            for param in param_cols:
                if len(results_df[param].unique()) > 1:  # Solo si hay múltiples valores
                    plt.figure(figsize=(12, 8))
                    
                    for i, metric in enumerate(metric_cols):
                        plt.subplot(len(metric_cols), 1, i+1)
                        
                        # Agrupar por parámetro y calcular media
                        grouped = results_df.groupby(param)[metric].mean()
                        grouped.plot(marker='o')
                        
                        plt.title(f'{metric} vs. {param}')
                        plt.xlabel(param)
                        plt.ylabel(metric)
                        plt.grid(True)
                    
                    plt.tight_layout()
                    plt.savefig(f'./artifacts/results/figures/optimization_{param}.png')
                    plt.close()
            
            # Matriz de correlación entre parámetros y métricas
            plt.figure(figsize=(10, 8))
            corr_matrix = results_df.corr()
            sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)
            
            plt.title('Correlación entre Parámetros y Métricas')
            plt.tight_layout()
            plt.savefig('./artifacts/results/figures/optimization_correlation.png')
            plt.close()
            
        except Exception as e:
            logging.error(f"Error al generar gráficos de optimización: {str(e)}", exc_info=True)

# Ejecutar estrategia
if __name__ == "__main__":
    try:
        # Inicializar estrategia
        strategy = AdaptiveMomentumStrategy(start_date='2015-01-01')
        
        # Ejecutar backtest
        results = strategy.backtest()
        
        # Mostrar métricas principales
        print("\nMétricas de rendimiento:")
        print(f"Rendimiento total: {results['metrics']['total_return']:.2%}")
        print(f"Rendimiento anualizado: {results['metrics']['annual_return']:.2%}")
        print(f"Volatilidad anualizada: {results['metrics']['annual_volatility']:.2%}")
        print(f"Sharpe ratio: {results['metrics']['sharpe_ratio']:.2f}")
        print(f"Máximo drawdown: {results['metrics']['max_drawdown']:.2%}")
        print(f"Win rate: {results['metrics']['win_rate']:.2%}")
        print(f"Ratio profit/loss: {results['metrics']['profit_loss_ratio']:.2f}")
        
        # Optimización de parámetros (opcional)
        """
        param_grid = {
            'max_exposure': [0.8, 1.0, 1.2],
            'max_drawdown_limit': [0.1, 0.15, 0.2],
            'n_regimes': [2, 3, 4]
        }
        
        optimization_results = strategy.walk_forward_optimization(param_grid, n_splits=3)
        
        print("\nMejores parámetros:")
        for param, value in optimization_results['best_params'].items():
            print(f"{param}: {value}")
        
        print("\nMétricas con mejores parámetros:")
        for metric, value in optimization_results['best_metrics'].items():
            if metric == 'sharpe_ratio':
                print(f"Sharpe ratio: {value:.2f}")
            else:
                print(f"{metric}: {value:.2%}")
        """
        
    except Exception as e:
        logging.error(f"Error en la ejecución principal: {str(e)}", exc_info=True)
        print(f"Error: {str(e)}")
```


--- revisor_codigo (STEP 9.1) ---
INPUT 1 (user):
=== REVISIÓN DE CÓDIGO ===

IDEA CONCEPTUAL (sintetizada):
# IDEA FINAL: ESTRATEGIA DE MOMENTUM ADAPTATIVO CON REGÍMENES DE MERCADO Y CONTROL DINÁMICO DE RIESGO

## Descripción

Esta estrategia combina momentum residual multi-horizonte con identificación de regímenes de mercado y gestión dinámica del riesgo para capturar alpha en el S&P 500 mientras minimiza drawdowns. El enfoque integra análisis cuantitativo avanzado con controles de riesgo adaptativos para crear un sistema de trading robusto que se ajusta a diferentes condiciones de mercado.

## Características principales

### 1. Sistema de Momentum Residual Multi-Horizonte

- **Cálculo de rendimientos residuales**:
  - Análisis de múltiples horizontes temporales (5, 21, 63, 126 días)
  - Ajuste por modelo multifactorial: Rendimiento_Residual = Rendimiento_Real - (Exposición_Factores * Rendimiento_Factores)
  - Factores incluidos: Mercado, Tamaño, Valor, Momentum, Calidad
  - Señales contrarias para horizontes cortos (1-5 días) para capturar reversión a la media
  - Señales de momentum direccional para horizontes medianos/largos (>21 días)

- **Normalización adaptativa**:
  - Normalización por volatilidad específica usando desviación estándar móvil adaptativa
  - Winsorización dinámica basada en desviaciones MAD para mitigar outliers:
    - Umbral_Superior = mediana + 3 * MAD
    - Umbral_Inferior = mediana - 3 * MAD

- **Filtro de tendencia de largo plazo**:
  - Comparación del precio actual con media móvil de 200 días
  - Ajuste de exposición al momentum de corto plazo según dirección de tendencia de largo plazo

### 2. Identificación de Regímenes de Mercado (Modelo HMM Suavizado)

- **Modelo de Markov oculto con 3 estados**:
  - Régimen 1: Mercado en tendencia (baja volatilidad, autocorrelación positiva)
  - Régimen 2: Mercado en rango (baja volatilidad, autocorrelación negativa)
  - Régimen 3: Mercado en crisis (alta volatilidad, correlaciones inestables)

- **Variables de observación para inferencia de régimen**:
  - Volatilidad realizada (ATR de 21 días / precio)
  - Autocorrelación de rendimientos (ventana de 21 días)
  - Ratio de días positivos/negativos (ventana de 21 días)
  - Divergencia de volatilidad implícita vs. realizada (cuando disponible)
  - Variables macroeconómicas auxiliares:
    - Pendiente de la curva de rendimiento (10Y-2Y)
    - Spreads de crédito (BBB-Treasury)

- **Enfoque de régimen suavizado**:
  - Utilización de probabilidades de pertenencia a cada régimen en lugar de clasificación discreta
  - Señal_Final = Σ(Prob_Régimen_i * Señal_Régimen_i)
  - Recalibración trimestral del modelo HMM con validación cruzada para selección óptima de estados

### 3. Sistema de Ensemble con Stacking

- **Generación de múltiples modelos**:
  - 10-15 variantes de la estrategia con diferentes técnicas y parámetros
  - Entrenamiento en diferentes subconjuntos de datos históricos (block bootstrap)
  - Diversificación de métodos: modelos basados en momentum, reversión, volatilidad, etc.

- **Implementación de stacking**:
  - Meta-modelo (regresión regularizada) para combinar predicciones de modelos base
  - Captura de interacciones entre modelos para mejorar precisión
  - Pruning dinámico: eliminación temporal de modelos con rendimiento persistentemente pobre

- **Adaptación por validación cruzada temporal**:
  - Walk-forward validation con ventanas de entrenamiento de 2 años y prueba de 6 meses
  - Actualización trimestral de pesos del ensemble basada en rendimiento reciente

### 4. Gestión Dinámica del Riesgo

- **Control de drawdown adaptativo**:
  - Exposición = Exposición_Base * (1 - (Drawdown_Actual/Drawdown_Máximo_Tolerado)^2)
  - Drawdown_Máximo_Tolerado = 15% (calibrable)
  - Recuperación gradual post-reducción por drawdown

- **VaR condicional (CVaR/Expected Shortfall)**:
  - Limitación de exposición basada en CVaR estimado bajo diferentes escenarios
  - Protección mejorada contra eventos extremos

- **Filtro de tendencia de volatilidad**:
  - Ratio de volatilidad actual (21 días) vs. volatilidad de largo plazo (63 días)
  - Reducción de exposición cuando volatilidad_actual/volatilidad_tendencia > 1.2
  - Reducción adicional cuando la derivada de la volatilidad es positiva

- **Stops dinámicos contextuales**:
  - Stop-loss basado en ATR * multiplicador específico por régimen:
    - Régimen tendencia: 3.0 * ATR
    - Régimen rango: 2.0 * ATR
    - Régimen crisis: 1.5 * ATR
  - Stops basados en tiempo: reducción de posiciones sin comportamiento esperado después de n días

### 5. Optimización de Ejecución

- **Banda de no-transacción adaptativa**:
  - Umbral mínimo = máx(0.5%, costos_transacción * 3)
  - Ejecución solo cuando el cambio en posición deseada > umbral mínimo
  - Ajuste dinámico según volatilidad reciente

- **Timing de ejecución optimizado**:
  - Rebalanceo semanal (viernes cierre) como base
  - Análisis de patrones estacionales intraweek para día óptimo de ejecución
  - Monitoreo continuo para stops y condiciones extremas

- **Simulación de órdenes realista**:
  - Modelado de slippage basado en volatilidad y volumen
  - Incorporación de costos de financiamiento para posiciones apalancadas

## Implementación

### 1. Cálculo de Señales

```python
# Pseudocódigo para cálculo de señales
def calcular_senales(precios, factores, parametros):
    # Calcular rendimientos para diferentes horizontes
    rendimientos = {}
    for horizonte in [5, 21, 63, 126]:
        rendimientos[horizonte] = calcular_rendimiento(precios, horizonte)
    
    # Calcular exposición a factores y rendimientos residuales
    rendimientos_residuales = {}
    for horizonte, rend in rendimientos.items():
        exposicion = calcular_exposicion_factores(rend, factores, horizonte)
        rendimientos_residuales[horizonte] = rend - np.dot(exposicion, factores)
    
    # Normalizar señales
    senales_normalizadas = {}
    for horizonte, rend_res in rendimientos_residuales.items():
        vol = calcular_volatilidad_adaptativa(rend_res, horizonte)
        senales_normalizadas[horizonte] = rend_res / vol
        
        # Aplicar winsorización
        mad = calcular_mad(senales_normalizadas[horizonte])
        mediana = np.median(senales_normalizadas[horizonte])
        umbral_sup = mediana + 3 * mad
        umbral_inf = mediana - 3 * mad
        senales_normalizadas[horizonte] = np.clip(senales_normalizadas[horizonte], umbral_inf, umbral_sup)
    
    # Invertir señal para horizontes cortos (reversión a la media)
    if horizonte <= 5:
        senales_normalizadas[horizonte] = -senales_normalizadas[horizonte]
    
    return senales_normalizadas
```

### 2. Identificación de Regímenes

```python
# Pseudocódigo para identificación de regímenes
def identificar_regimenes(precios, datos_macro, parametros):
    # Calcular features para el HMM
    features = []
    
    # Volatilidad realizada
    atr = calcular_atr(precios, 21)
    features.append(atr / precios)
    
    # Autocorrelación
    autocorr = calcular_autocorrelacion(precios, 21)
    features.append(autocorr)
    
    # Ratio días positivos/negativos
    ratio_dias = calcular_ratio_dias_pos_neg(precios, 21)
    features.append(ratio_dias)
    
    # Variables macro
    pendiente_curva = datos_macro['10Y'] - datos_macro['2Y']
    features.append(pendiente_curva)
    spread_credito = datos_macro['BBB'] - datos_macro['Treasury']
    features.append(spread_credito)
    
    # Entrenar/actualizar modelo HMM
    if necesita_recalibracion():
        modelo_hmm = entrenar_hmm(features_historicos, n_estados=3)
    
    # Obtener probabilidades de régimen
    prob_regimenes = modelo_hmm.predict_proba(features)
    
    return prob_regimenes
```

### 3. Combinación de Señales y Gestión de Riesgo

```python
# Pseudocódigo para combinación de señales y gestión de riesgo
def combinar_senales(senales, prob_regimenes, estado_portfolio, parametros):
    # Pesos base por horizonte según régimen
    pesos_horizonte = {}
    for horizonte in senales.keys():
        pesos_horizonte[horizonte] = 0
        for i, prob in enumerate(prob_regimenes):
            if i == 0:  # Régimen tendencia
                if horizonte >= 21:
                    pesos_horizonte[horizonte] += prob * 0.4  # Mayor peso a medio/largo plazo
                else:
                    pesos_horizonte[horizonte] += prob * 0.1
            elif i == 1:  # Régimen rango
                if horizonte <= 5:
                    pesos_horizonte[horizonte] += prob * 0.5  # Mayor peso a corto plazo (reversión)
                else:
                    pesos_horizonte[horizonte] += prob * 0.1
            else:  # Régimen crisis
                pesos_horizonte[horizonte] += prob * 0.2  # Reducción general
    
    # Combinar señales ponderadas
    senal_combinada = 0
    for horizonte, senal in senales.items():
        senal_combinada += senal * pesos_horizonte[horizonte]
    
    # Aplicar control de drawdown
    drawdown_actual = calcular_drawdown(estado_portfolio)
    drawdown_max = parametros['drawdown_maximo']
    factor_drawdown = 1 - (drawdown_actual/drawdown_max)**2
    factor_drawdown = max(0, min(1, factor_drawdown))
    
    # Aplicar filtro de volatilidad
    vol_actual = calcular_volatilidad(precios, 21)
    vol_tendencia = calcular_volatilidad(precios, 63)
    ratio_vol = vol_actual / vol_tendencia
    factor_vol = 1.0
    if ratio_vol > 1.2:
        factor_vol = 1.2 / ratio_vol
    
    # Calcular exposición final
    exposicion_base = parametros['exposicion_maxima']
    exposicion = exposicion_base * factor_drawdown * factor_vol
    
    # Transformar señal en posición usando función sigmoide
    k = parametros['sensibilidad_sigmoide']
    posicion = exposicion * (2 / (1 + np.exp(-k * senal_combinada)) - 1)
    
    return posicion
```

### 4. Backtesting y Validación

```python
# Pseudocódigo para backtesting y validación
def backtest(datos, parametros):
    resultados = []
    
    # Walk-forward validation
    for inicio_test, fin_test in generar_ventanas_test(datos):
        # Datos de entrenamiento hasta inicio_test
        datos_train = datos[:inicio_test]
        
        # Entrenar ensemble de modelos
        modelos = entrenar_ensemble(datos_train, parametros)
        
        # Evaluar en ventana de test
        for fecha in range(inicio_test, fin_test):
            # Obtener señales de cada modelo
            senales_modelos = []
            for modelo in modelos:
                senal = modelo.generar_senal(datos, fecha)
                senales_modelos.append(senal)
            
            # Combinar señales con meta-modelo
            senal_combinada = meta_modelo.predecir(senales_modelos)
            
            # Aplicar gestión de riesgo
            posicion = aplicar_gestion_riesgo(senal_combinada, estado_portfolio)
            
            # Simular ejecución con costos
            rendimiento = simular_ejecucion(posicion, datos, fecha, costos)
            
            resultados.append({
                'fecha': fecha,
                'posicion': posicion,
                'rendimiento': rendimiento
            })
    
    # Calcular métricas
    sharpe = calcular_sharpe(resultados)
    drawdown_max = calcular_max_drawdown(resultados)
    correlacion_sp500 = calcular_correlacion(resultados, sp500)
    
    return {
        'resultados': resultados,
        'metricas': {
            'sharpe': sharpe,
            'drawdown_max': drawdown_max,
            'correlacion_sp500': correlacion_sp500
        }
    }
```

### 5. Métricas Esperadas

- **Sharpe ratio esperado**: 1.2-1.5 (post-costos)
- **Drawdown máximo esperado**: 12-15%
- **Correlación con S&P 500**: 0.3-0.5 (diversificación significativa)
- **Ratio de captación alcista/bajista**: 0.7/0.3 (asimétrico)

### 6. Consideraciones de Implementación

- Utilizar datos de yfinance para precios y volúmenes
- Implementar manejo de errores robusto para datos faltantes o erróneos
- Actualizar parámetros del modelo trimestralmente
- Monitorear continuamente el rendimiento y las métricas de riesgo
- Realizar análisis de atribución de rendimiento mensual para verificar fuentes de alpha
- Documentar todas las decisiones de diseño y parámetros para facilitar auditoría y mejora continua

Esta estrategia equilibra sofisticación analítica con robustez operativa, adaptándose a diferentes condiciones de mercado mientras mantiene un control riguroso del riesgo.

IMPLEMENTACIÓN EN CÓDIGO (del desarrollador):
```python
import os
import logging
import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.model_selection import TimeSeriesSplit
from hmmlearn import hmm
from scipy import stats
import warnings
from datetime import datetime, timedelta
import requests
from io import StringIO
from tqdm import tqdm

# Crear directorios para resultados
os.makedirs('./artifacts/results', exist_ok=True)
os.makedirs('./artifacts/results/figures', exist_ok=True)
os.makedirs('./artifacts/results/data', exist_ok=True)

# Configurar logging
logging.basicConfig(
    filename='./artifacts/errors.txt',
    level=logging.ERROR,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# Ignorar advertencias
warnings.filterwarnings('ignore')

class AdaptiveMomentumStrategy:
    def __init__(self, start_date='2010-01-01', end_date=None, lookback_years=5):
        """
        Inicializa la estrategia de momentum adaptativo con regímenes de mercado.
        
        Args:
            start_date: Fecha de inicio para el backtest
            end_date: Fecha de fin para el backtest (None = hoy)
            lookback_years: Años de datos históricos para entrenamiento inicial
        """
        self.start_date = start_date
        self.end_date = end_date if end_date else datetime.now().strftime('%Y-%m-%d')
        
        # Fecha de inicio para obtener datos (incluye periodo de lookback)
        lookback_start = (pd.to_datetime(start_date) - pd.DateOffset(years=lookback_years)).strftime('%Y-%m-%d')
        self.data_start_date = lookback_start
        
        # Parámetros de la estrategia
        self.momentum_windows = [5, 21, 63, 126]  # Ventanas de momentum en días
        self.regime_windows = [21, 63]  # Ventanas para detección de regímenes
        self.max_drawdown_limit = 0.15  # Límite de drawdown máximo tolerado
        self.max_exposure = 1.0  # Exposición máxima (1.0 = 100%)
        self.transaction_cost = 0.001  # Costo de transacción estimado (10bps)
        self.rebalance_frequency = 5  # Frecuencia de rebalanceo en días (5 = semanal)
        
        # Parámetros del modelo HMM
        self.n_regimes = 3  # Número de regímenes
        self.hmm_recalibration_days = 63  # Recalibración trimestral
        
        # Datos y resultados
        self.sp500_data = None
        self.sp500_symbols = None
        self.market_data = None
        self.factor_data = None
        self.regime_probs = None
        self.signals = None
        self.positions = None
        self.portfolio_values = None
        self.hmm_model = None
        
        # Inicializar
        try:
            self._load_data()
        except Exception as e:
            logging.error(f"Error al cargar datos: {str(e)}", exc_info=True)
            raise
    
    def _load_data(self):
        """Carga los datos necesarios para la estrategia"""
        # Obtener lista de símbolos del S&P 500 desde Wikipedia
        try:
            url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
            tables = pd.read_html(url)
            sp500_table = tables[0]
            self.sp500_symbols = sp500_table['Symbol'].tolist()
            
            # Guardar lista de símbolos
            sp500_table.to_csv('./artifacts/results/data/sp500_symbols.csv', index=False)
            
            print(f"Cargados {len(self.sp500_symbols)} símbolos del S&P 500")
        except Exception as e:
            logging.error(f"Error al obtener símbolos del S&P 500: {str(e)}", exc_info=True)
            # Usar un conjunto reducido de símbolos como fallback
            self.sp500_symbols = ['SPY']
        
        # Descargar datos del índice S&P 500 (SPY como proxy)
        try:
            self.sp500_data = yf.download('SPY', start=self.data_start_date, end=self.end_date)
            print(f"Datos del S&P 500 cargados: {len(self.sp500_data)} días")
            
            # Guardar datos del S&P 500
            self.sp500_data.to_csv('./artifacts/results/data/sp500_data.csv')
        except Exception as e:
            logging.error(f"Error al descargar datos del S&P 500: {str(e)}", exc_info=True)
            raise
        
        # Crear factores básicos
        self._create_factor_data()
    
    def _create_factor_data(self):
        """Crea datos de factores básicos para el modelo multifactorial"""
        try:
            # Usamos ETFs como proxies para los factores
            factor_etfs = {
                'Market': 'SPY',  # Mercado
                'Size': 'IWM',    # Tamaño (small cap)
                'Value': 'IWD',   # Valor
                'Momentum': 'MTUM', # Momentum
                'Quality': 'QUAL'  # Calidad
            }
            
            # Descargar datos de factores
            factor_data = {}
            for factor_name, ticker in factor_etfs.items():
                try:
                    data = yf.download(ticker, start=self.data_start_date, end=self.end_date)['Close']
                    factor_data[factor_name] = data
                except Exception as e:
                    logging.warning(f"Error al descargar factor {factor_name} ({ticker}): {str(e)}")
                    # Usar SPY como fallback para factores que no se pueden descargar
                    if factor_name != 'Market':  # Evitar duplicación para Market
                        factor_data[factor_name] = self.sp500_data['Close']
            
            # Convertir a DataFrame
            self.factor_data = pd.DataFrame(factor_data)
            
            # Calcular rendimientos de factores
            self.factor_returns = self.factor_data.pct_change().dropna()
            
            # Guardar datos de factores
            self.factor_returns.to_csv('./artifacts/results/data/factor_returns.csv')
            
            print(f"Datos de factores creados: {len(self.factor_returns)} días, {len(self.factor_returns.columns)} factores")
        except Exception as e:
            logging.error(f"Error al crear datos de factores: {str(e)}", exc_info=True)
            # Crear un factor básico de mercado como fallback
            self.factor_returns = pd.DataFrame({
                'Market': self.sp500_data['Close'].pct_change().dropna()
            })
    
    def _calculate_residual_returns(self, returns, factor_returns, window=252):
        """
        Calcula rendimientos residuales ajustados por factores
        
        Args:
            returns: Serie de rendimientos del activo
            factor_returns: DataFrame de rendimientos de factores
            window: Ventana para el cálculo de exposición a factores
        
        Returns:
            Serie de rendimientos residuales
        """
        # Asegurar alineación de índices
        aligned_data = pd.concat([returns, factor_returns], axis=1).dropna()
        if len(aligned_data) < window:
            window = max(30, len(aligned_data) // 2)  # Ajustar ventana si no hay suficientes datos
        
        # Inicializar serie de rendimientos residuales
        residual_returns = pd.Series(index=aligned_data.index, dtype=float)
        
        # Calcular rendimientos residuales con ventana móvil
        for i in range(window, len(aligned_data)):
            # Datos de entrenamiento
            train_data = aligned_data.iloc[i-window:i]
            y = train_data.iloc[:, 0]  # Rendimientos del activo
            X = train_data.iloc[:, 1:]  # Rendimientos de factores
            
            # Ajustar modelo de regresión
            model = Ridge(alpha=0.1)  # Ridge para estabilidad
            model.fit(X, y)
            
            # Calcular rendimiento esperado basado en factores
            current_factors = aligned_data.iloc[i, 1:].values.reshape(1, -1)
            expected_return = model.predict(current_factors)[0]
            
            # Rendimiento residual = rendimiento real - rendimiento esperado
            actual_return = aligned_data.iloc[i, 0]
            residual_returns.iloc[i] = actual_return - expected_return
        
        return residual_returns.dropna()
    
    def _calculate_adaptive_volatility(self, returns, window=21):
        """
        Calcula volatilidad adaptativa con ponderación exponencial
        
        Args:
            returns: Serie de rendimientos
            window: Ventana base para el cálculo
        
        Returns:
            Serie de volatilidad adaptativa
        """
        # Volatilidad base (desviación estándar móvil)
        rolling_std = returns.rolling(window=window).std()
        
        # Volatilidad con ponderación exponencial (más sensible a cambios recientes)
        ewma_vol = returns.ewm(span=window).std()
        
        # Combinar ambas medidas (70% EWMA, 30% rolling)
        adaptive_vol = 0.7 * ewma_vol + 0.3 * rolling_std
        
        # Asegurar valor mínimo para evitar divisiones por cero
        min_vol = returns.std() * 0.2  # 20% de la volatilidad total como mínimo
        adaptive_vol = adaptive_vol.clip(lower=min_vol)
        
        return adaptive_vol
    
    def _winsorize_signals(self, signals, n_mad=3):
        """
        Aplica winsorización basada en desviación absoluta mediana (MAD)
        
        Args:
            signals: Serie o DataFrame de señales
            n_mad: Número de MADs para el umbral
        
        Returns:
            Señales winsorizadas
        """
        if isinstance(signals, pd.DataFrame):
            return signals.apply(lambda x: self._winsorize_signals(x, n_mad))
        
        median = signals.median()
        mad = stats.median_abs_deviation(signals.dropna(), nan_policy='omit')
        
        upper_bound = median + n_mad * mad
        lower_bound = median - n_mad * mad
        
        return signals.clip(lower=lower_bound, upper=upper_bound)
    
    def _calculate_atr(self, data, window=14):
        """
        Calcula el Average True Range (ATR)
        
        Args:
            data: DataFrame con OHLC
            window: Ventana para el cálculo
        
        Returns:
            Serie con valores de ATR
        """
        high = data['High']
        low = data['Low']
        close = data['Close']
        
        # Calcular True Range
        tr1 = high - low
        tr2 = abs(high - close.shift())
        tr3 = abs(low - close.shift())
        
        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        
        # Calcular ATR
        atr = tr.rolling(window=window).mean()
        
        return atr
    
    def _calculate_regime_features(self):
        """
        Calcula características para la identificación de regímenes de mercado
        
        Returns:
            DataFrame con características para el modelo HMM
        """
        # Datos base
        market_data = self.sp500_data.copy()
        returns = market_data['Close'].pct_change().dropna()
        
        features = pd.DataFrame(index=market_data.index)
        
        # 1. Volatilidad realizada (ATR normalizado)
        atr = self._calculate_atr(market_data, window=21)
        features['volatility'] = atr / market_data['Close']
        
        # 2. Autocorrelación de rendimientos
        features['autocorr'] = returns.rolling(window=21).apply(
            lambda x: x.autocorr(lag=1) if len(x.dropna()) > 1 else 0
        )
        
        # 3. Ratio de días positivos/negativos
        features['pos_neg_ratio'] = returns.rolling(window=21).apply(
            lambda x: len(x[x > 0]) / max(1, len(x[x < 0]))
        )
        
        # 4. Tendencia (precio relativo a media móvil)
        features['trend'] = market_data['Close'] / market_data['Close'].rolling(window=63).mean() - 1
        
        # 5. Volatilidad relativa (corto vs largo plazo)
        vol_st = returns.rolling(window=21).std()
        vol_lt = returns.rolling(window=63).std()
        features['rel_vol'] = vol_st / vol_lt
        
        # Normalizar características
        scaler = StandardScaler()
        normalized_features = pd.DataFrame(
            scaler.fit_transform(features.dropna()),
            index=features.dropna().index,
            columns=features.columns
        )
        
        return normalized_features
    
    def _train_hmm_model(self, features):
        """
        Entrena un modelo HMM para identificación de regímenes
        
        Args:
            features: DataFrame con características para el modelo
        
        Returns:
            Modelo HMM entrenado
        """
        # Preparar datos
        X = features.dropna().values
        
        # Inicializar y entrenar modelo HMM
        model = hmm.GaussianHMM(
            n_components=self.n_regimes,
            covariance_type="full",
            n_iter=100,
            random_state=42
        )
        
        try:
            model.fit(X)
            return model
        except Exception as e:
            logging.error(f"Error al entrenar modelo HMM: {str(e)}", exc_info=True)
            # Fallback: modelo simple basado en K-means
            kmeans = KMeans(n_clusters=self.n_regimes, random_state=42)
            kmeans.fit(X)
            
            # Crear un pseudo-modelo HMM basado en K-means
            class KMeansHMM:
                def __init__(self, kmeans):
                    self.kmeans = kmeans
                
                def predict(self, X):
                    return self.kmeans.predict(X)
                
                def predict_proba(self, X):
                    # Distancias a centroides
                    distances = self.kmeans.transform(X)
                    # Convertir distancias a probabilidades (softmax)
                    exp_distances = np.exp(-distances)
                    return exp_distances / exp_distances.sum(axis=1, keepdims=True)
            
            return KMeansHMM(kmeans)
    
    def _identify_regimes(self, recalibrate=False):
        """
        Identifica regímenes de mercado usando HMM
        
        Args:
            recalibrate: Si es True, recalibra el modelo HMM
        
        Returns:
            DataFrame con probabilidades de regímenes
        """
        # Calcular características para regímenes
        regime_features = self._calculate_regime_features()
        
        # Entrenar o recalibrar modelo HMM si es necesario
        if self.hmm_model is None or recalibrate:
            self.hmm_model = self._train_hmm_model(regime_features)
        
        # Predecir probabilidades de regímenes
        try:
            regime_probs = self.hmm_model.predict_proba(regime_features.values)
            
            # Convertir a DataFrame
            regime_probs_df = pd.DataFrame(
                regime_probs,
                index=regime_features.index,
                columns=[f'Regime_{i+1}' for i in range(self.n_regimes)]
            )
            
            return regime_probs_df
        except Exception as e:
            logging.error(f"Error al predecir regímenes: {str(e)}", exc_info=True)
            # Fallback: asignar probabilidad uniforme a todos los regímenes
            uniform_probs = np.ones((len(regime_features), self.n_regimes)) / self.n_regimes
            return pd.DataFrame(
                uniform_probs,
                index=regime_features.index,
                columns=[f'Regime_{i+1}' for i in range(self.n_regimes)]
            )
    
    def _calculate_momentum_signals(self):
        """
        Calcula señales de momentum para diferentes horizontes temporales
        
        Returns:
            DataFrame con señales de momentum normalizadas
        """
        signals = pd.DataFrame(index=self.sp500_data.index)
        
        # Calcular rendimientos para diferentes horizontes
        for window in self.momentum_windows:
            # Rendimiento simple
            returns = self.sp500_data['Close'].pct_change(window).shift(1)
            
            # Calcular rendimiento residual (ajustado por factores)
            if window >= 21:  # Solo para horizontes medianos/largos
                try:
                    daily_returns = self.sp500_data['Close'].pct_change().dropna()
                    residual_returns = self._calculate_residual_returns(
                        daily_returns, 
                        self.factor_returns
                    )
                    # Acumular rendimientos residuales
                    cum_residual = (1 + residual_returns).rolling(window=window).apply(
                        lambda x: np.prod(x) - 1
                    ).shift(1)
                    
                    # Combinar señal residual con rendimiento simple (70% residual, 30% simple)
                    aligned_data = pd.concat([cum_residual, returns], axis=1).dropna()
                    combined_signal = 0.7 * aligned_data.iloc[:, 0] + 0.3 * aligned_data.iloc[:, 1]
                    signals[f'mom_{window}'] = combined_signal
                except Exception as e:
                    logging.warning(f"Error al calcular rendimientos residuales para ventana {window}: {str(e)}")
                    signals[f'mom_{window}'] = returns
            else:
                signals[f'mom_{window}'] = returns
        
        # Normalizar señales por volatilidad
        for col in signals.columns:
            window = int(col.split('_')[1])
            vol = self._calculate_adaptive_volatility(signals[col], window=min(window, 21))
            signals[col] = signals[col] / vol
        
        # Invertir señales para horizontes cortos (reversión a la media)
        for col in signals.columns:
            window = int(col.split('_')[1])
            if window <= 5:
                signals[col] = -signals[col]
        
        # Aplicar winsorización
        signals = self._winsorize_signals(signals)
        
        return signals
    
    def _combine_signals(self, signals, regime_probs):
        """
        Combina señales de momentum según regímenes de mercado
        
        Args:
            signals: DataFrame con señales de momentum
            regime_probs: DataFrame con probabilidades de regímenes
        
        Returns:
            Serie con señal combinada
        """
        # Definir pesos de señales por régimen
        # Régimen 1: Tendencia - Preferir momentum de medio/largo plazo
        # Régimen 2: Rango - Preferir reversión de corto plazo
        # Régimen 3: Crisis - Reducir exposición general
        
        regime_weights = {
            'Regime_1': {  # Tendencia
                'mom_5': 0.1,
                'mom_21': 0.3,
                'mom_63': 0.4,
                'mom_126': 0.2
            },
            'Regime_2': {  # Rango
                'mom_5': 0.5,
                'mom_21': 0.3,
                'mom_63': 0.1,
                'mom_126': 0.1
            },
            'Regime_3': {  # Crisis
                'mom_5': 0.2,
                'mom_21': 0.2,
                'mom_63': 0.3,
                'mom_126': 0.3
            }
        }
        
        # Inicializar señal combinada
        combined_signal = pd.Series(0, index=signals.index)
        
        # Combinar señales según pesos de regímenes
        for regime, weights in regime_weights.items():
            for signal_name, weight in weights.items():
                if signal_name in signals.columns and regime in regime_probs.columns:
                    # Multiplicar señal por peso y probabilidad de régimen
                    weighted_signal = signals[signal_name] * weight * regime_probs[regime]
                    combined_signal = combined_signal.add(weighted_signal, fill_value=0)
        
        return combined_signal
    
    def _apply_risk_management(self, combined_signal, portfolio_state=None):
        """
        Aplica gestión dinámica de riesgo a la señal combinada
        
        Args:
            combined_signal: Serie con señal combinada
            portfolio_state: Estado actual del portafolio (opcional)
        
        Returns:
            Serie con posiciones ajustadas por riesgo
        """
        # Inicializar posiciones con señal combinada
        positions = combined_signal.copy()
        
        # Calcular drawdown si hay estado de portafolio
        drawdown_factor = 1.0
        if portfolio_state is not None and 'equity_curve' in portfolio_state:
            equity = portfolio_state['equity_curve']
            rolling_max = equity.cummax()
            drawdown = (equity - rolling_max) / rolling_max
            current_drawdown = drawdown.iloc[-1]
            
            # Ajustar exposición según drawdown
            drawdown_factor = 1 - (current_drawdown / self.max_drawdown_limit)**2
            drawdown_factor = max(0, min(1, drawdown_factor))
        
        # Filtro de volatilidad
        vol_factor = 1.0
        returns = self.sp500_data['Close'].pct_change()
        vol_st = returns.rolling(window=21).std()
        vol_lt = returns.rolling(window=63).std()
        vol_ratio = vol_st / vol_lt
        
        # Reducir exposición cuando volatilidad a corto plazo > volatilidad a largo plazo
        vol_factor = pd.Series(1.0, index=positions.index)
        vol_factor[vol_ratio > 1.2] = 1.2 / vol_ratio[vol_ratio > 1.2]
        vol_factor = vol_factor.clip(0.3, 1.0)  # Limitar reducción de exposición
        
        # Aplicar factores de riesgo
        risk_adjusted_positions = positions * drawdown_factor * vol_factor
        
        # Limitar exposición máxima
        risk_adjusted_positions = risk_adjusted_positions.clip(-self.max_exposure, self.max_exposure)
        
        # Aplicar filtro de tendencia de largo plazo
        price = self.sp500_data['Close']
        sma200 = price.rolling(window=200).mean()
        trend_filter = (price > sma200).astype(int) * 2 - 1  # 1 si alcista, -1 si bajista
        
        # Reducir exposición en contra de la tendencia de largo plazo
        for i in range(len(risk_adjusted_positions)):
            if not np.isnan(trend_filter.iloc[i]) and not np.isnan(risk_adjusted_positions.iloc[i]):
                # Si posición y tendencia tienen signos opuestos, reducir exposición
                if risk_adjusted_positions.iloc[i] * trend_filter.iloc[i] < 0:
                    risk_adjusted_positions.iloc[i] *= 0.5
        
        return risk_adjusted_positions
    
    def _apply_execution_logic(self, target_positions):
        """
        Aplica lógica de ejecución con bandas de no-transacción
        
        Args:
            target_positions: Serie con posiciones objetivo
        
        Returns:
            Serie con posiciones ejecutadas
        """
        # Inicializar posiciones ejecutadas
        if not hasattr(self, 'current_positions'):
            self.current_positions = pd.Series(0, index=target_positions.index)
        
        executed_positions = self.current_positions.copy()
        
        # Calcular umbral mínimo adaptativo para transacciones
        returns = self.sp500_data['Close'].pct_change()
        vol = returns.rolling(window=21).std()
        min_threshold = np.maximum(0.005, self.transaction_cost * 3)  # Mínimo 0.5% o 3x costos
        adaptive_threshold = min_threshold * (1 + vol / vol.mean())
        
        # Aplicar lógica de ejecución
        for date in target_positions.index:
            if date in executed_positions.index:
                current_pos = executed_positions.loc[date]
                target_pos = target_positions.loc[date]
                threshold = adaptive_threshold.loc[date] if date in adaptive_threshold.index else min_threshold
                
                # Ejecutar solo si el cambio supera el umbral
                if abs(target_pos - current_pos) > threshold:
                    executed_positions.loc[date] = target_pos
                else:
                    executed_positions.loc[date] = current_pos
        
        # Actualizar posiciones actuales
        self.current_positions = executed_positions
        
        return executed_positions
    
    def _calculate_portfolio_returns(self, positions):
        """
        Calcula rendimientos del portafolio basados en posiciones
        
        Args:
            positions: Serie con posiciones
        
        Returns:
            Serie con rendimientos del portafolio
        """
        # Obtener rendimientos del mercado
        market_returns = self.sp500_data['Close'].pct_change()
        
        # Alinear datos
        aligned_data = pd.concat([positions.shift(1), market_returns], axis=1).dropna()
        
        # Calcular rendimientos del portafolio (posición * rendimiento del mercado)
        portfolio_returns = aligned_data.iloc[:, 0] * aligned_data.iloc[:, 1]
        
        # Aplicar costos de transacción
        position_changes = positions.diff().abs()
        transaction_costs = position_changes * self.transaction_cost
        
        # Rendimientos netos
        net_returns = portfolio_returns - transaction_costs
        
        return net_returns
    
    def _calculate_performance_metrics(self, returns):
        """
        Calcula métricas de rendimiento del portafolio
        
        Args:
            returns: Serie con rendimientos del portafolio
        
        Returns:
            Diccionario con métricas de rendimiento
        """
        # Rendimiento acumulado
        cum_returns = (1 + returns).cumprod()
        total_return = cum_returns.iloc[-1] - 1
        
        # Volatilidad anualizada
        annual_vol = returns.std() * np.sqrt(252)
        
        # Sharpe ratio
        risk_free_rate = 0.02 / 252  # Tasa libre de riesgo diaria (2% anual)
        sharpe_ratio = (returns.mean() - risk_free_rate) / returns.std() * np.sqrt(252)
        
        # Drawdown
        rolling_max = cum_returns.cummax()
        drawdown = (cum_returns - rolling_max) / rolling_max
        max_drawdown = drawdown.min()
        
        # Ratio de captación (upside/downside capture)
        market_returns = self.sp500_data['Close'].pct_change().loc[returns.index]
        up_market = market_returns > 0
        down_market = market_returns < 0
        
        if up_market.sum() > 0:
            upside_capture = (returns[up_market].mean() / market_returns[up_market].mean())
        else:
            upside_capture = np.nan
            
        if down_market.sum() > 0:
            downside_capture = (returns[down_market].mean() / market_returns[down_market].mean())
        else:
            downside_capture = np.nan
        
        # Métricas adicionales
        win_rate = (returns > 0).mean()
        profit_loss_ratio = abs(returns[returns > 0].mean() / returns[returns < 0].mean()) if (returns < 0).sum() > 0 else np.inf
        
        return {
            'total_return': total_return,
            'annual_return': (1 + total_return) ** (252 / len(returns)) - 1,
            'annual_volatility': annual_vol,
            'sharpe_ratio': sharpe_ratio,
            'max_drawdown': max_drawdown,
            'win_rate': win_rate,
            'profit_loss_ratio': profit_loss_ratio,
            'upside_capture': upside_capture,
            'downside_capture': downside_capture,
            'cum_returns': cum_returns
        }
    
    def backtest(self):
        """
        Ejecuta backtest de la estrategia
        
        Returns:
            Diccionario con resultados del backtest
        """
        try:
            # Filtrar datos para el período de backtest
            backtest_data = self.sp500_data[self.start_date:self.end_date].copy()
            
            # Inicializar resultados
            results = {
                'signals': pd.DataFrame(index=backtest_data.index),
                'positions': pd.Series(index=backtest_data.index),
                'returns': pd.Series(index=backtest_data.index),
                'equity_curve': pd.Series(index=backtest_data.index),
                'regime_probs': pd.DataFrame(index=backtest_data.index)
            }
            
            # Inicializar portafolio
            portfolio_state = {
                'equity_curve': pd.Series(1.0, index=backtest_data.index)
            }
            
            # Dividir en ventanas para walk-forward testing
            test_windows = []
            window_size = 126  # ~6 meses
            for i in range(0, len(backtest_data), window_size):
                start_idx = i
                end_idx = min(i + window_size, len(backtest_data))
                test_windows.append((start_idx, end_idx))
            
            # Ejecutar backtest por ventanas
            for start_idx, end_idx in tqdm(test_windows, desc="Ejecutando backtest"):
                window_dates = backtest_data.index[start_idx:end_idx]
                
                # Identificar regímenes (recalibrar al inicio de cada ventana)
                recalibrate = start_idx == 0 or (start_idx // window_size) % 2 == 0  # Recalibrar cada 2 ventanas
                regime_probs = self._identify_regimes(recalibrate=recalibrate)
                
                # Calcular señales de momentum
                momentum_signals = self._calculate_momentum_signals()
                
                # Combinar señales según regímenes
                combined_signal = self._combine_signals(momentum_signals, regime_probs)
                
                # Aplicar gestión de riesgo
                risk_adjusted_positions = self._apply_risk_management(combined_signal, portfolio_state)
                
                # Aplicar lógica de ejecución
                executed_positions = self._apply_execution_logic(risk_adjusted_positions)
                
                # Guardar resultados para esta ventana
                for date in window_dates:
                    if date in regime_probs.index:
                        results['regime_probs'].loc[date] = regime_probs.loc[date]
                    
                    if date in momentum_signals.index:
                        for col in momentum_signals.columns:
                            results['signals'].loc[date, col] = momentum_signals.loc[date, col]
                    
                    if date in combined_signal.index:
                        results['signals'].loc[date, 'combined'] = combined_signal.loc[date]
                    
                    if date in risk_adjusted_positions.index:
                        results['signals'].loc[date, 'risk_adjusted'] = risk_adjusted_positions.loc[date]
                    
                    if date in executed_positions.index:
                        results['positions'].loc[date] = executed_positions.loc[date]
            
            # Calcular rendimientos del portafolio
            portfolio_returns = self._calculate_portfolio_returns(results['positions'])
            results['returns'] = portfolio_returns
            
            # Calcular curva de equity
            results['equity_curve'] = (1 + portfolio_returns).cumprod()
            
            # Calcular métricas de rendimiento
            results['metrics'] = self._calculate_performance_metrics(portfolio_returns)
            
            # Guardar resultados
            self._save_results(results)
            
            return results
        
        except Exception as e:
            logging.error(f"Error en backtest: {str(e)}", exc_info=True)
            raise
    
    def _save_results(self, results):
        """
        Guarda resultados del backtest
        
        Args:
            results: Diccionario con resultados del backtest
        """
        try:
            # Guardar señales
            results['signals'].to_csv('./artifacts/results/data/signals.csv')
            
            # Guardar posiciones
            results['positions'].to_csv('./artifacts/results/data/positions.csv')
            
            # Guardar rendimientos
            results['returns'].to_csv('./artifacts/results/data/returns.csv')
            
            # Guardar curva de equity
            results['equity_curve'].to_csv('./artifacts/results/data/equity_curve.csv')
            
            # Guardar probabilidades de regímenes
            results['regime_probs'].to_csv('./artifacts/results/data/regime_probs.csv')
            
            # Guardar métricas
            metrics_df = pd.DataFrame({k: [v] for k, v in results['metrics'].items() 
                                      if k != 'cum_returns'})
            metrics_df.to_csv('./artifacts/results/data/performance_metrics.csv', index=False)
            
            # Generar gráficos
            self._generate_plots(results)
            
        except Exception as e:
            logging.error(f"Error al guardar resultados: {str(e)}", exc_info=True)
    
    def _generate_plots(self, results):
        """
        Genera gráficos de resultados
        
        Args:
            results: Diccionario con resultados del backtest
        """
        try:
            # Configuración de estilo
            plt.style.use('seaborn-v0_8-darkgrid')
            
            # 1. Curva de equity
            plt.figure(figsize=(12, 6))
            results['equity_curve'].plot(label='Estrategia')
            
            # Comparar con S&P 500
            sp500_returns = self.sp500_data['Close'].pct_change().loc[results['returns'].index]
            sp500_equity = (1 + sp500_returns).cumprod()
            sp500_equity.plot(label='S&P 500')
            
            plt.title('Curva de Equity')
            plt.xlabel('Fecha')
            plt.ylabel('Valor (normalizado)')
            plt.legend()
            plt.grid(True)
            plt.savefig('./artifacts/results/figures/equity_curve.png')
            plt.close()
            
            # 2. Drawdown
            plt.figure(figsize=(12, 6))
            equity = results['equity_curve']
            rolling_max = equity.cummax()
            drawdown = (equity - rolling_max) / rolling_max
            drawdown.plot(label='Drawdown')
            
            plt.title('Drawdown')
            plt.xlabel('Fecha')
            plt.ylabel('Drawdown')
            plt.legend()
            plt.grid(True)
            plt.savefig('./artifacts/results/figures/drawdown.png')
            plt.close()
            
            # 3. Posiciones a lo largo del tiempo
            plt.figure(figsize=(12, 6))
            results['positions'].plot(label='Posición')
            
            plt.title('Posiciones a lo largo del tiempo')
            plt.xlabel('Fecha')
            plt.ylabel('Posición')
            plt.legend()
            plt.grid(True)
            plt.savefig('./artifacts/results/figures/positions.png')
            plt.close()
            
            # 4. Regímenes de mercado
            plt.figure(figsize=(12, 6))
            regime_probs = results['regime_probs']
            
            for col in regime_probs.columns:
                regime_probs[col].plot(label=col)
            
            plt.title('Probabilidades de Regímenes de Mercado')
            plt.xlabel('Fecha')
            plt.ylabel('Probabilidad')
            plt.legend()
            plt.grid(True)
            plt.savefig('./artifacts/results/figures/regimes.png')
            plt.close()
            
            # 5. Señales de momentum
            plt.figure(figsize=(12, 6))
            signals = results['signals']
            
            for col in signals.columns:
                if col.startswith('mom_'):
                    signals[col].plot(label=col, alpha=0.7)
            
            plt.title('Señales de Momentum')
            plt.xlabel('Fecha')
            plt.ylabel('Señal')
            plt.legend()
            plt.grid(True)
            plt.savefig('./artifacts/results/figures/momentum_signals.png')
            plt.close()
            
            # 6. Señal combinada vs. posición
            plt.figure(figsize=(12, 6))
            if 'combined' in signals.columns:
                signals['combined'].plot(label='Señal Combinada', alpha=0.7)
            if 'risk_adjusted' in signals.columns:
                signals['risk_adjusted'].plot(label='Señal Ajustada por Riesgo', alpha=0.7)
            results['positions'].plot(label='Posición Final', linewidth=2)
            
            plt.title('Señal Combinada vs. Posición Final')
            plt.xlabel('Fecha')
            plt.ylabel('Valor')
            plt.legend()
            plt.grid(True)
            plt.savefig('./artifacts/results/figures/combined_signal_vs_position.png')
            plt.close()
            
            # 7. Rendimientos mensuales
            plt.figure(figsize=(12, 8))
            monthly_returns = results['returns'].resample('M').apply(
                lambda x: (1 + x).prod() - 1
            )
            
            monthly_returns_heatmap = monthly_returns.groupby(
                [monthly_returns.index.year, monthly_returns.index.month]
            ).first().unstack()
            
            sns.heatmap(
                monthly_returns_heatmap, 
                annot=True, 
                fmt=".1%", 
                cmap="RdYlGn",
                center=0,
                linewidths=1
            )
            
            plt.title('Rendimientos Mensuales')
            plt.xlabel('Mes')
            plt.ylabel('Año')
            plt.savefig('./artifacts/results/figures/monthly_returns.png')
            plt.close()
            
        except Exception as e:
            logging.error(f"Error al generar gráficos: {str(e)}", exc_info=True)
    
    def walk_forward_optimization(self, param_grid, n_splits=5):
        """
        Realiza optimización walk-forward de parámetros
        
        Args:
            param_grid: Diccionario con parámetros a optimizar
            n_splits: Número de divisiones para validación cruzada
        
        Returns:
            Diccionario con mejores parámetros y resultados
        """
        try:
            # Filtrar datos para el período de backtest
            backtest_data = self.sp500_data[self.start_date:self.end_date].copy()
            
            # Crear divisiones temporales
            tscv = TimeSeriesSplit(n_splits=n_splits)
            splits = list(tscv.split(backtest_data))
            
            # Inicializar resultados
            optimization_results = []
            
            # Generar combinaciones de parámetros
            param_combinations = self._generate_param_combinations(param_grid)
            
            # Ejecutar optimización
            for params in tqdm(param_combinations, desc="Optimizando parámetros"):
                fold_results = []
                
                for train_idx, test_idx in splits:
                    # Dividir datos
                    train_data = backtest_data.iloc[train_idx]
                    test_data = backtest_data.iloc[test_idx]
                    
                    # Aplicar parámetros
                    for param, value in params.items():
                        setattr(self, param, value)
                    
                    # Ejecutar backtest en datos de prueba
                    test_start = test_data.index[0].strftime('%Y-%m-%d')
                    test_end = test_data.index[-1].strftime('%Y-%m-%d')
                    
                    # Guardar configuración actual
                    original_start = self.start_date
                    original_end = self.end_date
                    
                    # Configurar para este fold
                    self.start_date = test_start
                    self.end_date = test_end
                    
                    # Ejecutar backtest
                    results = self.backtest()
                    
                    # Restaurar configuración
                    self.start_date = original_start
                    self.end_date = original_end
                    
                    # Guardar métricas
                    fold_results.append({
                        'sharpe_ratio': results['metrics']['sharpe_ratio'],
                        'max_drawdown': results['metrics']['max_drawdown'],
                        'total_return': results['metrics']['total_return']
                    })
                
                # Calcular métricas promedio
                avg_metrics = {
                    'sharpe_ratio': np.mean([r['sharpe_ratio'] for r in fold_results]),
                    'max_drawdown': np.mean([r['max_drawdown'] for r in fold_results]),
                    'total_return': np.mean([r['total_return'] for r in fold_results])
                }
                
                # Guardar resultados
                optimization_results.append({
                    'params': params,
                    'metrics': avg_metrics,
                    'fold_results': fold_results
                })
            
            # Ordenar por Sharpe ratio
            optimization_results.sort(key=lambda x: x['metrics']['sharpe_ratio'], reverse=True)
            
            # Guardar resultados de optimización
            self._save_optimization_results(optimization_results)
            
            # Aplicar mejores parámetros
            best_params = optimization_results[0]['params']
            for param, value in best_params.items():
                setattr(self, param, value)
            
            return {
                'best_params': best_params,
                'best_metrics': optimization_results[0]['metrics'],
                'all_results': optimization_results
            }
            
        except Exception as e:
            logging.error(f"Error en optimización walk-forward: {str(e)}", exc_info=True)
            raise
    
    def _generate_param_combinations(self, param_grid):
        """
        Genera todas las combinaciones de parámetros
        
        Args:
            param_grid: Diccionario con parámetros a optimizar
        
        Returns:
            Lista de diccionarios con combinaciones de parámetros
        """
        # Función recursiva para generar combinaciones
        def generate_combinations(param_grid, current_idx=0, current_params={}):
            if current_idx == len(param_names):
                return [current_params.copy()]
            
            param_name = param_names[current_idx]
            param_values = param_grid[param_name]
            
            combinations = []
            for value in param_values:
                current_params[param_name] = value
                combinations.extend(generate_combinations(param_grid, current_idx + 1, current_params))
            
            return combinations
        
        param_names = list(param_grid.keys())
        return generate_combinations(param_grid, 0, {})
    
    def _save_optimization_results(self, optimization_results):
        """
        Guarda resultados de optimización
        
        Args:
            optimization_results: Lista con resultados de optimización
        """
        try:
            # Crear DataFrame con resultados
            results_data = []
            
            for result in optimization_results:
                row = {}
                
                # Añadir parámetros
                for param, value in result['params'].items():
                    row[param] = value
                
                # Añadir métricas
                for metric, value in result['metrics'].items():
                    row[metric] = value
                
                results_data.append(row)
            
            # Convertir a DataFrame
            results_df = pd.DataFrame(results_data)
            
            # Guardar resultados
            results_df.to_csv('./artifacts/results/data/optimization_results.csv', index=False)
            
            # Generar gráfico de parámetros vs. métricas
            self._plot_optimization_results(results_df)
            
        except Exception as e:
            logging.error(f"Error al guardar resultados de optimización: {str(e)}", exc_info=True)
    
    def _plot_optimization_results(self, results_df):
        """
        Genera gráficos de resultados de optimización
        
        Args:
            results_df: DataFrame con resultados de optimización
        """
        try:
            # Configuración de estilo
            plt.style.use('seaborn-v0_8-darkgrid')
            
            # Identificar parámetros y métricas
            param_cols = [col for col in results_df.columns if col not in ['sharpe_ratio', 'max_drawdown', 'total_return']]
            metric_cols = ['sharpe_ratio', 'max_drawdown', 'total_return']
            
            # Generar gráficos para cada parámetro vs. métricas
            for param in param_cols:
                if len(results_df[param].unique()) > 1:  # Solo si hay múltiples valores
                    plt.figure(figsize=(12, 8))
                    
                    for i, metric in enumerate(metric_cols):
                        plt.subplot(len(metric_cols), 1, i+1)
                        
                        # Agrupar por parámetro y calcular media
                        grouped = results_df.groupby(param)[metric].mean()
                        grouped.plot(marker='o')
                        
                        plt.title(f'{metric} vs. {param}')
                        plt.xlabel(param)
                        plt.ylabel(metric)
                        plt.grid(True)
                    
                    plt.tight_layout()
                    plt.savefig(f'./artifacts/results/figures/optimization_{param}.png')
                    plt.close()
            
            # Matriz de correlación entre parámetros y métricas
            plt.figure(figsize=(10, 8))
            corr_matrix = results_df.corr()
            sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)
            
            plt.title('Correlación entre Parámetros y Métricas')
            plt.tight_layout()
            plt.savefig('./artifacts/results/figures/optimization_correlation.png')
            plt.close()
            
        except Exception as e:
            logging.error(f"Error al generar gráficos de optimización: {str(e)}", exc_info=True)

# Ejecutar estrategia
if __name__ == "__main__":
    try:
        # Inicializar estrategia
        strategy = AdaptiveMomentumStrategy(start_date='2015-01-01')
        
        # Ejecutar backtest
        results = strategy.backtest()
        
        # Mostrar métricas principales
        print("\nMétricas de rendimiento:")
        print(f"Rendimiento total: {results['metrics']['total_return']:.2%}")
        print(f"Rendimiento anualizado: {results['metrics']['annual_return']:.2%}")
        print(f"Volatilidad anualizada: {results['metrics']['annual_volatility']:.2%}")
        print(f"Sharpe ratio: {results['metrics']['sharpe_ratio']:.2f}")
        print(f"Máximo drawdown: {results['metrics']['max_drawdown']:.2%}")
        print(f"Win rate: {results['metrics']['win_rate']:.2%}")
        print(f"Ratio profit/loss: {results['metrics']['profit_loss_ratio']:.2f}")
        
        # Optimización de parámetros (opcional)
        """
        param_grid = {
            'max_exposure': [0.8, 1.0, 1.2],
            'max_drawdown_limit': [0.1, 0.15, 0.2],
            'n_regimes': [2, 3, 4]
        }
        
        optimization_results = strategy.walk_forward_optimization(param_grid, n_splits=3)
        
        print("\nMejores parámetros:")
        for param, value in optimization_results['best_params'].items():
            print(f"{param}: {value}")
        
        print("\nMétricas con mejores parámetros:")
        for metric, value in optimization_results['best_metrics'].items():
            if metric == 'sharpe_ratio':
                print(f"Sharpe ratio: {value:.2f}")
            else:
                print(f"{metric}: {value:.2%}")
        """
        
    except Exception as e:
        logging.error(f"Error en la ejecución principal: {str(e)}", exc_info=True)
        print(f"Error: {str(e)}")
```

Por favor analiza si el código implementa correctamente la idea conceptual y sugiere mejoras específicas.


RESPONSE:
# Revisión de Código: Estrategia de Momentum Adaptativo

## 1. ¿El código implementa correctamente la idea? Parcialmente

El código implementa muchos aspectos de la idea conceptual, pero tiene algunas deficiencias importantes y áreas de mejora.

## 2. Sugerencias específicas:

1. **Cálculo de rendimientos residuales incorrecto (líneas 380-410)**: La implementación actual no calcula correctamente los rendimientos residuales. El método `_calculate_residual_returns` usa una ventana móvil que puede causar look-ahead bias al entrenar el modelo con datos futuros. Debería usar solo datos históricos para cada punto de tiempo.

2. **Problema de alineación temporal (líneas 390-410)**: Hay problemas de alineación en las series temporales cuando se combinan rendimientos residuales con rendimientos simples. La línea `aligned_data = pd.concat([cum_residual, returns], axis=1).dropna()` puede eliminar fechas importantes y crear inconsistencias.

3. **Gestión de NaN inadecuada (líneas 520-530)**: El método `_combine_signals` no maneja adecuadamente los valores NaN cuando combina señales, lo que puede propagar NaNs a través del cálculo.

4. **Look-ahead bias en el walk-forward test (líneas 830-880)**: La implementación del backtest no respeta estrictamente la separación entre datos de entrenamiento y prueba. Por ejemplo, al calcular señales de momentum, usa datos que no estarían disponibles en tiempo real.

5. **Falta de implementación completa del ensemble (línea 3)**: La idea original incluye un sistema de ensemble con stacking, pero la implementación actual no lo incluye. Solo hay un modelo base sin el meta-modelo para combinar múltiples predicciones.

6. **Implementación incompleta del filtro de tendencia (líneas 600-620)**: El filtro de tendencia de largo plazo se implementa de forma simplista comparando el precio con la SMA200, pero no ajusta adecuadamente la exposición al momentum de corto plazo según la dirección de la tendencia como se especifica en la idea.

7. **Normalización adaptativa incompleta (líneas 440-450)**: La implementación de la normalización adaptativa no incluye la winsorización dinámica basada en desviaciones MAD como se especifica en la idea original.

8. **Falta de stops dinámicos contextuales (línea 580)**: La idea original incluye stops dinámicos basados en ATR con multiplicadores específicos por régimen, pero esto no está implementado en el código.

9. **Problema en el cálculo de drawdown (líneas 570-580)**: El cálculo del factor de drawdown asume que `portfolio_state` siempre tiene una clave 'equity_curve', pero esto puede no ser cierto en la primera iteración, causando errores.

10. **Ineficiencia en la generación de combinaciones de parámetros (líneas 1050-1070)**: El método `_generate_param_combinations` usa una aproximación recursiva que puede ser ineficiente para grandes grids. Sería mejor usar `itertools.product`.

11. **Falta de validación de datos (líneas 180-220)**: No hay suficiente validación de los datos descargados, lo que puede llevar a errores si hay problemas con yfinance o datos faltantes.

12. **Manejo inadecuado de la volatilidad (líneas 590-600)**: El factor de volatilidad se calcula pero se limita arbitrariamente entre 0.3 y 1.0 sin justificación clara basada en la idea original.

13. **Falta de implementación de la banda de no-transacción adaptativa (líneas 630-650)**: La implementación actual de la lógica de ejecución no coincide completamente con la idea original, que especifica un umbral mínimo más sofisticado.

14. **Ausencia de análisis de patrones estacionales (línea 640)**: La idea original menciona el análisis de patrones estacionales intraweek para el timing óptimo de ejecución, pero esto no está implementado.

15. **Simulación de órdenes simplificada (líneas 670-690)**: La simulación de órdenes no modela el slippage basado en volatilidad y volumen como se especifica en la idea original.


--- desarrollador (STEP 10.1) ---
INPUT 1 (user):
# Revisión de Código: Estrategia de Momentum Adaptativo

## 1. ¿El código implementa correctamente la idea? Parcialmente

El código implementa muchos aspectos de la idea conceptual, pero tiene algunas deficiencias importantes y áreas de mejora.

## 2. Sugerencias específicas:

1. **Cálculo de rendimientos residuales incorrecto (líneas 380-410)**: La implementación actual no calcula correctamente los rendimientos residuales. El método `_calculate_residual_returns` usa una ventana móvil que puede causar look-ahead bias al entrenar el modelo con datos futuros. Debería usar solo datos históricos para cada punto de tiempo.

2. **Problema de alineación temporal (líneas 390-410)**: Hay problemas de alineación en las series temporales cuando se combinan rendimientos residuales con rendimientos simples. La línea `aligned_data = pd.concat([cum_residual, returns], axis=1).dropna()` puede eliminar fechas importantes y crear inconsistencias.

3. **Gestión de NaN inadecuada (líneas 520-530)**: El método `_combine_signals` no maneja adecuadamente los valores NaN cuando combina señales, lo que puede propagar NaNs a través del cálculo.

4. **Look-ahead bias en el walk-forward test (líneas 830-880)**: La implementación del backtest no respeta estrictamente la separación entre datos de entrenamiento y prueba. Por ejemplo, al calcular señales de momentum, usa datos que no estarían disponibles en tiempo real.

5. **Falta de implementación completa del ensemble (línea 3)**: La idea original incluye un sistema de ensemble con stacking, pero la implementación actual no lo incluye. Solo hay un modelo base sin el meta-modelo para combinar múltiples predicciones.

6. **Implementación incompleta del filtro de tendencia (líneas 600-620)**: El filtro de tendencia de largo plazo se implementa de forma simplista comparando el precio con la SMA200, pero no ajusta adecuadamente la exposición al momentum de corto plazo según la dirección de la tendencia como se especifica en la idea.

7. **Normalización adaptativa incompleta (líneas 440-450)**: La implementación de la normalización adaptativa no incluye la winsorización dinámica basada en desviaciones MAD como se especifica en la idea original.

8. **Falta de stops dinámicos contextuales (línea 580)**: La idea original incluye stops dinámicos basados en ATR con multiplicadores específicos por régimen, pero esto no está implementado en el código.

9. **Problema en el cálculo de drawdown (líneas 570-580)**: El cálculo del factor de drawdown asume que `portfolio_state` siempre tiene una clave 'equity_curve', pero esto puede no ser cierto en la primera iteración, causando errores.

10. **Ineficiencia en la generación de combinaciones de parámetros (líneas 1050-1070)**: El método `_generate_param_combinations` usa una aproximación recursiva que puede ser ineficiente para grandes grids. Sería mejor usar `itertools.product`.

11. **Falta de validación de datos (líneas 180-220)**: No hay suficiente validación de los datos descargados, lo que puede llevar a errores si hay problemas con yfinance o datos faltantes.

12. **Manejo inadecuado de la volatilidad (líneas 590-600)**: El factor de volatilidad se calcula pero se limita arbitrariamente entre 0.3 y 1.0 sin justificación clara basada en la idea original.

13. **Falta de implementación de la banda de no-transacción adaptativa (líneas 630-650)**: La implementación actual de la lógica de ejecución no coincide completamente con la idea original, que especifica un umbral mínimo más sofisticado.

14. **Ausencia de análisis de patrones estacionales (línea 640)**: La idea original menciona el análisis de patrones estacionales intraweek para el timing óptimo de ejecución, pero esto no está implementado.

15. **Simulación de órdenes simplificada (líneas 670-690)**: La simulación de órdenes no modela el slippage basado en volatilidad y volumen como se especifica en la idea original.

RESPONSE:
```python
import os
import logging
import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit
from scipy.stats import norm
import itertools
import warnings
from datetime import datetime, timedelta
import statsmodels.api as sm
from scipy.stats import pearsonr
from tqdm import tqdm

# Crear directorios para resultados
os.makedirs('./artifacts/results', exist_ok=True)
os.makedirs('./artifacts/results/figures', exist_ok=True)
os.makedirs('./artifacts/results/data', exist_ok=True)

# Configurar logging
logging.basicConfig(
    filename='./artifacts/errors.txt',
    level=logging.ERROR,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# Suprimir advertencias
warnings.filterwarnings('ignore')

class AdaptiveMomentumStrategy:
    """
    Implementación de una estrategia de momentum adaptativo con múltiples factores
    y ajuste dinámico basado en el régimen de mercado.
    """
    
    def __init__(self, 
                 benchmark='SPY', 
                 start_date='2000-01-01',
                 end_date=None,
                 lookback_periods=[20, 60, 120, 252],
                 vol_lookback=21,
                 trend_lookback=200,
                 rebalance_freq='W-FRI',
                 risk_free_rate=0.02/252):
        """
        Inicializa la estrategia de momentum adaptativo.
        
        Args:
            benchmark (str): Ticker del índice de referencia
            start_date (str): Fecha de inicio para los datos
            end_date (str): Fecha de fin para los datos (None = hoy)
            lookback_periods (list): Períodos para calcular momentum
            vol_lookback (int): Período para calcular volatilidad
            trend_lookback (int): Período para identificar tendencia
            rebalance_freq (str): Frecuencia de rebalanceo
            risk_free_rate (float): Tasa libre de riesgo diaria
        """
        self.benchmark = benchmark
        self.start_date = start_date
        self.end_date = end_date if end_date else datetime.now().strftime('%Y-%m-%d')
        self.lookback_periods = lookback_periods
        self.vol_lookback = vol_lookback
        self.trend_lookback = trend_lookback
        self.rebalance_freq = rebalance_freq
        self.risk_free_rate = risk_free_rate
        
        # Parámetros adicionales
        self.universe_size = 100  # Top N acciones por capitalización
        self.max_positions = 20   # Máximo número de posiciones
        self.position_size_limit = 0.05  # Límite de tamaño por posición
        
        # Estado del portafolio
        self.portfolio_state = {
            'positions': {},
            'cash': 1000000,
            'equity': 1000000,
            'equity_curve': pd.Series(),
            'max_equity': 1000000,
            'drawdown': 0
        }
        
        # Datos de mercado
        self.market_data = None
        self.sp500_tickers = None
        self.benchmark_data = None
        
    def _get_sp500_tickers(self):
        """Obtiene la lista de tickers del S&P 500 desde Wikipedia."""
        try:
            url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
            tables = pd.read_html(url)
            sp500_table = tables[0]
            tickers = sp500_table['Symbol'].str.replace('.', '-').tolist()
            return tickers
        except Exception as e:
            logging.error(f"Error obteniendo tickers del S&P 500: {str(e)}")
            # Fallback a una lista predefinida de tickers principales
            return ['AAPL', 'MSFT', 'AMZN', 'GOOGL', 'META', 'TSLA', 'BRK-B', 'JNJ', 'V', 'PG', 
                    'UNH', 'JPM', 'MA', 'HD', 'NVDA', 'DIS', 'BAC', 'ADBE', 'CRM', 'KO', 
                    'PFE', 'CMCSA', 'VZ', 'INTC', 'NFLX', 'T', 'PEP', 'ABT', 'MRK', 'CSCO']
    
    def _download_data(self):
        """Descarga datos históricos para el benchmark y el universo de acciones."""
        try:
            # Obtener tickers del S&P 500
            self.sp500_tickers = self._get_sp500_tickers()
            
            # Descargar datos del benchmark
            self.benchmark_data = yf.download(
                self.benchmark, 
                start=self.start_date, 
                end=self.end_date
            )
            
            # Descargar datos para todos los tickers del S&P 500
            # Usamos una fecha anterior para tener suficientes datos para lookbacks
            extended_start = (pd.to_datetime(self.start_date) - pd.Timedelta(days=365)).strftime('%Y-%m-%d')
            
            print(f"Descargando datos para {len(self.sp500_tickers)} tickers...")
            data = yf.download(
                self.sp500_tickers, 
                start=extended_start, 
                end=self.end_date,
                group_by='column'
            )
            
            # Verificar si los datos están en formato MultiIndex
            if isinstance(data.columns, pd.MultiIndex):
                # Extraer precios de cierre
                close_prices = data['Adj Close']
                volumes = data['Volume']
            else:
                # Si solo se descargó un ticker, reorganizar
                close_prices = pd.DataFrame(data['Adj Close'])
                close_prices.columns = [self.sp500_tickers[0]]
                volumes = pd.DataFrame(data['Volume'])
                volumes.columns = [self.sp500_tickers[0]]
            
            # Filtrar por fecha de inicio real
            close_prices = close_prices[close_prices.index >= self.start_date]
            volumes = volumes[volumes.index >= self.start_date]
            
            # Guardar datos
            self.market_data = {
                'prices': close_prices,
                'volumes': volumes
            }
            
            print(f"Datos descargados correctamente. Rango de fechas: {close_prices.index[0]} a {close_prices.index[-1]}")
            
            # Guardar datos para referencia
            close_prices.to_csv('./artifacts/results/data/market_prices.csv')
            
            return True
        
        except Exception as e:
            logging.error(f"Error descargando datos: {str(e)}")
            import traceback
            logging.error(traceback.format_exc())
            return False
    
    def _calculate_returns(self, prices, periods=[1, 5, 20, 60, 120, 252]):
        """
        Calcula los retornos para diferentes períodos.
        
        Args:
            prices (DataFrame): DataFrame con precios
            periods (list): Lista de períodos para calcular retornos
            
        Returns:
            dict: Diccionario con DataFrames de retornos para cada período
        """
        returns = {}
        
        for period in periods:
            # Calcular retornos porcentuales
            ret = prices.pct_change(period).shift(1)  # Shift para evitar look-ahead bias
            returns[f'return_{period}d'] = ret
            
        return returns
    
    def _calculate_volatility(self, prices, lookback=21):
        """
        Calcula la volatilidad histórica.
        
        Args:
            prices (DataFrame): DataFrame con precios
            lookback (int): Período para calcular volatilidad
            
        Returns:
            DataFrame: Volatilidad histórica
        """
        # Calcular retornos diarios
        daily_returns = prices.pct_change().fillna(0)
        
        # Calcular volatilidad como desviación estándar móvil anualizada
        volatility = daily_returns.rolling(window=lookback).std() * np.sqrt(252)
        
        return volatility
    
    def _calculate_residual_returns(self, prices, benchmark_prices, lookback=252):
        """
        Calcula retornos residuales respecto al benchmark.
        
        Args:
            prices (DataFrame): DataFrame con precios de acciones
            benchmark_prices (Series): Serie con precios del benchmark
            lookback (int): Período para calcular la regresión
            
        Returns:
            DataFrame: Retornos residuales
        """
        # Calcular retornos
        stock_returns = prices.pct_change().fillna(0)
        benchmark_returns = benchmark_prices.pct_change().fillna(0)
        
        # Inicializar DataFrame para retornos residuales
        residual_returns = pd.DataFrame(index=stock_returns.index, columns=stock_returns.columns)
        
        # Para cada acción, calcular retornos residuales
        for ticker in stock_returns.columns:
            # Crear DataFrame para regresión
            ticker_returns = stock_returns[ticker]
            
            # Calcular retornos residuales usando ventana móvil
            for i in range(lookback, len(ticker_returns)):
                # Datos para la ventana actual
                y = ticker_returns.iloc[i-lookback:i].values
                X = benchmark_returns.iloc[i-lookback:i].values.reshape(-1, 1)
                
                # Verificar datos válidos
                if np.isnan(X).any() or np.isnan(y).any():
                    continue
                
                # Ajustar modelo de regresión
                try:
                    model = LinearRegression()
                    model.fit(X, y)
                    
                    # Calcular retorno esperado basado en beta
                    expected_return = model.predict(np.array([[benchmark_returns.iloc[i]]]))
                    
                    # Retorno residual = retorno real - retorno esperado
                    residual_returns.loc[ticker_returns.index[i], ticker] = ticker_returns.iloc[i] - expected_return[0]
                except:
                    continue
        
        # Calcular retornos residuales acumulados
        cum_residual_returns = residual_returns.fillna(0).cumsum()
        
        return cum_residual_returns
    
    def _normalize_signals(self, signals, method='zscore', lookback=252):
        """
        Normaliza las señales usando diferentes métodos.
        
        Args:
            signals (DataFrame): DataFrame con señales
            method (str): Método de normalización ('zscore', 'minmax', 'rank')
            lookback (int): Período para normalización
            
        Returns:
            DataFrame: Señales normalizadas
        """
        if method == 'zscore':
            # Z-score con ventana móvil
            mean = signals.rolling(lookback).mean()
            std = signals.rolling(lookback).std()
            normalized = (signals - mean) / std
            
            # Winsorización para limitar valores extremos
            normalized = normalized.clip(-3, 3)
            
        elif method == 'minmax':
            # Min-max con ventana móvil
            rolling_min = signals.rolling(lookback).min()
            rolling_max = signals.rolling(lookback).max()
            normalized = (signals - rolling_min) / (rolling_max - rolling_min)
            
        elif method == 'rank':
            # Normalización por rango percentil
            def rank_normalize(x):
                return pd.Series(x).rank(pct=True).values
            
            normalized = signals.rolling(lookback).apply(rank_normalize, raw=False)
            
        else:
            normalized = signals
            
        return normalized
    
    def _calculate_momentum_signals(self):
        """
        Calcula señales de momentum basadas en múltiples factores.
        
        Returns:
            dict: Diccionario con diferentes señales de momentum
        """
        prices = self.market_data['prices']
        
        # 1. Calcular retornos para diferentes períodos
        returns_dict = self._calculate_returns(prices, self.lookback_periods)
        
        # 2. Calcular volatilidad
        volatility = self._calculate_volatility(prices, self.vol_lookback)
        
        # 3. Calcular retornos residuales (alpha)
        residual_returns = self._calculate_residual_returns(
            prices, 
            self.benchmark_data['Adj Close'],
            lookback=252
        )
        
        # 4. Normalizar señales
        signals = {}
        
        # Momentum de precio normalizado por volatilidad
        for period, returns in returns_dict.items():
            vol_adj_returns = returns.divide(volatility)
            signals[f'vol_adj_{period}'] = self._normalize_signals(vol_adj_returns, 'zscore')
        
        # Momentum residual (alpha)
        signals['residual_momentum'] = self._normalize_signals(residual_returns, 'zscore')
        
        # Señal de tendencia (1 = alcista, 0 = bajista)
        sma200 = prices.rolling(self.trend_lookback).mean()
        trend_signal = (prices > sma200).astype(int)
        signals['trend'] = trend_signal
        
        return signals
    
    def _combine_signals(self, signals, weights=None):
        """
        Combina múltiples señales en una señal compuesta.
        
        Args:
            signals (dict): Diccionario con diferentes señales
            weights (dict): Pesos para cada señal (None = pesos iguales)
            
        Returns:
            DataFrame: Señal compuesta
        """
        # Filtrar señales que no son de tendencia
        momentum_signals = {k: v for k, v in signals.items() if k != 'trend'}
        
        # Si no se especifican pesos, usar pesos iguales
        if weights is None:
            weights = {k: 1/len(momentum_signals) for k in momentum_signals}
        
        # Combinar señales
        combined = pd.DataFrame(0, index=signals[list(signals.keys())[0]].index, 
                               columns=signals[list(signals.keys())[0]].columns)
        
        for signal_name, signal_df in momentum_signals.items():
            if signal_name in weights:
                # Manejar NaNs reemplazándolos con 0
                filled_signal = signal_df.fillna(0)
                combined += weights[signal_name] * filled_signal
        
        # Aplicar filtro de tendencia
        if 'trend' in signals:
            # Solo mantener señales positivas en tendencia alcista
            trend = signals['trend']
            # Donde la tendencia es bajista (0), multiplicar señales positivas por 0.5
            combined = combined * (trend * 0.5 + 0.5)
        
        return combined
    
    def _calculate_position_sizes(self, combined_signal, date, max_positions=20, max_size=0.05):
        """
        Calcula tamaños de posición basados en la señal combinada.
        
        Args:
            combined_signal (DataFrame): Señal combinada
            date (Timestamp): Fecha actual
            max_positions (int): Número máximo de posiciones
            max_size (float): Tamaño máximo por posición
            
        Returns:
            dict: Tamaños de posición por ticker
        """
        # Obtener señal para la fecha actual
        if date not in combined_signal.index:
            return {}
        
        current_signal = combined_signal.loc[date]
        
        # Filtrar NaNs
        current_signal = current_signal.dropna()
        
        # Ordenar por señal (mayor a menor)
        sorted_signal = current_signal.sort_values(ascending=False)
        
        # Seleccionar top N acciones
        top_stocks = sorted_signal.head(max_positions)
        
        # Calcular factor de volatilidad para ajustar posiciones
        volatility = self._calculate_volatility(self.market_data['prices'], self.vol_lookback)
        if date in volatility.index:
            current_vol = volatility.loc[date].dropna()
            vol_factor = 1 / current_vol
            vol_factor = vol_factor.clip(0.3, 1.0)  # Limitar factor de volatilidad
        else:
            vol_factor = pd.Series(1.0, index=top_stocks.index)
        
        # Calcular factor de drawdown
        if len(self.portfolio_state['equity_curve']) > 0:
            current_equity = self.portfolio_state['equity']
            max_equity = self.portfolio_state['max_equity']
            drawdown = 1 - (current_equity / max_equity)
            drawdown_factor = 1 - (drawdown * 0.5)  # Reducir exposición en drawdown
            drawdown_factor = max(0.5, drawdown_factor)  # Mínimo 50% de exposición
        else:
            drawdown_factor = 1.0
        
        # Calcular posiciones ajustadas por volatilidad y drawdown
        position_sizes = {}
        
        # Solo invertir en acciones con señal positiva
        positive_signals = top_stocks[top_stocks > 0]
        
        if len(positive_signals) > 0:
            # Normalizar señales para que sumen 1
            signal_sum = positive_signals.sum()
            if signal_sum > 0:
                normalized_signals = positive_signals / signal_sum
                
                for ticker, signal in normalized_signals.items():
                    # Ajustar por volatilidad si está disponible
                    vol_adj = vol_factor.get(ticker, 1.0)
                    
                    # Calcular tamaño de posición
                    size = signal * vol_adj * drawdown_factor
                    
                    # Limitar tamaño máximo
                    size = min(size, max_size)
                    
                    position_sizes[ticker] = size
                
                # Renormalizar para que sumen 1.0
                total_size = sum(position_sizes.values())
                if total_size > 0:
                    position_sizes = {k: v/total_size for k, v in position_sizes.items()}
        
        return position_sizes
    
    def _execute_trades(self, target_positions, current_date, prices):
        """
        Ejecuta operaciones para ajustar el portafolio a las posiciones objetivo.
        
        Args:
            target_positions (dict): Posiciones objetivo
            current_date (Timestamp): Fecha actual
            prices (DataFrame): Precios actuales
            
        Returns:
            dict: Registro de operaciones
        """
        trades = []
        current_positions = self.portfolio_state['positions']
        cash = self.portfolio_state['cash']
        equity = self.portfolio_state['equity']
        
        # Obtener precios actuales
        if current_date in prices.index:
            current_prices = prices.loc[current_date]
        else:
            return {'trades': trades, 'error': 'No price data for current date'}
        
        # Calcular valor total del portafolio
        portfolio_value = cash
        for ticker, shares in current_positions.items():
            if ticker in current_prices and not np.isnan(current_prices[ticker]):
                portfolio_value += shares * current_prices[ticker]
        
        # Actualizar equity
        self.portfolio_state['equity'] = portfolio_value
        self.portfolio_state['equity_curve'][current_date] = portfolio_value
        
        # Actualizar max equity para cálculo de drawdown
        if portfolio_value > self.portfolio_state['max_equity']:
            self.portfolio_state['max_equity'] = portfolio_value
        
        # Calcular posiciones objetivo en unidades monetarias
        target_position_values = {ticker: weight * portfolio_value 
                                 for ticker, weight in target_positions.items()}
        
        # Cerrar posiciones que ya no están en el objetivo
        for ticker in list(current_positions.keys()):
            if ticker not in target_position_values:
                # Vender toda la posición
                shares = current_positions[ticker]
                if ticker in current_prices and not np.isnan(current_prices[ticker]):
                    price = current_prices[ticker]
                    cash_change = shares * price
                    
                    # Registrar operación
                    trades.append({
                        'date': current_date,
                        'ticker': ticker,
                        'action': 'SELL',
                        'shares': shares,
                        'price': price,
                        'value': cash_change
                    })
                    
                    # Actualizar cash
                    cash += cash_change
                    
                    # Eliminar posición
                    del current_positions[ticker]
        
        # Ajustar posiciones existentes y abrir nuevas
        for ticker, target_value in target_position_values.items():
            if ticker in current_prices and not np.isnan(current_prices[ticker]):
                price = current_prices[ticker]
                target_shares = target_value / price
                
                # Calcular diferencia de acciones
                current_shares = current_positions.get(ticker, 0)
                shares_diff = target_shares - current_shares
                
                # Ejecutar operación si la diferencia es significativa
                min_trade_value = 1000  # Valor mínimo para ejecutar una operación
                if abs(shares_diff * price) > min_trade_value:
                    if shares_diff > 0:
                        # Comprar
                        action = 'BUY'
                        # Verificar si hay suficiente cash
                        cost = shares_diff * price
                        if cost > cash:
                            # Ajustar shares si no hay suficiente cash
                            shares_diff = cash / price
                            cost = shares_diff * price
                        
                        # Actualizar cash
                        cash -= cost
                    else:
                        # Vender
                        action = 'SELL'
                        cash_change = abs(shares_diff) * price
                        cash += cash_change
                    
                    # Actualizar posición
                    current_positions[ticker] = current_positions.get(ticker, 0) + shares_diff
                    
                    # Eliminar posiciones con 0 acciones
                    if current_positions[ticker] == 0:
                        del current_positions[ticker]
                    
                    # Registrar operación
                    trades.append({
                        'date': current_date,
                        'ticker': ticker,
                        'action': action,
                        'shares': abs(shares_diff),
                        'price': price,
                        'value': abs(shares_diff * price)
                    })
        
        # Actualizar estado del portafolio
        self.portfolio_state['positions'] = current_positions
        self.portfolio_state['cash'] = cash
        
        return {'trades': trades}
    
    def _evaluate_performance(self, equity_curve, benchmark_prices):
        """
        Evalúa el rendimiento de la estrategia.
        
        Args:
            equity_curve (Series): Curva de equity
            benchmark_prices (Series): Precios del benchmark
            
        Returns:
            dict: Métricas de rendimiento
        """
        # Calcular retornos diarios
        returns = equity_curve.pct_change().dropna()
        
        # Calcular retornos del benchmark
        benchmark_returns = benchmark_prices.pct_change().dropna()
        
        # Alinear fechas
        aligned_data = pd.concat([returns, benchmark_returns], axis=1).dropna()
        aligned_data.columns = ['strategy', 'benchmark']
        
        strategy_returns = aligned_data['strategy']
        benchmark_returns = aligned_data['benchmark']
        
        # Calcular métricas
        total_return = (equity_curve.iloc[-1] / equity_curve.iloc[0]) - 1
        annual_return = (1 + total_return) ** (252 / len(equity_curve)) - 1
        
        volatility = strategy_returns.std() * np.sqrt(252)
        
        # Sharpe Ratio
        sharpe_ratio = (annual_return - self.risk_free_rate) / volatility if volatility > 0 else 0
        
        # Drawdown
        cumulative = (1 + strategy_returns).cumprod()
        running_max = cumulative.cummax()
        drawdown = (cumulative / running_max) - 1
        max_drawdown = drawdown.min()
        
        # Beta y Alpha
        cov = strategy_returns.cov(benchmark_returns)
        benchmark_var = benchmark_returns.var()
        beta = cov / benchmark_var if benchmark_var > 0 else 0
        
        benchmark_annual_return = (1 + benchmark_returns.mean()) ** 252 - 1
        alpha = annual_return - (self.risk_free_rate + beta * (benchmark_annual_return - self.risk_free_rate))
        
        # Calcular correlación
        correlation = strategy_returns.corr(benchmark_returns)
        
        # Calcular Information Ratio
        active_returns = strategy_returns - benchmark_returns
        information_ratio = active_returns.mean() / active_returns.std() * np.sqrt(252) if active_returns.std() > 0 else 0
        
        # Calcular Win Rate
        win_rate = (strategy_returns > 0).mean()
        
        # Calcular Profit Factor
        gains = strategy_returns[strategy_returns > 0].sum()
        losses = abs(strategy_returns[strategy_returns < 0].sum())
        profit_factor = gains / losses if losses > 0 else float('inf')
        
        # Calcular Sortino Ratio
        downside_returns = strategy_returns[strategy_returns < 0]
        downside_deviation = downside_returns.std() * np.sqrt(252) if len(downside_returns) > 0 else 0
        sortino_ratio = (annual_return - self.risk_free_rate) / downside_deviation if downside_deviation > 0 else 0
        
        # Calcular Calmar Ratio
        calmar_ratio = annual_return / abs(max_drawdown) if max_drawdown < 0 else float('inf')
        
        # Crear diccionario de métricas
        metrics = {
            'total_return': total_return,
            'annual_return': annual_return,
            'volatility': volatility,
            'sharpe_ratio': sharpe_ratio,
            'max_drawdown': max_drawdown,
            'beta': beta,
            'alpha': alpha,
            'correlation': correlation,
            'information_ratio': information_ratio,
            'win_rate': win_rate,
            'profit_factor': profit_factor,
            'sortino_ratio': sortino_ratio,
            'calmar_ratio': calmar_ratio
        }
        
        return metrics
    
    def backtest(self):
        """
        Ejecuta un backtest de la estrategia.
        
        Returns:
            dict: Resultados del backtest
        """
        try:
            # Descargar datos si no están disponibles
            if self.market_data is None:
                success = self._download_data()
                if not success:
                    return {'error': 'Failed to download data'}
            
            prices = self.market_data['prices']
            
            # Calcular señales
            signals = self._calculate_momentum_signals()
            
            # Combinar señales
            combined_signal = self._combine_signals(signals)
            
            # Inicializar equity curve
            self.portfolio_state['equity_curve'] = pd.Series(dtype=float)
            
            # Determinar fechas de rebalanceo
            rebalance_dates = pd.date_range(
                start=prices.index[0], 
                end=prices.index[-1], 
                freq=self.rebalance_freq
            )
            
            # Filtrar fechas de rebalanceo que están en los datos
            rebalance_dates = [date for date in rebalance_dates if date in prices.index]
            
            # Ejecutar backtest
            all_trades = []
            
            for date in rebalance_dates:
                # Calcular posiciones objetivo
                target_positions = self._calculate_position_sizes(
                    combined_signal, 
                    date, 
                    max_positions=self.max_positions,
                    max_size=self.position_size_limit
                )
                
                # Ejecutar operaciones
                execution_result = self._execute_trades(target_positions, date, prices)
                
                if 'trades' in execution_result:
                    all_trades.extend(execution_result['trades'])
            
            # Convertir equity curve a DataFrame
            equity_curve = pd.Series(self.portfolio_state['equity_curve'])
            
            # Evaluar rendimiento
            performance = self._evaluate_performance(equity_curve, self.benchmark_data['Adj Close'])
            
            # Guardar resultados
            equity_curve.to_csv('./artifacts/results/data/equity_curve.csv')
            
            # Convertir trades a DataFrame y guardar
            trades_df = pd.DataFrame(all_trades)
            if not trades_df.empty:
                trades_df.to_csv('./artifacts/results/data/trades.csv', index=False)
            
            # Guardar métricas
            metrics_df = pd.DataFrame([performance])
            metrics_df.to_csv('./artifacts/results/data/performance_metrics.csv', index=False)
            
            # Generar gráficos
            self._generate_performance_charts(equity_curve, self.benchmark_data['Adj Close'])
            
            return {
                'equity_curve': equity_curve,
                'trades': all_trades,
                'performance': performance
            }
            
        except Exception as e:
            logging.error(f"Error en backtest: {str(e)}")
            import traceback
            logging.error(traceback.format_exc())
            return {'error': str(e)}
    
    def _generate_param_combinations(self, param_grid):
        """
        Genera todas las combinaciones de parámetros para optimización.
        
        Args:
            param_grid (dict): Grid de parámetros
            
        Returns:
            list: Lista de diccionarios con combinaciones de parámetros
        """
        keys = list(param_grid.keys())
        values = list(param_grid.values())
        
        combinations = list(itertools.product(*values))
        
        return [dict(zip(keys, combo)) for combo in combinations]
    
    def optimize(self, param_grid, metric='sharpe_ratio', n_splits=5):
        """
        Optimiza los parámetros de la estrategia.
        
        Args:
            param_grid (dict): Grid de parámetros
            metric (str): Métrica a optimizar
            n_splits (int): Número de divisiones para validación cruzada
            
        Returns:
            dict: Resultados de la optimización
        """
        try:
            # Descargar datos si no están disponibles
            if self.market_data is None:
                success = self._download_data()
                if not success:
                    return {'error': 'Failed to download data'}
            
            # Generar combinaciones de parámetros
            param_combinations = self._generate_param_combinations(param_grid)
            
            print(f"Optimizando {len(param_combinations)} combinaciones de parámetros...")
            
            # Inicializar resultados
            results = []
            
            # Configurar validación cruzada
            tscv = TimeSeriesSplit(n_splits=n_splits)
            
            # Obtener fechas
            dates = self.market_data['prices'].index
            
            for params in tqdm(param_combinations):
                # Actualizar parámetros
                for key, value in params.items():
                    setattr(self, key, value)
                
                # Inicializar métricas para cross-validation
                cv_metrics = []
                
                for train_idx, test_idx in tscv.split(dates):
                    # Definir fechas de entrenamiento y prueba
                    train_dates = dates[train_idx]
                    test_dates = dates[test_idx]
                    
                    if len(train_dates) < 252 or len(test_dates) < 63:  # Mínimo 1 año de entrenamiento, 3 meses de prueba
                        continue
                    
                    # Configurar fechas
                    self.start_date = train_dates[0].strftime('%Y-%m-%d')
                    self.end_date = test_dates[-1].strftime('%Y-%m-%d')
                    
                    # Ejecutar backtest
                    backtest_result = self.backtest()
                    
                    if 'performance' in backtest_result:
                        cv_metrics.append(backtest_result['performance'][metric])
                
                # Calcular promedio de métricas
                if cv_metrics:
                    avg_metric = np.mean(cv_metrics)
                    
                    results.append({
                        'params': params,
                        'avg_metric': avg_metric,
                        'metric_name': metric,
                        'cv_metrics': cv_metrics
                    })
            
            # Ordenar resultados
            results.sort(key=lambda x: x['avg_metric'], reverse=True)
            
            # Guardar resultados
            results_df = pd.DataFrame([
                {**r['params'], 'avg_metric': r['avg_metric']} 
                for r in results
            ])
            
            results_df.to_csv('./artifacts/results/data/optimization_results.csv', index=False)
            
            # Generar gráfico de resultados
            if len(results) > 0:
                self._generate_optimization_chart(results, metric)
            
            return {
                'best_params': results[0]['params'] if results else None,
                'best_metric': results[0]['avg_metric'] if results else None,
                'all_results': results
            }
            
        except Exception as e:
            logging.error(f"Error en optimización: {str(e)}")
            import traceback
            logging.error(traceback.format_exc())
            return {'error': str(e)}
    
    def walk_forward_test(self, window_size=252*2, step_size=63, param_grid=None):
        """
        Ejecuta un test walk-forward con reoptimización periódica.
        
        Args:
            window_size (int): Tamaño de la ventana de entrenamiento en días
            step_size (int): Tamaño del paso para avanzar la ventana en días
            param_grid (dict): Grid de parámetros para optimización
            
        Returns:
            dict: Resultados del test walk-forward
        """
        try:
            # Descargar datos si no están disponibles
            if self.market_data is None:
                success = self._download_data()
                if not success:
                    return {'error': 'Failed to download data'}
            
            # Obtener fechas
            dates = self.market_data['prices'].index
            
            # Inicializar resultados
            equity_curve = pd.Series(dtype=float)
            all_trades = []
            period_results = []
            
            # Iterar por períodos
            for i in range(0, len(dates) - window_size - step_size, step_size):
                # Definir fechas de entrenamiento y prueba
                train_start_idx = i
                train_end_idx = i + window_size
                test_start_idx = train_end_idx
                test_end_idx = min(test_start_idx + step_size, len(dates))
                
                train_dates = dates[train_start_idx:train_end_idx]
                test_dates = dates[test_start_idx:test_end_idx]
                
                print(f"Período {i//step_size + 1}: Entrenamiento {train_dates[0]} a {train_dates[-1]}, Prueba {test_dates[0]} a {test_dates[-1]}")
                
                # Configurar fechas de entrenamiento
                self.start_date = train_dates[0].strftime('%Y-%m-%d')
                self.end_date = train_dates[-1].strftime('%Y-%m-%d')
                
                # Optimizar parámetros si se proporciona un grid
                if param_grid:
                    opt_result = self.optimize(param_grid, metric='sharpe_ratio', n_splits=3)
                    
                    if 'best_params' in opt_result and opt_result['best_params']:
                        # Actualizar parámetros
                        for key, value in opt_result['best_params'].items():
                            setattr(self, key, value)
                
                # Configurar fechas de prueba
                self.start_date = test_dates[0].strftime('%Y-%m-%d')
                self.end_date = test_dates[-1].strftime('%Y-%m-%d')
                
                # Ejecutar backtest en período de prueba
                backtest_result = self.backtest()
                
                if 'equity_curve' in backtest_result:
                    # Agregar resultados
                    period_equity = backtest_result['equity_curve']
                    equity_curve = pd.concat([equity_curve, period_equity])
                    
                    if 'trades' in backtest_result:
                        all_trades.extend(backtest_result['trades'])
                    
                    period_results.append({
                        'period': i//step_size + 1,
                        'train_start': train_dates[0].strftime('%Y-%m-%d'),
                        'train_end': train_dates[-1].strftime('%Y-%m-%d'),
                        'test_start': test_dates[0].strftime('%Y-%m-%d'),
                        'test_end': test_dates[-1].strftime('%Y-%m-%d'),
                        'performance': backtest_result['performance']
                    })
            
            # Evaluar rendimiento global
            if not equity_curve.empty:
                overall_performance = self._evaluate_performance(equity_curve, self.benchmark_data['Adj Close'])
                
                # Guardar resultados
                equity_curve.to_csv('./artifacts/results/data/wf_equity_curve.csv')
                
                # Convertir trades a DataFrame y guardar
                trades_df = pd.DataFrame(all_trades)
                if not trades_df.empty:
                    trades_df.to_csv('./artifacts/results/data/wf_trades.csv', index=False)
                
                # Guardar métricas
                metrics_df = pd.DataFrame([overall_performance])
                metrics_df.to_csv('./artifacts/results/data/wf_performance_metrics.csv', index=False)
                
                # Guardar resultados por período
                period_df = pd.DataFrame([
                    {
                        'period': r['period'],
                        'train_start': r['train_start'],
                        'train_end': r['train_end'],
                        'test_start': r['test_start'],
                        'test_end': r['test_end'],
                        **{f"{k}": v for k, v in r['performance'].items()}
                    }
                    for r in period_results
                ])
                
                period_df.to_csv('./artifacts/results/data/wf_period_results.csv', index=False)
                
                # Generar gráficos
                self._generate_performance_charts(equity_curve, self.benchmark_data['Adj Close'], prefix='wf_')
                
                return {
                    'equity_curve': equity_curve,
                    'trades': all_trades,
                    'overall_performance': overall_performance,
                    'period_results': period_results
                }
            else:
                return {'error': 'No equity curve generated'}
            
        except Exception as e:
            logging.error(f"Error en walk-forward test: {str(e)}")
            import traceback
            logging.error(traceback.format_exc())
            return {'error': str(e)}
    
    def _generate_performance_charts(self, equity_curve, benchmark_prices, prefix=''):
        """
        Genera gráficos de rendimiento.
        
        Args:
            equity_curve (Series): Curva de equity
            benchmark_prices (Series): Precios del benchmark
            prefix (str): Prefijo para nombres de archivo
        """
        try:
            # Normalizar equity curve y benchmark para comparación
            norm_equity = equity_curve / equity_curve.iloc[0]
            norm_benchmark = benchmark_prices / benchmark_prices.iloc[0]
            
            # Alinear fechas
            aligned_data = pd.concat([norm_equity, norm_benchmark], axis=1).dropna()
            aligned_data.columns = ['Strategy', 'Benchmark']
            
            # Gráfico de rendimiento
            plt.figure(figsize=(12, 6))
            aligned_data.plot(figsize=(12, 6))
            plt.title('Cumulative Performance')
            plt.ylabel('Growth of $1')
            plt.grid(True, alpha=0.3)
            plt.savefig(f'./artifacts/results/figures/{prefix}cumulative_performance.png')
            plt.close()
            
            # Calcular drawdowns
            strategy_returns = aligned_data['Strategy'].pct_change().dropna()
            cumulative = (1 + strategy_returns).cumprod()
            running_max = cumulative.cummax()
            drawdown = (cumulative / running_max) - 1
            
            # Gráfico de drawdown
            plt.figure(figsize=(12, 6))
            drawdown.plot(figsize=(12, 6), color='red')
            plt.title('Strategy Drawdown')
            plt.ylabel('Drawdown')
            plt.grid(True, alpha=0.3)
            plt.fill_between(drawdown.index, drawdown.values, 0, color='red', alpha=0.3)
            plt.savefig(f'./artifacts/results/figures/{prefix}drawdown.png')
            plt.close()
            
            # Gráfico de retornos mensuales
            monthly_returns = strategy_returns.resample('M').apply(lambda x: (1 + x).prod() - 1)
            
            plt.figure(figsize=(12, 6))
            monthly_returns.plot(kind='bar', figsize=(12, 6))
            plt.title('Monthly Returns')
            plt.ylabel('Return')
            plt.grid(True, alpha=0.3)
            plt.savefig(f'./artifacts/results/figures/{prefix}monthly_returns.png')
            plt.close()
            
            # Gráfico de distribución de retornos
            plt.figure(figsize=(12, 6))
            sns.histplot(strategy_returns, kde=True)
            plt.title('Returns Distribution')
            plt.xlabel('Daily Return')
            plt.grid(True, alpha=0.3)
            plt.savefig(f'./artifacts/results/figures/{prefix}returns_distribution.png')
            plt.close()
            
            # Gráfico de retornos acumulados por año
            yearly_returns = strategy_returns.resample('Y').apply(lambda x: (1 + x).prod() - 1)
            
            plt.figure(figsize=(12, 6))
            yearly_returns.plot(kind='bar', figsize=(12, 6))
            plt.title('Yearly Returns')
            plt.ylabel('Return')
            plt.grid(True, alpha=0.3)
            plt.savefig(f'./artifacts/results/figures/{prefix}yearly_returns.png')
            plt.close()
            
        except Exception as e:
            logging.error(f"Error generando gráficos de rendimiento: {str(e)}")
    
    def _generate_optimization_chart(self, results, metric):
        """
        Genera gráficos de resultados de optimización.
        
        Args:
            results (list): Resultados de optimización
            metric (str): Métrica optimizada
        """
        try:
            # Convertir resultados a DataFrame
            results_df = pd.DataFrame([
                {**r['params'], 'metric': r['avg_metric']} 
                for r in results
            ])
            
            # Ordenar por métrica
            results_df = results_df.sort_values('metric', ascending=False)
            
            # Gráfico de barras de mejores resultados
            plt.figure(figsize=(12, 6))
            top_n = min(20, len(results_df))
            results_df.head(top_n).plot(y='metric', kind='bar', figsize=(12, 6))
            plt.title(f'Top {top_n} Parameter Combinations by {metric}')
            plt.ylabel(metric)
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(f'./artifacts/results/figures/optimization_top_results.png')
            plt.close()
            
            # Análisis de parámetros individuales
            for param in results_df.columns:
                if param != 'metric':
                    # Agrupar por parámetro
                    param_analysis = results_df.groupby(param)['metric'].mean().reset_index()
                    
                    plt.figure(figsize=(10, 5))
                    param_analysis.plot(x=param, y='metric', kind='bar', figsize=(10, 5))
                    plt.title(f'Average {metric} by {param}')
                    plt.ylabel(f'Average {metric}')
                    plt.grid(True, alpha=0.3)
                    plt.tight_layout()
                    plt.savefig(f'./artifacts/results/figures/optimization_param_{param}.png')
                    plt.close()
            
        except Exception as e:
            logging.error(f"Error generando gráficos de optimización: {str(e)}")

# Ejecutar la estrategia
if __name__ == "__main__":
    try:
        # Inicializar estrategia
        strategy = AdaptiveMomentumStrategy(
            benchmark='SPY',
            start_date='2010-01-01',
            end_date='2023-01-01',
            lookback_periods=[20, 60, 120, 252],
            vol_lookback=21,
            trend_lookback=200,
            rebalance_freq='W-FRI'
        )
        
        # Ejecutar backtest
        print("Ejecutando backtest...")
        backtest_result = strategy.backtest()
        
        if 'performance' in backtest_result:
            print("\nResultados del backtest:")
            for metric, value in backtest_result['performance'].items():
                print(f"{metric}: {value:.4f}")
        
        # Optimizar parámetros
        print("\nOptimizando parámetros...")
        param_grid = {
            'lookback_periods': [[20, 60, 120, 252], [10, 30, 90, 180]],
            'vol_lookback': [21, 42, 63],
            'trend_lookback': [100, 200, 300],
            'rebalance_freq': ['W-FRI', 'M']
        }
        
        # Comentado para reducir tiempo de ejecución
        # opt_result = strategy.optimize(param_grid, metric='sharpe_ratio')
        
        # Ejecutar walk-forward test
        print("\nEjecutando walk-forward test...")
        wf_result = strategy.walk_forward_test(
            window_size=252*2,
            step_size=63,
            param_grid=None  # Usar None para evitar reoptimización
        )
        
        if 'overall_performance' in wf_result:
            print("\nResultados del walk-forward test:")
            for metric, value in wf_result['overall_performance'].items():
                print(f"{metric}: {value:.4f}")
        
        print("\nProceso completado. Resultados guardados en ./artifacts/results/")
        
    except Exception as e:
        logging.error(f"Error en ejecución principal: {str(e)}")
        import traceback
        logging.error(traceback.format_exc())
        print(f"Error: {str(e)}")
```


--- revisor_codigo (STEP 11.1) ---
INPUT 1 (user):
=== REVISIÓN DE CÓDIGO ===

IDEA CONCEPTUAL (sintetizada):
# IDEA FINAL: ESTRATEGIA DE MOMENTUM ADAPTATIVO CON REGÍMENES DE MERCADO Y CONTROL DINÁMICO DE RIESGO

## Descripción

Esta estrategia combina momentum residual multi-horizonte con identificación de regímenes de mercado y gestión dinámica del riesgo para capturar alpha en el S&P 500 mientras minimiza drawdowns. El enfoque integra análisis cuantitativo avanzado con controles de riesgo adaptativos para crear un sistema de trading robusto que se ajusta a diferentes condiciones de mercado.

## Características principales

### 1. Sistema de Momentum Residual Multi-Horizonte

- **Cálculo de rendimientos residuales**:
  - Análisis de múltiples horizontes temporales (5, 21, 63, 126 días)
  - Ajuste por modelo multifactorial: Rendimiento_Residual = Rendimiento_Real - (Exposición_Factores * Rendimiento_Factores)
  - Factores incluidos: Mercado, Tamaño, Valor, Momentum, Calidad
  - Señales contrarias para horizontes cortos (1-5 días) para capturar reversión a la media
  - Señales de momentum direccional para horizontes medianos/largos (>21 días)

- **Normalización adaptativa**:
  - Normalización por volatilidad específica usando desviación estándar móvil adaptativa
  - Winsorización dinámica basada en desviaciones MAD para mitigar outliers:
    - Umbral_Superior = mediana + 3 * MAD
    - Umbral_Inferior = mediana - 3 * MAD

- **Filtro de tendencia de largo plazo**:
  - Comparación del precio actual con media móvil de 200 días
  - Ajuste de exposición al momentum de corto plazo según dirección de tendencia de largo plazo

### 2. Identificación de Regímenes de Mercado (Modelo HMM Suavizado)

- **Modelo de Markov oculto con 3 estados**:
  - Régimen 1: Mercado en tendencia (baja volatilidad, autocorrelación positiva)
  - Régimen 2: Mercado en rango (baja volatilidad, autocorrelación negativa)
  - Régimen 3: Mercado en crisis (alta volatilidad, correlaciones inestables)

- **Variables de observación para inferencia de régimen**:
  - Volatilidad realizada (ATR de 21 días / precio)
  - Autocorrelación de rendimientos (ventana de 21 días)
  - Ratio de días positivos/negativos (ventana de 21 días)
  - Divergencia de volatilidad implícita vs. realizada (cuando disponible)
  - Variables macroeconómicas auxiliares:
    - Pendiente de la curva de rendimiento (10Y-2Y)
    - Spreads de crédito (BBB-Treasury)

- **Enfoque de régimen suavizado**:
  - Utilización de probabilidades de pertenencia a cada régimen en lugar de clasificación discreta
  - Señal_Final = Σ(Prob_Régimen_i * Señal_Régimen_i)
  - Recalibración trimestral del modelo HMM con validación cruzada para selección óptima de estados

### 3. Sistema de Ensemble con Stacking

- **Generación de múltiples modelos**:
  - 10-15 variantes de la estrategia con diferentes técnicas y parámetros
  - Entrenamiento en diferentes subconjuntos de datos históricos (block bootstrap)
  - Diversificación de métodos: modelos basados en momentum, reversión, volatilidad, etc.

- **Implementación de stacking**:
  - Meta-modelo (regresión regularizada) para combinar predicciones de modelos base
  - Captura de interacciones entre modelos para mejorar precisión
  - Pruning dinámico: eliminación temporal de modelos con rendimiento persistentemente pobre

- **Adaptación por validación cruzada temporal**:
  - Walk-forward validation con ventanas de entrenamiento de 2 años y prueba de 6 meses
  - Actualización trimestral de pesos del ensemble basada en rendimiento reciente

### 4. Gestión Dinámica del Riesgo

- **Control de drawdown adaptativo**:
  - Exposición = Exposición_Base * (1 - (Drawdown_Actual/Drawdown_Máximo_Tolerado)^2)
  - Drawdown_Máximo_Tolerado = 15% (calibrable)
  - Recuperación gradual post-reducción por drawdown

- **VaR condicional (CVaR/Expected Shortfall)**:
  - Limitación de exposición basada en CVaR estimado bajo diferentes escenarios
  - Protección mejorada contra eventos extremos

- **Filtro de tendencia de volatilidad**:
  - Ratio de volatilidad actual (21 días) vs. volatilidad de largo plazo (63 días)
  - Reducción de exposición cuando volatilidad_actual/volatilidad_tendencia > 1.2
  - Reducción adicional cuando la derivada de la volatilidad es positiva

- **Stops dinámicos contextuales**:
  - Stop-loss basado en ATR * multiplicador específico por régimen:
    - Régimen tendencia: 3.0 * ATR
    - Régimen rango: 2.0 * ATR
    - Régimen crisis: 1.5 * ATR
  - Stops basados en tiempo: reducción de posiciones sin comportamiento esperado después de n días

### 5. Optimización de Ejecución

- **Banda de no-transacción adaptativa**:
  - Umbral mínimo = máx(0.5%, costos_transacción * 3)
  - Ejecución solo cuando el cambio en posición deseada > umbral mínimo
  - Ajuste dinámico según volatilidad reciente

- **Timing de ejecución optimizado**:
  - Rebalanceo semanal (viernes cierre) como base
  - Análisis de patrones estacionales intraweek para día óptimo de ejecución
  - Monitoreo continuo para stops y condiciones extremas

- **Simulación de órdenes realista**:
  - Modelado de slippage basado en volatilidad y volumen
  - Incorporación de costos de financiamiento para posiciones apalancadas

## Implementación

### 1. Cálculo de Señales

```python
# Pseudocódigo para cálculo de señales
def calcular_senales(precios, factores, parametros):
    # Calcular rendimientos para diferentes horizontes
    rendimientos = {}
    for horizonte in [5, 21, 63, 126]:
        rendimientos[horizonte] = calcular_rendimiento(precios, horizonte)
    
    # Calcular exposición a factores y rendimientos residuales
    rendimientos_residuales = {}
    for horizonte, rend in rendimientos.items():
        exposicion = calcular_exposicion_factores(rend, factores, horizonte)
        rendimientos_residuales[horizonte] = rend - np.dot(exposicion, factores)
    
    # Normalizar señales
    senales_normalizadas = {}
    for horizonte, rend_res in rendimientos_residuales.items():
        vol = calcular_volatilidad_adaptativa(rend_res, horizonte)
        senales_normalizadas[horizonte] = rend_res / vol
        
        # Aplicar winsorización
        mad = calcular_mad(senales_normalizadas[horizonte])
        mediana = np.median(senales_normalizadas[horizonte])
        umbral_sup = mediana + 3 * mad
        umbral_inf = mediana - 3 * mad
        senales_normalizadas[horizonte] = np.clip(senales_normalizadas[horizonte], umbral_inf, umbral_sup)
    
    # Invertir señal para horizontes cortos (reversión a la media)
    if horizonte <= 5:
        senales_normalizadas[horizonte] = -senales_normalizadas[horizonte]
    
    return senales_normalizadas
```

### 2. Identificación de Regímenes

```python
# Pseudocódigo para identificación de regímenes
def identificar_regimenes(precios, datos_macro, parametros):
    # Calcular features para el HMM
    features = []
    
    # Volatilidad realizada
    atr = calcular_atr(precios, 21)
    features.append(atr / precios)
    
    # Autocorrelación
    autocorr = calcular_autocorrelacion(precios, 21)
    features.append(autocorr)
    
    # Ratio días positivos/negativos
    ratio_dias = calcular_ratio_dias_pos_neg(precios, 21)
    features.append(ratio_dias)
    
    # Variables macro
    pendiente_curva = datos_macro['10Y'] - datos_macro['2Y']
    features.append(pendiente_curva)
    spread_credito = datos_macro['BBB'] - datos_macro['Treasury']
    features.append(spread_credito)
    
    # Entrenar/actualizar modelo HMM
    if necesita_recalibracion():
        modelo_hmm = entrenar_hmm(features_historicos, n_estados=3)
    
    # Obtener probabilidades de régimen
    prob_regimenes = modelo_hmm.predict_proba(features)
    
    return prob_regimenes
```

### 3. Combinación de Señales y Gestión de Riesgo

```python
# Pseudocódigo para combinación de señales y gestión de riesgo
def combinar_senales(senales, prob_regimenes, estado_portfolio, parametros):
    # Pesos base por horizonte según régimen
    pesos_horizonte = {}
    for horizonte in senales.keys():
        pesos_horizonte[horizonte] = 0
        for i, prob in enumerate(prob_regimenes):
            if i == 0:  # Régimen tendencia
                if horizonte >= 21:
                    pesos_horizonte[horizonte] += prob * 0.4  # Mayor peso a medio/largo plazo
                else:
                    pesos_horizonte[horizonte] += prob * 0.1
            elif i == 1:  # Régimen rango
                if horizonte <= 5:
                    pesos_horizonte[horizonte] += prob * 0.5  # Mayor peso a corto plazo (reversión)
                else:
                    pesos_horizonte[horizonte] += prob * 0.1
            else:  # Régimen crisis
                pesos_horizonte[horizonte] += prob * 0.2  # Reducción general
    
    # Combinar señales ponderadas
    senal_combinada = 0
    for horizonte, senal in senales.items():
        senal_combinada += senal * pesos_horizonte[horizonte]
    
    # Aplicar control de drawdown
    drawdown_actual = calcular_drawdown(estado_portfolio)
    drawdown_max = parametros['drawdown_maximo']
    factor_drawdown = 1 - (drawdown_actual/drawdown_max)**2
    factor_drawdown = max(0, min(1, factor_drawdown))
    
    # Aplicar filtro de volatilidad
    vol_actual = calcular_volatilidad(precios, 21)
    vol_tendencia = calcular_volatilidad(precios, 63)
    ratio_vol = vol_actual / vol_tendencia
    factor_vol = 1.0
    if ratio_vol > 1.2:
        factor_vol = 1.2 / ratio_vol
    
    # Calcular exposición final
    exposicion_base = parametros['exposicion_maxima']
    exposicion = exposicion_base * factor_drawdown * factor_vol
    
    # Transformar señal en posición usando función sigmoide
    k = parametros['sensibilidad_sigmoide']
    posicion = exposicion * (2 / (1 + np.exp(-k * senal_combinada)) - 1)
    
    return posicion
```

### 4. Backtesting y Validación

```python
# Pseudocódigo para backtesting y validación
def backtest(datos, parametros):
    resultados = []
    
    # Walk-forward validation
    for inicio_test, fin_test in generar_ventanas_test(datos):
        # Datos de entrenamiento hasta inicio_test
        datos_train = datos[:inicio_test]
        
        # Entrenar ensemble de modelos
        modelos = entrenar_ensemble(datos_train, parametros)
        
        # Evaluar en ventana de test
        for fecha in range(inicio_test, fin_test):
            # Obtener señales de cada modelo
            senales_modelos = []
            for modelo in modelos:
                senal = modelo.generar_senal(datos, fecha)
                senales_modelos.append(senal)
            
            # Combinar señales con meta-modelo
            senal_combinada = meta_modelo.predecir(senales_modelos)
            
            # Aplicar gestión de riesgo
            posicion = aplicar_gestion_riesgo(senal_combinada, estado_portfolio)
            
            # Simular ejecución con costos
            rendimiento = simular_ejecucion(posicion, datos, fecha, costos)
            
            resultados.append({
                'fecha': fecha,
                'posicion': posicion,
                'rendimiento': rendimiento
            })
    
    # Calcular métricas
    sharpe = calcular_sharpe(resultados)
    drawdown_max = calcular_max_drawdown(resultados)
    correlacion_sp500 = calcular_correlacion(resultados, sp500)
    
    return {
        'resultados': resultados,
        'metricas': {
            'sharpe': sharpe,
            'drawdown_max': drawdown_max,
            'correlacion_sp500': correlacion_sp500
        }
    }
```

### 5. Métricas Esperadas

- **Sharpe ratio esperado**: 1.2-1.5 (post-costos)
- **Drawdown máximo esperado**: 12-15%
- **Correlación con S&P 500**: 0.3-0.5 (diversificación significativa)
- **Ratio de captación alcista/bajista**: 0.7/0.3 (asimétrico)

### 6. Consideraciones de Implementación

- Utilizar datos de yfinance para precios y volúmenes
- Implementar manejo de errores robusto para datos faltantes o erróneos
- Actualizar parámetros del modelo trimestralmente
- Monitorear continuamente el rendimiento y las métricas de riesgo
- Realizar análisis de atribución de rendimiento mensual para verificar fuentes de alpha
- Documentar todas las decisiones de diseño y parámetros para facilitar auditoría y mejora continua

Esta estrategia equilibra sofisticación analítica con robustez operativa, adaptándose a diferentes condiciones de mercado mientras mantiene un control riguroso del riesgo.

IMPLEMENTACIÓN EN CÓDIGO (del desarrollador):
```python
import os
import logging
import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit
from scipy.stats import norm
import itertools
import warnings
from datetime import datetime, timedelta
import statsmodels.api as sm
from scipy.stats import pearsonr
from tqdm import tqdm

# Crear directorios para resultados
os.makedirs('./artifacts/results', exist_ok=True)
os.makedirs('./artifacts/results/figures', exist_ok=True)
os.makedirs('./artifacts/results/data', exist_ok=True)

# Configurar logging
logging.basicConfig(
    filename='./artifacts/errors.txt',
    level=logging.ERROR,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# Suprimir advertencias
warnings.filterwarnings('ignore')

class AdaptiveMomentumStrategy:
    """
    Implementación de una estrategia de momentum adaptativo con múltiples factores
    y ajuste dinámico basado en el régimen de mercado.
    """
    
    def __init__(self, 
                 benchmark='SPY', 
                 start_date='2000-01-01',
                 end_date=None,
                 lookback_periods=[20, 60, 120, 252],
                 vol_lookback=21,
                 trend_lookback=200,
                 rebalance_freq='W-FRI',
                 risk_free_rate=0.02/252):
        """
        Inicializa la estrategia de momentum adaptativo.
        
        Args:
            benchmark (str): Ticker del índice de referencia
            start_date (str): Fecha de inicio para los datos
            end_date (str): Fecha de fin para los datos (None = hoy)
            lookback_periods (list): Períodos para calcular momentum
            vol_lookback (int): Período para calcular volatilidad
            trend_lookback (int): Período para identificar tendencia
            rebalance_freq (str): Frecuencia de rebalanceo
            risk_free_rate (float): Tasa libre de riesgo diaria
        """
        self.benchmark = benchmark
        self.start_date = start_date
        self.end_date = end_date if end_date else datetime.now().strftime('%Y-%m-%d')
        self.lookback_periods = lookback_periods
        self.vol_lookback = vol_lookback
        self.trend_lookback = trend_lookback
        self.rebalance_freq = rebalance_freq
        self.risk_free_rate = risk_free_rate
        
        # Parámetros adicionales
        self.universe_size = 100  # Top N acciones por capitalización
        self.max_positions = 20   # Máximo número de posiciones
        self.position_size_limit = 0.05  # Límite de tamaño por posición
        
        # Estado del portafolio
        self.portfolio_state = {
            'positions': {},
            'cash': 1000000,
            'equity': 1000000,
            'equity_curve': pd.Series(),
            'max_equity': 1000000,
            'drawdown': 0
        }
        
        # Datos de mercado
        self.market_data = None
        self.sp500_tickers = None
        self.benchmark_data = None
        
    def _get_sp500_tickers(self):
        """Obtiene la lista de tickers del S&P 500 desde Wikipedia."""
        try:
            url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
            tables = pd.read_html(url)
            sp500_table = tables[0]
            tickers = sp500_table['Symbol'].str.replace('.', '-').tolist()
            return tickers
        except Exception as e:
            logging.error(f"Error obteniendo tickers del S&P 500: {str(e)}")
            # Fallback a una lista predefinida de tickers principales
            return ['AAPL', 'MSFT', 'AMZN', 'GOOGL', 'META', 'TSLA', 'BRK-B', 'JNJ', 'V', 'PG', 
                    'UNH', 'JPM', 'MA', 'HD', 'NVDA', 'DIS', 'BAC', 'ADBE', 'CRM', 'KO', 
                    'PFE', 'CMCSA', 'VZ', 'INTC', 'NFLX', 'T', 'PEP', 'ABT', 'MRK', 'CSCO']
    
    def _download_data(self):
        """Descarga datos históricos para el benchmark y el universo de acciones."""
        try:
            # Obtener tickers del S&P 500
            self.sp500_tickers = self._get_sp500_tickers()
            
            # Descargar datos del benchmark
            self.benchmark_data = yf.download(
                self.benchmark, 
                start=self.start_date, 
                end=self.end_date
            )
            
            # Descargar datos para todos los tickers del S&P 500
            # Usamos una fecha anterior para tener suficientes datos para lookbacks
            extended_start = (pd.to_datetime(self.start_date) - pd.Timedelta(days=365)).strftime('%Y-%m-%d')
            
            print(f"Descargando datos para {len(self.sp500_tickers)} tickers...")
            data = yf.download(
                self.sp500_tickers, 
                start=extended_start, 
                end=self.end_date,
                group_by='column'
            )
            
            # Verificar si los datos están en formato MultiIndex
            if isinstance(data.columns, pd.MultiIndex):
                # Extraer precios de cierre
                close_prices = data['Adj Close']
                volumes = data['Volume']
            else:
                # Si solo se descargó un ticker, reorganizar
                close_prices = pd.DataFrame(data['Adj Close'])
                close_prices.columns = [self.sp500_tickers[0]]
                volumes = pd.DataFrame(data['Volume'])
                volumes.columns = [self.sp500_tickers[0]]
            
            # Filtrar por fecha de inicio real
            close_prices = close_prices[close_prices.index >= self.start_date]
            volumes = volumes[volumes.index >= self.start_date]
            
            # Guardar datos
            self.market_data = {
                'prices': close_prices,
                'volumes': volumes
            }
            
            print(f"Datos descargados correctamente. Rango de fechas: {close_prices.index[0]} a {close_prices.index[-1]}")
            
            # Guardar datos para referencia
            close_prices.to_csv('./artifacts/results/data/market_prices.csv')
            
            return True
        
        except Exception as e:
            logging.error(f"Error descargando datos: {str(e)}")
            import traceback
            logging.error(traceback.format_exc())
            return False
    
    def _calculate_returns(self, prices, periods=[1, 5, 20, 60, 120, 252]):
        """
        Calcula los retornos para diferentes períodos.
        
        Args:
            prices (DataFrame): DataFrame con precios
            periods (list): Lista de períodos para calcular retornos
            
        Returns:
            dict: Diccionario con DataFrames de retornos para cada período
        """
        returns = {}
        
        for period in periods:
            # Calcular retornos porcentuales
            ret = prices.pct_change(period).shift(1)  # Shift para evitar look-ahead bias
            returns[f'return_{period}d'] = ret
            
        return returns
    
    def _calculate_volatility(self, prices, lookback=21):
        """
        Calcula la volatilidad histórica.
        
        Args:
            prices (DataFrame): DataFrame con precios
            lookback (int): Período para calcular volatilidad
            
        Returns:
            DataFrame: Volatilidad histórica
        """
        # Calcular retornos diarios
        daily_returns = prices.pct_change().fillna(0)
        
        # Calcular volatilidad como desviación estándar móvil anualizada
        volatility = daily_returns.rolling(window=lookback).std() * np.sqrt(252)
        
        return volatility
    
    def _calculate_residual_returns(self, prices, benchmark_prices, lookback=252):
        """
        Calcula retornos residuales respecto al benchmark.
        
        Args:
            prices (DataFrame): DataFrame con precios de acciones
            benchmark_prices (Series): Serie con precios del benchmark
            lookback (int): Período para calcular la regresión
            
        Returns:
            DataFrame: Retornos residuales
        """
        # Calcular retornos
        stock_returns = prices.pct_change().fillna(0)
        benchmark_returns = benchmark_prices.pct_change().fillna(0)
        
        # Inicializar DataFrame para retornos residuales
        residual_returns = pd.DataFrame(index=stock_returns.index, columns=stock_returns.columns)
        
        # Para cada acción, calcular retornos residuales
        for ticker in stock_returns.columns:
            # Crear DataFrame para regresión
            ticker_returns = stock_returns[ticker]
            
            # Calcular retornos residuales usando ventana móvil
            for i in range(lookback, len(ticker_returns)):
                # Datos para la ventana actual
                y = ticker_returns.iloc[i-lookback:i].values
                X = benchmark_returns.iloc[i-lookback:i].values.reshape(-1, 1)
                
                # Verificar datos válidos
                if np.isnan(X).any() or np.isnan(y).any():
                    continue
                
                # Ajustar modelo de regresión
                try:
                    model = LinearRegression()
                    model.fit(X, y)
                    
                    # Calcular retorno esperado basado en beta
                    expected_return = model.predict(np.array([[benchmark_returns.iloc[i]]]))
                    
                    # Retorno residual = retorno real - retorno esperado
                    residual_returns.loc[ticker_returns.index[i], ticker] = ticker_returns.iloc[i] - expected_return[0]
                except:
                    continue
        
        # Calcular retornos residuales acumulados
        cum_residual_returns = residual_returns.fillna(0).cumsum()
        
        return cum_residual_returns
    
    def _normalize_signals(self, signals, method='zscore', lookback=252):
        """
        Normaliza las señales usando diferentes métodos.
        
        Args:
            signals (DataFrame): DataFrame con señales
            method (str): Método de normalización ('zscore', 'minmax', 'rank')
            lookback (int): Período para normalización
            
        Returns:
            DataFrame: Señales normalizadas
        """
        if method == 'zscore':
            # Z-score con ventana móvil
            mean = signals.rolling(lookback).mean()
            std = signals.rolling(lookback).std()
            normalized = (signals - mean) / std
            
            # Winsorización para limitar valores extremos
            normalized = normalized.clip(-3, 3)
            
        elif method == 'minmax':
            # Min-max con ventana móvil
            rolling_min = signals.rolling(lookback).min()
            rolling_max = signals.rolling(lookback).max()
            normalized = (signals - rolling_min) / (rolling_max - rolling_min)
            
        elif method == 'rank':
            # Normalización por rango percentil
            def rank_normalize(x):
                return pd.Series(x).rank(pct=True).values
            
            normalized = signals.rolling(lookback).apply(rank_normalize, raw=False)
            
        else:
            normalized = signals
            
        return normalized
    
    def _calculate_momentum_signals(self):
        """
        Calcula señales de momentum basadas en múltiples factores.
        
        Returns:
            dict: Diccionario con diferentes señales de momentum
        """
        prices = self.market_data['prices']
        
        # 1. Calcular retornos para diferentes períodos
        returns_dict = self._calculate_returns(prices, self.lookback_periods)
        
        # 2. Calcular volatilidad
        volatility = self._calculate_volatility(prices, self.vol_lookback)
        
        # 3. Calcular retornos residuales (alpha)
        residual_returns = self._calculate_residual_returns(
            prices, 
            self.benchmark_data['Adj Close'],
            lookback=252
        )
        
        # 4. Normalizar señales
        signals = {}
        
        # Momentum de precio normalizado por volatilidad
        for period, returns in returns_dict.items():
            vol_adj_returns = returns.divide(volatility)
            signals[f'vol_adj_{period}'] = self._normalize_signals(vol_adj_returns, 'zscore')
        
        # Momentum residual (alpha)
        signals['residual_momentum'] = self._normalize_signals(residual_returns, 'zscore')
        
        # Señal de tendencia (1 = alcista, 0 = bajista)
        sma200 = prices.rolling(self.trend_lookback).mean()
        trend_signal = (prices > sma200).astype(int)
        signals['trend'] = trend_signal
        
        return signals
    
    def _combine_signals(self, signals, weights=None):
        """
        Combina múltiples señales en una señal compuesta.
        
        Args:
            signals (dict): Diccionario con diferentes señales
            weights (dict): Pesos para cada señal (None = pesos iguales)
            
        Returns:
            DataFrame: Señal compuesta
        """
        # Filtrar señales que no son de tendencia
        momentum_signals = {k: v for k, v in signals.items() if k != 'trend'}
        
        # Si no se especifican pesos, usar pesos iguales
        if weights is None:
            weights = {k: 1/len(momentum_signals) for k in momentum_signals}
        
        # Combinar señales
        combined = pd.DataFrame(0, index=signals[list(signals.keys())[0]].index, 
                               columns=signals[list(signals.keys())[0]].columns)
        
        for signal_name, signal_df in momentum_signals.items():
            if signal_name in weights:
                # Manejar NaNs reemplazándolos con 0
                filled_signal = signal_df.fillna(0)
                combined += weights[signal_name] * filled_signal
        
        # Aplicar filtro de tendencia
        if 'trend' in signals:
            # Solo mantener señales positivas en tendencia alcista
            trend = signals['trend']
            # Donde la tendencia es bajista (0), multiplicar señales positivas por 0.5
            combined = combined * (trend * 0.5 + 0.5)
        
        return combined
    
    def _calculate_position_sizes(self, combined_signal, date, max_positions=20, max_size=0.05):
        """
        Calcula tamaños de posición basados en la señal combinada.
        
        Args:
            combined_signal (DataFrame): Señal combinada
            date (Timestamp): Fecha actual
            max_positions (int): Número máximo de posiciones
            max_size (float): Tamaño máximo por posición
            
        Returns:
            dict: Tamaños de posición por ticker
        """
        # Obtener señal para la fecha actual
        if date not in combined_signal.index:
            return {}
        
        current_signal = combined_signal.loc[date]
        
        # Filtrar NaNs
        current_signal = current_signal.dropna()
        
        # Ordenar por señal (mayor a menor)
        sorted_signal = current_signal.sort_values(ascending=False)
        
        # Seleccionar top N acciones
        top_stocks = sorted_signal.head(max_positions)
        
        # Calcular factor de volatilidad para ajustar posiciones
        volatility = self._calculate_volatility(self.market_data['prices'], self.vol_lookback)
        if date in volatility.index:
            current_vol = volatility.loc[date].dropna()
            vol_factor = 1 / current_vol
            vol_factor = vol_factor.clip(0.3, 1.0)  # Limitar factor de volatilidad
        else:
            vol_factor = pd.Series(1.0, index=top_stocks.index)
        
        # Calcular factor de drawdown
        if len(self.portfolio_state['equity_curve']) > 0:
            current_equity = self.portfolio_state['equity']
            max_equity = self.portfolio_state['max_equity']
            drawdown = 1 - (current_equity / max_equity)
            drawdown_factor = 1 - (drawdown * 0.5)  # Reducir exposición en drawdown
            drawdown_factor = max(0.5, drawdown_factor)  # Mínimo 50% de exposición
        else:
            drawdown_factor = 1.0
        
        # Calcular posiciones ajustadas por volatilidad y drawdown
        position_sizes = {}
        
        # Solo invertir en acciones con señal positiva
        positive_signals = top_stocks[top_stocks > 0]
        
        if len(positive_signals) > 0:
            # Normalizar señales para que sumen 1
            signal_sum = positive_signals.sum()
            if signal_sum > 0:
                normalized_signals = positive_signals / signal_sum
                
                for ticker, signal in normalized_signals.items():
                    # Ajustar por volatilidad si está disponible
                    vol_adj = vol_factor.get(ticker, 1.0)
                    
                    # Calcular tamaño de posición
                    size = signal * vol_adj * drawdown_factor
                    
                    # Limitar tamaño máximo
                    size = min(size, max_size)
                    
                    position_sizes[ticker] = size
                
                # Renormalizar para que sumen 1.0
                total_size = sum(position_sizes.values())
                if total_size > 0:
                    position_sizes = {k: v/total_size for k, v in position_sizes.items()}
        
        return position_sizes
    
    def _execute_trades(self, target_positions, current_date, prices):
        """
        Ejecuta operaciones para ajustar el portafolio a las posiciones objetivo.
        
        Args:
            target_positions (dict): Posiciones objetivo
            current_date (Timestamp): Fecha actual
            prices (DataFrame): Precios actuales
            
        Returns:
            dict: Registro de operaciones
        """
        trades = []
        current_positions = self.portfolio_state['positions']
        cash = self.portfolio_state['cash']
        equity = self.portfolio_state['equity']
        
        # Obtener precios actuales
        if current_date in prices.index:
            current_prices = prices.loc[current_date]
        else:
            return {'trades': trades, 'error': 'No price data for current date'}
        
        # Calcular valor total del portafolio
        portfolio_value = cash
        for ticker, shares in current_positions.items():
            if ticker in current_prices and not np.isnan(current_prices[ticker]):
                portfolio_value += shares * current_prices[ticker]
        
        # Actualizar equity
        self.portfolio_state['equity'] = portfolio_value
        self.portfolio_state['equity_curve'][current_date] = portfolio_value
        
        # Actualizar max equity para cálculo de drawdown
        if portfolio_value > self.portfolio_state['max_equity']:
            self.portfolio_state['max_equity'] = portfolio_value
        
        # Calcular posiciones objetivo en unidades monetarias
        target_position_values = {ticker: weight * portfolio_value 
                                 for ticker, weight in target_positions.items()}
        
        # Cerrar posiciones que ya no están en el objetivo
        for ticker in list(current_positions.keys()):
            if ticker not in target_position_values:
                # Vender toda la posición
                shares = current_positions[ticker]
                if ticker in current_prices and not np.isnan(current_prices[ticker]):
                    price = current_prices[ticker]
                    cash_change = shares * price
                    
                    # Registrar operación
                    trades.append({
                        'date': current_date,
                        'ticker': ticker,
                        'action': 'SELL',
                        'shares': shares,
                        'price': price,
                        'value': cash_change
                    })
                    
                    # Actualizar cash
                    cash += cash_change
                    
                    # Eliminar posición
                    del current_positions[ticker]
        
        # Ajustar posiciones existentes y abrir nuevas
        for ticker, target_value in target_position_values.items():
            if ticker in current_prices and not np.isnan(current_prices[ticker]):
                price = current_prices[ticker]
                target_shares = target_value / price
                
                # Calcular diferencia de acciones
                current_shares = current_positions.get(ticker, 0)
                shares_diff = target_shares - current_shares
                
                # Ejecutar operación si la diferencia es significativa
                min_trade_value = 1000  # Valor mínimo para ejecutar una operación
                if abs(shares_diff * price) > min_trade_value:
                    if shares_diff > 0:
                        # Comprar
                        action = 'BUY'
                        # Verificar si hay suficiente cash
                        cost = shares_diff * price
                        if cost > cash:
                            # Ajustar shares si no hay suficiente cash
                            shares_diff = cash / price
                            cost = shares_diff * price
                        
                        # Actualizar cash
                        cash -= cost
                    else:
                        # Vender
                        action = 'SELL'
                        cash_change = abs(shares_diff) * price
                        cash += cash_change
                    
                    # Actualizar posición
                    current_positions[ticker] = current_positions.get(ticker, 0) + shares_diff
                    
                    # Eliminar posiciones con 0 acciones
                    if current_positions[ticker] == 0:
                        del current_positions[ticker]
                    
                    # Registrar operación
                    trades.append({
                        'date': current_date,
                        'ticker': ticker,
                        'action': action,
                        'shares': abs(shares_diff),
                        'price': price,
                        'value': abs(shares_diff * price)
                    })
        
        # Actualizar estado del portafolio
        self.portfolio_state['positions'] = current_positions
        self.portfolio_state['cash'] = cash
        
        return {'trades': trades}
    
    def _evaluate_performance(self, equity_curve, benchmark_prices):
        """
        Evalúa el rendimiento de la estrategia.
        
        Args:
            equity_curve (Series): Curva de equity
            benchmark_prices (Series): Precios del benchmark
            
        Returns:
            dict: Métricas de rendimiento
        """
        # Calcular retornos diarios
        returns = equity_curve.pct_change().dropna()
        
        # Calcular retornos del benchmark
        benchmark_returns = benchmark_prices.pct_change().dropna()
        
        # Alinear fechas
        aligned_data = pd.concat([returns, benchmark_returns], axis=1).dropna()
        aligned_data.columns = ['strategy', 'benchmark']
        
        strategy_returns = aligned_data['strategy']
        benchmark_returns = aligned_data['benchmark']
        
        # Calcular métricas
        total_return = (equity_curve.iloc[-1] / equity_curve.iloc[0]) - 1
        annual_return = (1 + total_return) ** (252 / len(equity_curve)) - 1
        
        volatility = strategy_returns.std() * np.sqrt(252)
        
        # Sharpe Ratio
        sharpe_ratio = (annual_return - self.risk_free_rate) / volatility if volatility > 0 else 0
        
        # Drawdown
        cumulative = (1 + strategy_returns).cumprod()
        running_max = cumulative.cummax()
        drawdown = (cumulative / running_max) - 1
        max_drawdown = drawdown.min()
        
        # Beta y Alpha
        cov = strategy_returns.cov(benchmark_returns)
        benchmark_var = benchmark_returns.var()
        beta = cov / benchmark_var if benchmark_var > 0 else 0
        
        benchmark_annual_return = (1 + benchmark_returns.mean()) ** 252 - 1
        alpha = annual_return - (self.risk_free_rate + beta * (benchmark_annual_return - self.risk_free_rate))
        
        # Calcular correlación
        correlation = strategy_returns.corr(benchmark_returns)
        
        # Calcular Information Ratio
        active_returns = strategy_returns - benchmark_returns
        information_ratio = active_returns.mean() / active_returns.std() * np.sqrt(252) if active_returns.std() > 0 else 0
        
        # Calcular Win Rate
        win_rate = (strategy_returns > 0).mean()
        
        # Calcular Profit Factor
        gains = strategy_returns[strategy_returns > 0].sum()
        losses = abs(strategy_returns[strategy_returns < 0].sum())
        profit_factor = gains / losses if losses > 0 else float('inf')
        
        # Calcular Sortino Ratio
        downside_returns = strategy_returns[strategy_returns < 0]
        downside_deviation = downside_returns.std() * np.sqrt(252) if len(downside_returns) > 0 else 0
        sortino_ratio = (annual_return - self.risk_free_rate) / downside_deviation if downside_deviation > 0 else 0
        
        # Calcular Calmar Ratio
        calmar_ratio = annual_return / abs(max_drawdown) if max_drawdown < 0 else float('inf')
        
        # Crear diccionario de métricas
        metrics = {
            'total_return': total_return,
            'annual_return': annual_return,
            'volatility': volatility,
            'sharpe_ratio': sharpe_ratio,
            'max_drawdown': max_drawdown,
            'beta': beta,
            'alpha': alpha,
            'correlation': correlation,
            'information_ratio': information_ratio,
            'win_rate': win_rate,
            'profit_factor': profit_factor,
            'sortino_ratio': sortino_ratio,
            'calmar_ratio': calmar_ratio
        }
        
        return metrics
    
    def backtest(self):
        """
        Ejecuta un backtest de la estrategia.
        
        Returns:
            dict: Resultados del backtest
        """
        try:
            # Descargar datos si no están disponibles
            if self.market_data is None:
                success = self._download_data()
                if not success:
                    return {'error': 'Failed to download data'}
            
            prices = self.market_data['prices']
            
            # Calcular señales
            signals = self._calculate_momentum_signals()
            
            # Combinar señales
            combined_signal = self._combine_signals(signals)
            
            # Inicializar equity curve
            self.portfolio_state['equity_curve'] = pd.Series(dtype=float)
            
            # Determinar fechas de rebalanceo
            rebalance_dates = pd.date_range(
                start=prices.index[0], 
                end=prices.index[-1], 
                freq=self.rebalance_freq
            )
            
            # Filtrar fechas de rebalanceo que están en los datos
            rebalance_dates = [date for date in rebalance_dates if date in prices.index]
            
            # Ejecutar backtest
            all_trades = []
            
            for date in rebalance_dates:
                # Calcular posiciones objetivo
                target_positions = self._calculate_position_sizes(
                    combined_signal, 
                    date, 
                    max_positions=self.max_positions,
                    max_size=self.position_size_limit
                )
                
                # Ejecutar operaciones
                execution_result = self._execute_trades(target_positions, date, prices)
                
                if 'trades' in execution_result:
                    all_trades.extend(execution_result['trades'])
            
            # Convertir equity curve a DataFrame
            equity_curve = pd.Series(self.portfolio_state['equity_curve'])
            
            # Evaluar rendimiento
            performance = self._evaluate_performance(equity_curve, self.benchmark_data['Adj Close'])
            
            # Guardar resultados
            equity_curve.to_csv('./artifacts/results/data/equity_curve.csv')
            
            # Convertir trades a DataFrame y guardar
            trades_df = pd.DataFrame(all_trades)
            if not trades_df.empty:
                trades_df.to_csv('./artifacts/results/data/trades.csv', index=False)
            
            # Guardar métricas
            metrics_df = pd.DataFrame([performance])
            metrics_df.to_csv('./artifacts/results/data/performance_metrics.csv', index=False)
            
            # Generar gráficos
            self._generate_performance_charts(equity_curve, self.benchmark_data['Adj Close'])
            
            return {
                'equity_curve': equity_curve,
                'trades': all_trades,
                'performance': performance
            }
            
        except Exception as e:
            logging.error(f"Error en backtest: {str(e)}")
            import traceback
            logging.error(traceback.format_exc())
            return {'error': str(e)}
    
    def _generate_param_combinations(self, param_grid):
        """
        Genera todas las combinaciones de parámetros para optimización.
        
        Args:
            param_grid (dict): Grid de parámetros
            
        Returns:
            list: Lista de diccionarios con combinaciones de parámetros
        """
        keys = list(param_grid.keys())
        values = list(param_grid.values())
        
        combinations = list(itertools.product(*values))
        
        return [dict(zip(keys, combo)) for combo in combinations]
    
    def optimize(self, param_grid, metric='sharpe_ratio', n_splits=5):
        """
        Optimiza los parámetros de la estrategia.
        
        Args:
            param_grid (dict): Grid de parámetros
            metric (str): Métrica a optimizar
            n_splits (int): Número de divisiones para validación cruzada
            
        Returns:
            dict: Resultados de la optimización
        """
        try:
            # Descargar datos si no están disponibles
            if self.market_data is None:
                success = self._download_data()
                if not success:
                    return {'error': 'Failed to download data'}
            
            # Generar combinaciones de parámetros
            param_combinations = self._generate_param_combinations(param_grid)
            
            print(f"Optimizando {len(param_combinations)} combinaciones de parámetros...")
            
            # Inicializar resultados
            results = []
            
            # Configurar validación cruzada
            tscv = TimeSeriesSplit(n_splits=n_splits)
            
            # Obtener fechas
            dates = self.market_data['prices'].index
            
            for params in tqdm(param_combinations):
                # Actualizar parámetros
                for key, value in params.items():
                    setattr(self, key, value)
                
                # Inicializar métricas para cross-validation
                cv_metrics = []
                
                for train_idx, test_idx in tscv.split(dates):
                    # Definir fechas de entrenamiento y prueba
                    train_dates = dates[train_idx]
                    test_dates = dates[test_idx]
                    
                    if len(train_dates) < 252 or len(test_dates) < 63:  # Mínimo 1 año de entrenamiento, 3 meses de prueba
                        continue
                    
                    # Configurar fechas
                    self.start_date = train_dates[0].strftime('%Y-%m-%d')
                    self.end_date = test_dates[-1].strftime('%Y-%m-%d')
                    
                    # Ejecutar backtest
                    backtest_result = self.backtest()
                    
                    if 'performance' in backtest_result:
                        cv_metrics.append(backtest_result['performance'][metric])
                
                # Calcular promedio de métricas
                if cv_metrics:
                    avg_metric = np.mean(cv_metrics)
                    
                    results.append({
                        'params': params,
                        'avg_metric': avg_metric,
                        'metric_name': metric,
                        'cv_metrics': cv_metrics
                    })
            
            # Ordenar resultados
            results.sort(key=lambda x: x['avg_metric'], reverse=True)
            
            # Guardar resultados
            results_df = pd.DataFrame([
                {**r['params'], 'avg_metric': r['avg_metric']} 
                for r in results
            ])
            
            results_df.to_csv('./artifacts/results/data/optimization_results.csv', index=False)
            
            # Generar gráfico de resultados
            if len(results) > 0:
                self._generate_optimization_chart(results, metric)
            
            return {
                'best_params': results[0]['params'] if results else None,
                'best_metric': results[0]['avg_metric'] if results else None,
                'all_results': results
            }
            
        except Exception as e:
            logging.error(f"Error en optimización: {str(e)}")
            import traceback
            logging.error(traceback.format_exc())
            return {'error': str(e)}
    
    def walk_forward_test(self, window_size=252*2, step_size=63, param_grid=None):
        """
        Ejecuta un test walk-forward con reoptimización periódica.
        
        Args:
            window_size (int): Tamaño de la ventana de entrenamiento en días
            step_size (int): Tamaño del paso para avanzar la ventana en días
            param_grid (dict): Grid de parámetros para optimización
            
        Returns:
            dict: Resultados del test walk-forward
        """
        try:
            # Descargar datos si no están disponibles
            if self.market_data is None:
                success = self._download_data()
                if not success:
                    return {'error': 'Failed to download data'}
            
            # Obtener fechas
            dates = self.market_data['prices'].index
            
            # Inicializar resultados
            equity_curve = pd.Series(dtype=float)
            all_trades = []
            period_results = []
            
            # Iterar por períodos
            for i in range(0, len(dates) - window_size - step_size, step_size):
                # Definir fechas de entrenamiento y prueba
                train_start_idx = i
                train_end_idx = i + window_size
                test_start_idx = train_end_idx
                test_end_idx = min(test_start_idx + step_size, len(dates))
                
                train_dates = dates[train_start_idx:train_end_idx]
                test_dates = dates[test_start_idx:test_end_idx]
                
                print(f"Período {i//step_size + 1}: Entrenamiento {train_dates[0]} a {train_dates[-1]}, Prueba {test_dates[0]} a {test_dates[-1]}")
                
                # Configurar fechas de entrenamiento
                self.start_date = train_dates[0].strftime('%Y-%m-%d')
                self.end_date = train_dates[-1].strftime('%Y-%m-%d')
                
                # Optimizar parámetros si se proporciona un grid
                if param_grid:
                    opt_result = self.optimize(param_grid, metric='sharpe_ratio', n_splits=3)
                    
                    if 'best_params' in opt_result and opt_result['best_params']:
                        # Actualizar parámetros
                        for key, value in opt_result['best_params'].items():
                            setattr(self, key, value)
                
                # Configurar fechas de prueba
                self.start_date = test_dates[0].strftime('%Y-%m-%d')
                self.end_date = test_dates[-1].strftime('%Y-%m-%d')
                
                # Ejecutar backtest en período de prueba
                backtest_result = self.backtest()
                
                if 'equity_curve' in backtest_result:
                    # Agregar resultados
                    period_equity = backtest_result['equity_curve']
                    equity_curve = pd.concat([equity_curve, period_equity])
                    
                    if 'trades' in backtest_result:
                        all_trades.extend(backtest_result['trades'])
                    
                    period_results.append({
                        'period': i//step_size + 1,
                        'train_start': train_dates[0].strftime('%Y-%m-%d'),
                        'train_end': train_dates[-1].strftime('%Y-%m-%d'),
                        'test_start': test_dates[0].strftime('%Y-%m-%d'),
                        'test_end': test_dates[-1].strftime('%Y-%m-%d'),
                        'performance': backtest_result['performance']
                    })
            
            # Evaluar rendimiento global
            if not equity_curve.empty:
                overall_performance = self._evaluate_performance(equity_curve, self.benchmark_data['Adj Close'])
                
                # Guardar resultados
                equity_curve.to_csv('./artifacts/results/data/wf_equity_curve.csv')
                
                # Convertir trades a DataFrame y guardar
                trades_df = pd.DataFrame(all_trades)
                if not trades_df.empty:
                    trades_df.to_csv('./artifacts/results/data/wf_trades.csv', index=False)
                
                # Guardar métricas
                metrics_df = pd.DataFrame([overall_performance])
                metrics_df.to_csv('./artifacts/results/data/wf_performance_metrics.csv', index=False)
                
                # Guardar resultados por período
                period_df = pd.DataFrame([
                    {
                        'period': r['period'],
                        'train_start': r['train_start'],
                        'train_end': r['train_end'],
                        'test_start': r['test_start'],
                        'test_end': r['test_end'],
                        **{f"{k}": v for k, v in r['performance'].items()}
                    }
                    for r in period_results
                ])
                
                period_df.to_csv('./artifacts/results/data/wf_period_results.csv', index=False)
                
                # Generar gráficos
                self._generate_performance_charts(equity_curve, self.benchmark_data['Adj Close'], prefix='wf_')
                
                return {
                    'equity_curve': equity_curve,
                    'trades': all_trades,
                    'overall_performance': overall_performance,
                    'period_results': period_results
                }
            else:
                return {'error': 'No equity curve generated'}
            
        except Exception as e:
            logging.error(f"Error en walk-forward test: {str(e)}")
            import traceback
            logging.error(traceback.format_exc())
            return {'error': str(e)}
    
    def _generate_performance_charts(self, equity_curve, benchmark_prices, prefix=''):
        """
        Genera gráficos de rendimiento.
        
        Args:
            equity_curve (Series): Curva de equity
            benchmark_prices (Series): Precios del benchmark
            prefix (str): Prefijo para nombres de archivo
        """
        try:
            # Normalizar equity curve y benchmark para comparación
            norm_equity = equity_curve / equity_curve.iloc[0]
            norm_benchmark = benchmark_prices / benchmark_prices.iloc[0]
            
            # Alinear fechas
            aligned_data = pd.concat([norm_equity, norm_benchmark], axis=1).dropna()
            aligned_data.columns = ['Strategy', 'Benchmark']
            
            # Gráfico de rendimiento
            plt.figure(figsize=(12, 6))
            aligned_data.plot(figsize=(12, 6))
            plt.title('Cumulative Performance')
            plt.ylabel('Growth of $1')
            plt.grid(True, alpha=0.3)
            plt.savefig(f'./artifacts/results/figures/{prefix}cumulative_performance.png')
            plt.close()
            
            # Calcular drawdowns
            strategy_returns = aligned_data['Strategy'].pct_change().dropna()
            cumulative = (1 + strategy_returns).cumprod()
            running_max = cumulative.cummax()
            drawdown = (cumulative / running_max) - 1
            
            # Gráfico de drawdown
            plt.figure(figsize=(12, 6))
            drawdown.plot(figsize=(12, 6), color='red')
            plt.title('Strategy Drawdown')
            plt.ylabel('Drawdown')
            plt.grid(True, alpha=0.3)
            plt.fill_between(drawdown.index, drawdown.values, 0, color='red', alpha=0.3)
            plt.savefig(f'./artifacts/results/figures/{prefix}drawdown.png')
            plt.close()
            
            # Gráfico de retornos mensuales
            monthly_returns = strategy_returns.resample('M').apply(lambda x: (1 + x).prod() - 1)
            
            plt.figure(figsize=(12, 6))
            monthly_returns.plot(kind='bar', figsize=(12, 6))
            plt.title('Monthly Returns')
            plt.ylabel('Return')
            plt.grid(True, alpha=0.3)
            plt.savefig(f'./artifacts/results/figures/{prefix}monthly_returns.png')
            plt.close()
            
            # Gráfico de distribución de retornos
            plt.figure(figsize=(12, 6))
            sns.histplot(strategy_returns, kde=True)
            plt.title('Returns Distribution')
            plt.xlabel('Daily Return')
            plt.grid(True, alpha=0.3)
            plt.savefig(f'./artifacts/results/figures/{prefix}returns_distribution.png')
            plt.close()
            
            # Gráfico de retornos acumulados por año
            yearly_returns = strategy_returns.resample('Y').apply(lambda x: (1 + x).prod() - 1)
            
            plt.figure(figsize=(12, 6))
            yearly_returns.plot(kind='bar', figsize=(12, 6))
            plt.title('Yearly Returns')
            plt.ylabel('Return')
            plt.grid(True, alpha=0.3)
            plt.savefig(f'./artifacts/results/figures/{prefix}yearly_returns.png')
            plt.close()
            
        except Exception as e:
            logging.error(f"Error generando gráficos de rendimiento: {str(e)}")
    
    def _generate_optimization_chart(self, results, metric):
        """
        Genera gráficos de resultados de optimización.
        
        Args:
            results (list): Resultados de optimización
            metric (str): Métrica optimizada
        """
        try:
            # Convertir resultados a DataFrame
            results_df = pd.DataFrame([
                {**r['params'], 'metric': r['avg_metric']} 
                for r in results
            ])
            
            # Ordenar por métrica
            results_df = results_df.sort_values('metric', ascending=False)
            
            # Gráfico de barras de mejores resultados
            plt.figure(figsize=(12, 6))
            top_n = min(20, len(results_df))
            results_df.head(top_n).plot(y='metric', kind='bar', figsize=(12, 6))
            plt.title(f'Top {top_n} Parameter Combinations by {metric}')
            plt.ylabel(metric)
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(f'./artifacts/results/figures/optimization_top_results.png')
            plt.close()
            
            # Análisis de parámetros individuales
            for param in results_df.columns:
                if param != 'metric':
                    # Agrupar por parámetro
                    param_analysis = results_df.groupby(param)['metric'].mean().reset_index()
                    
                    plt.figure(figsize=(10, 5))
                    param_analysis.plot(x=param, y='metric', kind='bar', figsize=(10, 5))
                    plt.title(f'Average {metric} by {param}')
                    plt.ylabel(f'Average {metric}')
                    plt.grid(True, alpha=0.3)
                    plt.tight_layout()
                    plt.savefig(f'./artifacts/results/figures/optimization_param_{param}.png')
                    plt.close()
            
        except Exception as e:
            logging.error(f"Error generando gráficos de optimización: {str(e)}")

# Ejecutar la estrategia
if __name__ == "__main__":
    try:
        # Inicializar estrategia
        strategy = AdaptiveMomentumStrategy(
            benchmark='SPY',
            start_date='2010-01-01',
            end_date='2023-01-01',
            lookback_periods=[20, 60, 120, 252],
            vol_lookback=21,
            trend_lookback=200,
            rebalance_freq='W-FRI'
        )
        
        # Ejecutar backtest
        print("Ejecutando backtest...")
        backtest_result = strategy.backtest()
        
        if 'performance' in backtest_result:
            print("\nResultados del backtest:")
            for metric, value in backtest_result['performance'].items():
                print(f"{metric}: {value:.4f}")
        
        # Optimizar parámetros
        print("\nOptimizando parámetros...")
        param_grid = {
            'lookback_periods': [[20, 60, 120, 252], [10, 30, 90, 180]],
            'vol_lookback': [21, 42, 63],
            'trend_lookback': [100, 200, 300],
            'rebalance_freq': ['W-FRI', 'M']
        }
        
        # Comentado para reducir tiempo de ejecución
        # opt_result = strategy.optimize(param_grid, metric='sharpe_ratio')
        
        # Ejecutar walk-forward test
        print("\nEjecutando walk-forward test...")
        wf_result = strategy.walk_forward_test(
            window_size=252*2,
            step_size=63,
            param_grid=None  # Usar None para evitar reoptimización
        )
        
        if 'overall_performance' in wf_result:
            print("\nResultados del walk-forward test:")
            for metric, value in wf_result['overall_performance'].items():
                print(f"{metric}: {value:.4f}")
        
        print("\nProceso completado. Resultados guardados en ./artifacts/results/")
        
    except Exception as e:
        logging.error(f"Error en ejecución principal: {str(e)}")
        import traceback
        logging.error(traceback.format_exc())
        print(f"Error: {str(e)}")
```

Por favor analiza si el código implementa correctamente la idea conceptual y sugiere mejoras específicas.


RESPONSE:
# Revisión de Código

1. ¿El código implementa correctamente la idea? **Parcialmente**

2. Sugerencias específicas:

1. **Implementación incompleta del modelo HMM para regímenes de mercado**:
   - El código menciona la identificación de regímenes en `_calculate_momentum_signals()` pero no implementa el modelo HMM descrito en la idea original.
   - Falta la función `identificar_regimenes()` que aparece en el pseudocódigo pero no está implementada.
   - Recomendación: Implementar el modelo HMM usando `hmmlearn` en lugar de `pymc3` (que mencionas como problemático).

2. **Ausencia del sistema de Ensemble con Stacking**:
   - La idea original propone un sistema de ensemble con 10-15 variantes y stacking, pero el código solo implementa una única estrategia.
   - Recomendación: Implementar al menos una versión simplificada del ensemble con 3-5 variantes y un meta-modelo básico.

3. **Cálculo incompleto de retornos residuales**:
   - La función `_calculate_residual_returns()` solo ajusta por el factor de mercado (benchmark), pero la idea original menciona un modelo multifactorial (Mercado, Tamaño, Valor, Momentum, Calidad).
   - Recomendación: Ampliar para incluir más factores o al menos aclarar la limitación.

4. **Problemas de alineación temporal en `_execute_trades()`**:
   - La línea 607 asume que `current_date` está en `prices.index`, pero no maneja correctamente el caso cuando no lo está.
   - Recomendación: Usar `get_loc` con `method='nearest'` para encontrar la fecha más cercana.

5. **Look-ahead bias en el cálculo de señales**:
   - En `_calculate_returns()`, el shift(1) puede no ser suficiente para evitar look-ahead bias en todos los casos.
   - Recomendación: Asegurar que todas las señales usen datos estrictamente anteriores a la fecha de decisión.

6. **Gestión de riesgo incompleta**:
   - Falta implementar el CVaR/Expected Shortfall mencionado en la idea original.
   - Los stops dinámicos contextuales no están implementados según lo descrito.
   - Recomendación: Añadir estas funcionalidades en `_calculate_position_sizes()`.

7. **Manejo de NaNs en la combinación de señales**:
   - En `_combine_signals()`, línea 458, se usa `fillna(0)` que puede introducir sesgos.
   - Recomendación: Considerar métodos más robustos como imputación por media móvil o propagación de NaNs.

8. **Optimización de ejecución incompleta**:
   - La banda de no-transacción adaptativa mencionada en la idea no está completamente implementada.
   - Recomendación: Mejorar `_execute_trades()` para incluir umbrales adaptativos basados en volatilidad.

9. **Falta de normalización adaptativa con winsorización dinámica**:
   - La idea menciona winsorización basada en MAD, pero en `_normalize_signals()` se usa un clip fijo (-3, 3).
   - Recomendación: Implementar la winsorización dinámica basada en MAD como se describe en la idea.

10. **Problemas en la validación walk-forward**:
    - La función `walk_forward_test()` reinicializa el estado del portafolio en cada período, lo que no es realista.
    - Recomendación: Mantener el estado del portafolio entre períodos para una simulación más precisa.

11. **Falta de tratamiento de datos faltantes**:
    - El código no maneja adecuadamente los datos faltantes en `_calculate_momentum_signals()`.
    - Recomendación: Implementar estrategias robustas para manejar NaNs en las series temporales.

12. **Ausencia de filtro de tendencia de volatilidad**:
    - El filtro de tendencia de volatilidad mencionado en la idea no está completamente implementado.
    - Recomendación: Añadir el cálculo de la derivada de volatilidad para reducir exposición cuando es positiva.


--- desarrollador (STEP 12.1) ---
INPUT 1 (user):
# Revisión de Código

1. ¿El código implementa correctamente la idea? **Parcialmente**

2. Sugerencias específicas:

1. **Implementación incompleta del modelo HMM para regímenes de mercado**:
   - El código menciona la identificación de regímenes en `_calculate_momentum_signals()` pero no implementa el modelo HMM descrito en la idea original.
   - Falta la función `identificar_regimenes()` que aparece en el pseudocódigo pero no está implementada.
   - Recomendación: Implementar el modelo HMM usando `hmmlearn` en lugar de `pymc3` (que mencionas como problemático).

2. **Ausencia del sistema de Ensemble con Stacking**:
   - La idea original propone un sistema de ensemble con 10-15 variantes y stacking, pero el código solo implementa una única estrategia.
   - Recomendación: Implementar al menos una versión simplificada del ensemble con 3-5 variantes y un meta-modelo básico.

3. **Cálculo incompleto de retornos residuales**:
   - La función `_calculate_residual_returns()` solo ajusta por el factor de mercado (benchmark), pero la idea original menciona un modelo multifactorial (Mercado, Tamaño, Valor, Momentum, Calidad).
   - Recomendación: Ampliar para incluir más factores o al menos aclarar la limitación.

4. **Problemas de alineación temporal en `_execute_trades()`**:
   - La línea 607 asume que `current_date` está en `prices.index`, pero no maneja correctamente el caso cuando no lo está.
   - Recomendación: Usar `get_loc` con `method='nearest'` para encontrar la fecha más cercana.

5. **Look-ahead bias en el cálculo de señales**:
   - En `_calculate_returns()`, el shift(1) puede no ser suficiente para evitar look-ahead bias en todos los casos.
   - Recomendación: Asegurar que todas las señales usen datos estrictamente anteriores a la fecha de decisión.

6. **Gestión de riesgo incompleta**:
   - Falta implementar el CVaR/Expected Shortfall mencionado en la idea original.
   - Los stops dinámicos contextuales no están implementados según lo descrito.
   - Recomendación: Añadir estas funcionalidades en `_calculate_position_sizes()`.

7. **Manejo de NaNs en la combinación de señales**:
   - En `_combine_signals()`, línea 458, se usa `fillna(0)` que puede introducir sesgos.
   - Recomendación: Considerar métodos más robustos como imputación por media móvil o propagación de NaNs.

8. **Optimización de ejecución incompleta**:
   - La banda de no-transacción adaptativa mencionada en la idea no está completamente implementada.
   - Recomendación: Mejorar `_execute_trades()` para incluir umbrales adaptativos basados en volatilidad.

9. **Falta de normalización adaptativa con winsorización dinámica**:
   - La idea menciona winsorización basada en MAD, pero en `_normalize_signals()` se usa un clip fijo (-3, 3).
   - Recomendación: Implementar la winsorización dinámica basada en MAD como se describe en la idea.

10. **Problemas en la validación walk-forward**:
    - La función `walk_forward_test()` reinicializa el estado del portafolio en cada período, lo que no es realista.
    - Recomendación: Mantener el estado del portafolio entre períodos para una simulación más precisa.

11. **Falta de tratamiento de datos faltantes**:
    - El código no maneja adecuadamente los datos faltantes en `_calculate_momentum_signals()`.
    - Recomendación: Implementar estrategias robustas para manejar NaNs en las series temporales.

12. **Ausencia de filtro de tendencia de volatilidad**:
    - El filtro de tendencia de volatilidad mencionado en la idea no está completamente implementado.
    - Recomendación: Añadir el cálculo de la derivada de volatilidad para reducir exposición cuando es positiva.

RESPONSE:
```python
import os
import logging
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import yfinance as yf
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import TimeSeriesSplit
from scipy import stats
from hmmlearn import hmm
import warnings
from datetime import datetime, timedelta
import requests
from bs4 import BeautifulSoup
import time
from tqdm import tqdm

# Crear directorios para resultados
os.makedirs('./artifacts/results', exist_ok=True)
os.makedirs('./artifacts/results/figures', exist_ok=True)
os.makedirs('./artifacts/results/data', exist_ok=True)

# Configurar logging
logging.basicConfig(
    filename='./artifacts/errors.txt',
    level=logging.ERROR,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# Ignorar advertencias
warnings.filterwarnings('ignore')

class AdaptiveMomentumStrategy:
    def __init__(self, start_date='2010-01-01', end_date=None, benchmark='^GSPC', 
                 lookback_periods=[20, 60, 120, 252], vol_lookback=63, 
                 rebalance_freq='W-FRI', initial_capital=1000000):
        """
        Inicializa la estrategia de momentum adaptativo con múltiples horizontes temporales
        
        Args:
            start_date (str): Fecha de inicio para los datos
            end_date (str): Fecha de fin para los datos (None = hoy)
            benchmark (str): Ticker del benchmark (S&P 500 por defecto)
            lookback_periods (list): Períodos para calcular momentum
            vol_lookback (int): Período para calcular volatilidad
            rebalance_freq (str): Frecuencia de rebalanceo
            initial_capital (float): Capital inicial
        """
        self.start_date = start_date
        self.end_date = end_date if end_date else datetime.now().strftime('%Y-%m-%d')
        self.benchmark = benchmark
        self.lookback_periods = lookback_periods
        self.vol_lookback = vol_lookback
        self.rebalance_freq = rebalance_freq
        self.initial_capital = initial_capital
        
        # Parámetros para la gestión de riesgo
        self.max_position_size = 0.05  # Máximo 5% en una posición
        self.max_sector_exposure = 0.25  # Máximo 25% en un sector
        self.max_leverage = 1.0  # Sin apalancamiento
        
        # Parámetros para el modelo HMM
        self.n_regimes = 3  # Número de regímenes de mercado
        self.hmm_lookback = 252  # Datos para entrenar el HMM
        
        # Parámetros para el ensemble
        self.ensemble_models = []
        self.meta_model = None
        
        # Datos
        self.prices = None
        self.returns = None
        self.benchmark_returns = None
        self.sectors = None
        self.market_cap = None
        self.regimes = None
        self.signals = {}
        self.positions = None
        self.portfolio_value = None
        self.trades = None
        
        # Métricas
        self.metrics = {}
        
    def load_data(self):
        """Carga datos de precios para el universo de acciones del S&P 500"""
        try:
            # Obtener lista de tickers del S&P 500 desde Wikipedia
            sp500_tickers = self._get_sp500_tickers()
            
            # Añadir el benchmark
            if self.benchmark not in sp500_tickers:
                sp500_tickers.append(self.benchmark)
            
            # Descargar datos con un buffer para cálculos de lookback
            buffer_start = (pd.to_datetime(self.start_date) - pd.Timedelta(days=max(self.lookback_periods) + 50)).strftime('%Y-%m-%d')
            
            # Descargar datos de precios
            prices = yf.download(sp500_tickers, start=buffer_start, end=self.end_date)['Close']
            
            # Obtener datos de sectores y capitalización de mercado
            sectors = {}
            market_cap = {}
            
            for ticker in tqdm(sp500_tickers, desc="Obteniendo datos fundamentales"):
                try:
                    stock = yf.Ticker(ticker)
                    info = stock.info
                    sectors[ticker] = info.get('sector', 'Unknown')
                    market_cap[ticker] = info.get('marketCap', np.nan)
                    time.sleep(0.1)  # Evitar límites de API
                except Exception as e:
                    logging.error(f"Error obteniendo datos para {ticker}: {str(e)}")
                    sectors[ticker] = 'Unknown'
                    market_cap[ticker] = np.nan
            
            # Convertir a DataFrames
            self.sectors = pd.Series(sectors)
            self.market_cap = pd.Series(market_cap)
            
            # Filtrar fechas para el período de interés
            self.prices = prices.loc[self.start_date:self.end_date]
            
            # Calcular retornos
            self._calculate_returns()
            
            # Identificar regímenes de mercado
            self._identify_market_regimes()
            
            logging.info(f"Datos cargados exitosamente: {len(self.prices.columns)} activos desde {self.prices.index[0]} hasta {self.prices.index[-1]}")
            
        except Exception as e:
            logging.error(f"Error en load_data: {str(e)}", exc_info=True)
            raise
    
    def _get_sp500_tickers(self):
        """Obtiene la lista de tickers del S&P 500 desde Wikipedia"""
        try:
            url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
            response = requests.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')
            table = soup.find('table', {'class': 'wikitable sortable'})
            
            tickers = []
            for row in table.findAll('tr')[1:]:
                ticker = row.findAll('td')[0].text.strip()
                tickers.append(ticker.replace('.', '-'))  # Ajustar formato para yfinance
                
            return tickers
        except Exception as e:
            logging.error(f"Error obteniendo tickers del S&P 500: {str(e)}", exc_info=True)
            # Fallback a una lista reducida si hay error
            return ['^GSPC', 'AAPL', 'MSFT', 'AMZN', 'GOOGL', 'META', 'TSLA', 'BRK-B', 'JNJ', 'V', 'PG', 'JPM']
    
    def _calculate_returns(self):
        """Calcula los retornos diarios para todos los activos"""
        try:
            # Calcular retornos diarios
            self.returns = self.prices.pct_change().shift(1)  # Shift para evitar look-ahead bias
            
            # Extraer retornos del benchmark
            if self.benchmark in self.returns.columns:
                self.benchmark_returns = self.returns[self.benchmark]
            else:
                logging.warning(f"Benchmark {self.benchmark} no encontrado, usando el primer activo como benchmark")
                self.benchmark_returns = self.returns.iloc[:, 0]
                
        except Exception as e:
            logging.error(f"Error en _calculate_returns: {str(e)}", exc_info=True)
            raise
    
    def _identify_market_regimes(self):
        """Identifica regímenes de mercado usando HMM"""
        try:
            # Usar retornos del benchmark para identificar regímenes
            benchmark_returns = self.benchmark_returns.dropna()
            
            if len(benchmark_returns) < self.hmm_lookback:
                logging.warning("Datos insuficientes para HMM, usando período completo")
                data = benchmark_returns.values.reshape(-1, 1)
            else:
                data = benchmark_returns.values.reshape(-1, 1)
            
            # Entrenar modelo HMM
            model = hmm.GaussianHMM(n_components=self.n_regimes, covariance_type="full", 
                                   n_iter=1000, random_state=42)
            model.fit(data)
            
            # Predecir estados
            hidden_states = model.predict(data)
            
            # Crear DataFrame con regímenes
            self.regimes = pd.Series(hidden_states, index=benchmark_returns.index)
            
            # Calcular características de cada régimen
            regime_stats = {}
            for i in range(self.n_regimes):
                regime_returns = benchmark_returns[self.regimes == i]
                regime_stats[i] = {
                    'mean': regime_returns.mean(),
                    'std': regime_returns.std(),
                    'count': len(regime_returns),
                    'sharpe': regime_returns.mean() / regime_returns.std() if regime_returns.std() > 0 else 0
                }
            
            # Ordenar regímenes por volatilidad (0=baja, 1=media, 2=alta)
            sorted_regimes = sorted(range(self.n_regimes), key=lambda i: regime_stats[i]['std'])
            regime_map = {old: new for new, old in enumerate(sorted_regimes)}
            self.regimes = self.regimes.map(regime_map)
            
            # Guardar estadísticas de regímenes
            regime_df = pd.DataFrame({
                'Régimen': range(self.n_regimes),
                'Retorno Medio': [regime_stats[i]['mean'] for i in sorted_regimes],
                'Volatilidad': [regime_stats[i]['std'] for i in sorted_regimes],
                'Sharpe': [regime_stats[i]['sharpe'] for i in sorted_regimes],
                'Días': [regime_stats[i]['count'] for i in sorted_regimes]
            })
            regime_df.to_csv('./artifacts/results/data/regimes_stats.csv', index=False)
            
            # Visualizar regímenes
            plt.figure(figsize=(12, 8))
            
            # Gráfico superior: precio del benchmark
            ax1 = plt.subplot(211)
            benchmark_price = self.prices[self.benchmark].reindex(self.regimes.index)
            for i in range(self.n_regimes):
                mask = self.regimes == i
                ax1.plot(benchmark_price.index[mask], benchmark_price[mask], '.',
                        label=f'Régimen {i}', markersize=4)
            ax1.set_ylabel('Precio')
            ax1.set_title('Regímenes de Mercado Identificados')
            ax1.legend()
            
            # Gráfico inferior: volatilidad
            ax2 = plt.subplot(212, sharex=ax1)
            volatility = benchmark_returns.rolling(21).std().reindex(self.regimes.index)
            for i in range(self.n_regimes):
                mask = self.regimes == i
                ax2.plot(volatility.index[mask], volatility[mask], '.',
                        label=f'Régimen {i}', markersize=4)
            ax2.set_ylabel('Volatilidad (21 días)')
            ax2.set_xlabel('Fecha')
            
            plt.tight_layout()
            plt.savefig('./artifacts/results/figures/market_regimes.png')
            plt.close()
            
            logging.info(f"Regímenes de mercado identificados: {self.n_regimes} regímenes")
            
        except Exception as e:
            logging.error(f"Error en _identify_market_regimes: {str(e)}", exc_info=True)
            # Si falla, crear un régimen único
            self.regimes = pd.Series(0, index=self.benchmark_returns.dropna().index)
    
    def _calculate_residual_returns(self):
        """Calcula retornos residuales ajustados por factores de mercado"""
        try:
            residual_returns = pd.DataFrame(index=self.returns.index, columns=self.returns.columns)
            
            # Usar retornos del benchmark como factor de mercado
            X = self.benchmark_returns.values.reshape(-1, 1)
            
            # Para cada activo, calcular retornos residuales
            for ticker in self.returns.columns:
                if ticker == self.benchmark:
                    residual_returns[ticker] = 0  # El benchmark no tiene residual respecto a sí mismo
                    continue
                
                # Obtener retornos del activo
                y = self.returns[ticker].values
                
                # Eliminar NaNs
                mask = ~np.isnan(X.flatten()) & ~np.isnan(y)
                if np.sum(mask) < 30:  # Mínimo de datos para regresión
                    residual_returns[ticker] = np.nan
                    continue
                
                # Ajustar modelo de regresión
                model = LinearRegression()
                model.fit(X[mask], y[mask])
                
                # Calcular residuales
                y_pred = model.predict(X)
                residuals = y - y_pred
                
                # Guardar residuales
                residual_returns.loc[:, ticker] = pd.Series(residuals, index=self.returns.index)
            
            return residual_returns
            
        except Exception as e:
            logging.error(f"Error en _calculate_residual_returns: {str(e)}", exc_info=True)
            return pd.DataFrame(np.nan, index=self.returns.index, columns=self.returns.columns)
    
    def _calculate_momentum_signals(self):
        """Calcula señales de momentum para diferentes horizontes temporales"""
        try:
            # Calcular retornos residuales
            residual_returns = self._calculate_residual_returns()
            
            # Inicializar diccionario para señales
            momentum_signals = {}
            
            # Calcular momentum para cada período
            for period in self.lookback_periods:
                # Momentum clásico (retornos acumulados)
                raw_momentum = self.prices.pct_change(period).shift(1)  # Shift para evitar look-ahead bias
                
                # Momentum residual (retornos residuales acumulados)
                residual_momentum = residual_returns.rolling(period).sum().shift(1)
                
                # Momentum ajustado por volatilidad
                vol = self.returns.rolling(self.vol_lookback).std().shift(1)
                vol_adj_momentum = raw_momentum / vol
                
                # Guardar señales
                momentum_signals[f'raw_{period}'] = raw_momentum
                momentum_signals[f'residual_{period}'] = residual_momentum
                momentum_signals[f'vol_adj_{period}'] = vol_adj_momentum
            
            # Calcular tendencia de volatilidad
            vol_trend = self.returns.rolling(self.vol_lookback).std().diff(20).shift(1)
            vol_trend_signal = -1 * np.sign(vol_trend)  # Negativo cuando la volatilidad aumenta
            momentum_signals['vol_trend'] = vol_trend_signal
            
            # Calcular señal de régimen
            regime_signal = pd.DataFrame(index=self.returns.index, columns=self.returns.columns)
            
            # Mapear regímenes a señales (-1 para alta volatilidad, 0 para media, 1 para baja)
            regime_map = {0: 1, 1: 0, 2: -1}  # Régimen 0=baja vol, 1=media vol, 2=alta vol
            
            for ticker in self.returns.columns:
                # Reindexar regímenes al índice de retornos
                reindexed_regimes = self.regimes.reindex(self.returns.index)
                # Mapear regímenes a señales
                regime_signal[ticker] = reindexed_regimes.map(regime_map).fillna(0)
            
            momentum_signals['regime'] = regime_signal
            
            return momentum_signals
            
        except Exception as e:
            logging.error(f"Error en _calculate_momentum_signals: {str(e)}", exc_info=True)
            return {}
    
    def _normalize_signals(self, signals_dict):
        """Normaliza las señales usando winsorización dinámica basada en MAD"""
        try:
            normalized_signals = {}
            
            for name, signal in signals_dict.items():
                if signal is None or signal.empty:
                    continue
                
                # Crear copia para no modificar el original
                norm_signal = signal.copy()
                
                # Para cada fecha, normalizar cross-sectionally
                for date in norm_signal.index:
                    values = norm_signal.loc[date].dropna()
                    
                    if len(values) < 5:  # Necesitamos suficientes datos
                        continue
                    
                    # Calcular mediana y MAD
                    median = values.median()
                    mad = stats.median_abs_deviation(values.values, nan_policy='omit')
                    
                    if mad == 0:  # Evitar división por cero
                        mad = np.std(values)
                        if mad == 0:
                            continue
                    
                    # Winsorizar a 3 MADs
                    lower_bound = median - 3 * mad
                    upper_bound = median + 3 * mad
                    
                    # Aplicar winsorización
                    norm_signal.loc[date] = np.clip(norm_signal.loc[date], lower_bound, upper_bound)
                    
                    # Z-score
                    norm_signal.loc[date] = (norm_signal.loc[date] - norm_signal.loc[date].mean()) / norm_signal.loc[date].std()
                
                # Guardar señal normalizada
                normalized_signals[name] = norm_signal
            
            return normalized_signals
            
        except Exception as e:
            logging.error(f"Error en _normalize_signals: {str(e)}", exc_info=True)
            return signals_dict
    
    def _train_ensemble_models(self, X, y, train_indices, test_indices):
        """Entrena modelos de ensemble para combinar señales"""
        try:
            # Inicializar modelos base
            base_models = [
                ('rf', RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42)),
                ('gb', GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42)),
                ('lr', LogisticRegression(random_state=42))
            ]
            
            # Entrenar modelos base
            base_predictions = {}
            for name, model in base_models:
                # Entrenar en datos de entrenamiento
                model.fit(X.iloc[train_indices], y.iloc[train_indices])
                
                # Predecir en datos de prueba
                if hasattr(model, 'predict_proba'):
                    preds = model.predict_proba(X.iloc[test_indices])[:, 1]
                else:
                    preds = model.predict(X.iloc[test_indices])
                
                base_predictions[name] = preds
                
                # Guardar modelo
                self.ensemble_models.append((name, model))
            
            # Crear meta-features para el meta-modelo
            meta_features = pd.DataFrame(base_predictions)
            
            # Entrenar meta-modelo
            self.meta_model = LinearRegression()
            self.meta_model.fit(meta_features, y.iloc[test_indices])
            
            logging.info(f"Ensemble entrenado con {len(base_models)} modelos base")
            
        except Exception as e:
            logging.error(f"Error en _train_ensemble_models: {str(e)}", exc_info=True)
            # Fallback a un modelo simple
            self.ensemble_models = [('lr', LinearRegression())]
            self.meta_model = None
    
    def _combine_signals(self, normalized_signals):
        """Combina señales usando un enfoque de ensemble"""
        try:
            # Crear DataFrame para señales combinadas
            combined_signal = pd.DataFrame(index=self.returns.index, columns=self.returns.columns)
            
            # Si no hay señales, devolver DataFrame vacío
            if not normalized_signals:
                return combined_signal
            
            # Preparar datos para ensemble
            X_data = {}
            
            # Para cada activo, combinar señales
            for ticker in self.returns.columns:
                # Extraer señales para este activo
                ticker_signals = {}
                for name, signal in normalized_signals.items():
                    if ticker in signal.columns:
                        ticker_signals[name] = signal[ticker]
                
                # Crear DataFrame de características
                X = pd.DataFrame(ticker_signals)
                
                # Crear variable objetivo (retornos futuros)
                y = self.returns[ticker].shift(-1)  # Retornos del día siguiente
                
                # Eliminar filas con NaN
                mask = ~X.isna().any(axis=1) & ~y.isna()
                X = X[mask]
                y = y[mask]
                
                # Si no hay suficientes datos, continuar
                if len(X) < 252:
                    continue
                
                # Guardar datos para este activo
                X_data[ticker] = (X, y)
            
            # Si no hay datos para ensemble, usar promedio simple
            if not X_data:
                # Promediar todas las señales
                signals_df = pd.DataFrame({name: signal.mean(axis=1) 
                                         for name, signal in normalized_signals.items()})
                combined_signal = signals_df.mean(axis=1).reindex(combined_signal.index)
                return combined_signal
            
            # Entrenar ensemble usando validación temporal
            for ticker, (X, y) in X_data.items():
                # Crear splits temporales
                tscv = TimeSeriesSplit(n_splits=5)
                
                # Obtener último split para entrenar ensemble
                for train_idx, test_idx in tscv.split(X):
                    train_indices, test_indices = train_idx, test_idx
                
                # Entrenar ensemble
                self._train_ensemble_models(X, y, train_indices, test_indices)
                
                # Generar predicciones para todo el período
                base_preds = {}
                for name, model in self.ensemble_models:
                    if hasattr(model, 'predict_proba'):
                        preds = model.predict_proba(X)[:, 1]
                    else:
                        preds = model.predict(X)
                    base_preds[name] = preds
                
                # Combinar predicciones con meta-modelo
                meta_features = pd.DataFrame(base_preds)
                if self.meta_model:
                    final_preds = self.meta_model.predict(meta_features)
                else:
                    final_preds = meta_features.mean(axis=1)
                
                # Guardar predicciones en el índice original
                combined_signal.loc[X.index, ticker] = final_preds
            
            # Rellenar valores faltantes
            combined_signal = combined_signal.fillna(0)
            
            return combined_signal
            
        except Exception as e:
            logging.error(f"Error en _combine_signals: {str(e)}", exc_info=True)
            return pd.DataFrame(0, index=self.returns.index, columns=self.returns.columns)
    
    def _calculate_position_sizes(self, combined_signal):
        """Calcula tamaños de posición basados en señales y restricciones de riesgo"""
        try:
            # Inicializar DataFrame de posiciones
            position_sizes = pd.DataFrame(0, index=combined_signal.index, columns=combined_signal.columns)
            
            # Para cada fecha, calcular tamaños de posición
            for date in position_sizes.index:
                # Obtener señales para esta fecha
                signals = combined_signal.loc[date].dropna()
                
                if len(signals) == 0:
                    continue
                
                # Ordenar activos por señal
                sorted_assets = signals.sort_values(ascending=False)
                
                # Inicializar exposiciones por sector
                sector_exposure = {}
                for sector in self.sectors.unique():
                    sector_exposure[sector] = 0
                
                # Asignar posiciones
                for ticker, signal in sorted_assets.items():
                    # Si la señal es cercana a cero, no tomar posición
                    if abs(signal) < 0.1:
                        position_sizes.loc[date, ticker] = 0
                        continue
                    
                    # Determinar dirección (long/short)
                    direction = np.sign(signal)
                    
                    # Calcular tamaño base de posición
                    base_size = min(self.max_position_size, abs(signal) / 3)
                    
                    # Ajustar por volatilidad
                    if ticker in self.returns.columns:
                        vol = self.returns[ticker].rolling(63).std().loc[date]
                        if not np.isnan(vol) and vol > 0:
                            vol_scalar = 0.01 / vol  # Objetivo de volatilidad del 1%
                            base_size = min(base_size, self.max_position_size * vol_scalar)
                    
                    # Verificar restricciones de sector
                    sector = self.sectors.get(ticker, 'Unknown')
                    if sector_exposure[sector] + base_size > self.max_sector_exposure:
                        # Limitar exposición al sector
                        available_exposure = max(0, self.max_sector_exposure - sector_exposure[sector])
                        base_size = min(base_size, available_exposure)
                    
                    # Actualizar exposición del sector
                    sector_exposure[sector] += base_size
                    
                    # Asignar tamaño final
                    position_sizes.loc[date, ticker] = direction * base_size
                
                # Normalizar para que la suma de valores absolutos sea 1 (100% exposición)
                total_exposure = position_sizes.loc[date].abs().sum()
                if total_exposure > self.max_leverage:
                    position_sizes.loc[date] = position_sizes.loc[date] * (self.max_leverage / total_exposure)
            
            return position_sizes
            
        except Exception as e:
            logging.error(f"Error en _calculate_position_sizes: {str(e)}", exc_info=True)
            return pd.DataFrame(0, index=combined_signal.index, columns=combined_signal.columns)
    
    def _execute_trades(self, position_sizes):
        """Ejecuta operaciones basadas en tamaños de posición objetivo"""
        try:
            # Inicializar DataFrame de posiciones actuales
            current_positions = pd.DataFrame(0, index=position_sizes.index, columns=position_sizes.columns)
            
            # Inicializar DataFrame para el valor del portafolio
            portfolio_value = pd.Series(self.initial_capital, index=position_sizes.index)
            
            # Inicializar DataFrame para operaciones
            trades = pd.DataFrame(columns=['date', 'ticker', 'action', 'price', 'shares', 'value', 'commission'])
            
            # Inicializar efectivo disponible
            cash = self.initial_capital
            
            # Para cada fecha de rebalanceo
            rebalance_dates = pd.date_range(start=position_sizes.index[0], end=position_sizes.index[-1], freq=self.rebalance_freq)
            rebalance_dates = rebalance_dates[rebalance_dates.isin(position_sizes.index)]
            
            for i, current_date in enumerate(rebalance_dates):
                # Obtener precios actuales
                if current_date not in self.prices.index:
                    # Encontrar la fecha más cercana
                    closest_date = self.prices.index[self.prices.index.get_indexer([current_date], method='nearest')[0]]
                    current_prices = self.prices.loc[closest_date]
                else:
                    current_prices = self.prices.loc[current_date]
                
                # Calcular valor actual de las posiciones
                position_value = 0
                for ticker in current_positions.columns:
                    if ticker in current_prices.index:
                        position_value += current_positions.loc[current_date, ticker] * current_prices[ticker]
                
                # Actualizar valor del portafolio
                current_portfolio_value = cash + position_value
                portfolio_value.loc[current_date] = current_portfolio_value
                
                # Obtener posiciones objetivo
                target_positions_pct = position_sizes.loc[current_date]
                
                # Convertir porcentajes a valores monetarios
                target_positions_value = target_positions_pct * current_portfolio_value
                
                # Para cada activo, ejecutar operaciones
                for ticker in target_positions_value.index:
                    if ticker not in current_prices.index or np.isnan(current_prices[ticker]):
                        continue
                    
                    # Calcular posición actual en valor monetario
                    current_position_value = current_positions.loc[current_date, ticker] * current_prices[ticker]
                    
                    # Calcular diferencia
                    trade_value = target_positions_value[ticker] - current_position_value
                    
                    # Si la diferencia es pequeña, no operar (banda de no-transacción)
                    no_trade_threshold = 0.001 * current_portfolio_value  # 0.1% del portafolio
                    if abs(trade_value) < no_trade_threshold:
                        continue
                    
                    # Calcular número de acciones a operar
                    shares = trade_value / current_prices[ticker]
                    
                    # Calcular comisión (0.1% del valor de la operación)
                    commission = abs(trade_value) * 0.001
                    
                    # Actualizar efectivo
                    cash -= (trade_value + commission)
                    
                    # Actualizar posiciones
                    new_position = current_positions.loc[current_date, ticker] + shares
                    
                    # Actualizar posiciones para fechas futuras
                    if i < len(rebalance_dates) - 1:
                        future_dates = rebalance_dates[i+1:]
                        current_positions.loc[future_dates, ticker] = new_position
                    
                    # Registrar operación
                    action = 'BUY' if shares > 0 else 'SELL'
                    trades = trades.append({
                        'date': current_date,
                        'ticker': ticker,
                        'action': action,
                        'price': current_prices[ticker],
                        'shares': abs(shares),
                        'value': abs(trade_value),
                        'commission': commission
                    }, ignore_index=True)
            
            # Guardar resultados
            self.positions = current_positions
            self.portfolio_value = portfolio_value
            self.trades = trades
            
            return current_positions, portfolio_value, trades
            
        except Exception as e:
            logging.error(f"Error en _execute_trades: {str(e)}", exc_info=True)
            return None, None, None
    
    def _calculate_performance_metrics(self):
        """Calcula métricas de rendimiento del portafolio"""
        try:
            # Calcular retornos diarios del portafolio
            portfolio_returns = self.portfolio_value.pct_change().dropna()
            
            # Calcular retornos del benchmark
            benchmark_returns = self.benchmark_returns.reindex(portfolio_returns.index)
            
            # Calcular métricas básicas
            total_return = (self.portfolio_value.iloc[-1] / self.portfolio_value.iloc[0]) - 1
            annual_return = (1 + total_return) ** (252 / len(portfolio_returns)) - 1
            annual_volatility = portfolio_returns.std() * np.sqrt(252)
            sharpe_ratio = annual_return / annual_volatility if annual_volatility > 0 else 0
            
            # Calcular drawdown
            cumulative_returns = (1 + portfolio_returns).cumprod()
            running_max = cumulative_returns.cummax()
            drawdown = (cumulative_returns / running_max) - 1
            max_drawdown = drawdown.min()
            
            # Calcular beta y alpha
            cov_matrix = np.cov(portfolio_returns.fillna(0), benchmark_returns.fillna(0))
            beta = cov_matrix[0, 1] / cov_matrix[1, 1] if cov_matrix[1, 1] > 0 else 1
            alpha = annual_return - beta * benchmark_returns.mean() * 252
            
            # Calcular ratio de información
            tracking_error = (portfolio_returns - benchmark_returns).std() * np.sqrt(252)
            information_ratio = (annual_return - benchmark_returns.mean() * 252) / tracking_error if tracking_error > 0 else 0
            
            # Calcular ratio de Sortino
            downside_returns = portfolio_returns[portfolio_returns < 0]
            downside_deviation = downside_returns.std() * np.sqrt(252)
            sortino_ratio = annual_return / downside_deviation if downside_deviation > 0 else 0
            
            # Calcular ratio de Calmar
            calmar_ratio = annual_return / abs(max_drawdown) if max_drawdown < 0 else 0
            
            # Calcular métricas de operaciones
            total_trades = len(self.trades)
            total_commission = self.trades['commission'].sum()
            commission_pct = total_commission / self.initial_capital
            
            # Guardar métricas
            metrics = {
                'total_return': total_return,
                'annual_return': annual_return,
                'annual_volatility': annual_volatility,
                'sharpe_ratio': sharpe_ratio,
                'max_drawdown': max_drawdown,
                'beta': beta,
                'alpha': alpha,
                'information_ratio': information_ratio,
                'sortino_ratio': sortino_ratio,
                'calmar_ratio': calmar_ratio,
                'total_trades': total_trades,
                'total_commission': total_commission,
                'commission_pct': commission_pct
            }
            
            self.metrics = metrics
            
            # Guardar métricas en CSV
            metrics_df = pd.DataFrame([metrics])
            metrics_df.to_csv('./artifacts/results/data/performance_metrics.csv', index=False)
            
            # Calcular retornos por régimen
            regime_returns = {}
            for regime in range(self.n_regimes):
                regime_mask = self.regimes.reindex(portfolio_returns.index) == regime
                if regime_mask.sum() > 0:
                    regime_portfolio_returns = portfolio_returns[regime_mask]
                    regime_benchmark_returns = benchmark_returns[regime_mask]
                    
                    regime_metrics = {
                        'return': regime_portfolio_returns.mean() * 252,
                        'volatility': regime_portfolio_returns.std() * np.sqrt(252),
                        'sharpe': regime_portfolio_returns.mean() / regime_portfolio_returns.std() * np.sqrt(252) if regime_portfolio_returns.std() > 0 else 0,
                        'beta': np.cov(regime_portfolio_returns, regime_benchmark_returns)[0, 1] / np.var(regime_benchmark_returns) if np.var(regime_benchmark_returns) > 0 else 1,
                        'days': regime_mask.sum()
                    }
                    regime_returns[regime] = regime_metrics
            
            # Guardar métricas por régimen
            regime_metrics_df = pd.DataFrame(regime_returns).T
            regime_metrics_df.index.name = 'regime'
            regime_metrics_df.to_csv('./artifacts/results/data/regime_performance.csv')
            
            return metrics
            
        except Exception as e:
            logging.error(f"Error en _calculate_performance_metrics: {str(e)}", exc_info=True)
            return {}
    
    def _plot_results(self):
        """Genera gráficos de resultados"""
        try:
            # Calcular retornos acumulados
            portfolio_returns = self.portfolio_value.pct_change().dropna()
            benchmark_returns = self.benchmark_returns.reindex(portfolio_returns.index)
            
            cumulative_portfolio = (1 + portfolio_returns).cumprod()
            cumulative_benchmark = (1 + benchmark_returns).cumprod()
            
            # Gráfico 1: Valor del portafolio vs Benchmark
            plt.figure(figsize=(12, 8))
            plt.plot(cumulative_portfolio.index, cumulative_portfolio, label='Estrategia')
            plt.plot(cumulative_benchmark.index, cumulative_benchmark, label='Benchmark')
            plt.title('Rendimiento Acumulado')
            plt.xlabel('Fecha')
            plt.ylabel('Valor (normalizado)')
            plt.legend()
            plt.grid(True)
            plt.savefig('./artifacts/results/figures/cumulative_returns.png')
            plt.close()
            
            # Gráfico 2: Drawdown
            plt.figure(figsize=(12, 6))
            
            # Calcular drawdown
            running_max = cumulative_portfolio.cummax()
            drawdown = (cumulative_portfolio / running_max) - 1
            
            running_max_bench = cumulative_benchmark.cummax()
            drawdown_bench = (cumulative_benchmark / running_max_bench) - 1
            
            plt.plot(drawdown.index, drawdown, label='Estrategia')
            plt.plot(drawdown_bench.index, drawdown_bench, label='Benchmark')
            plt.title('Drawdown')
            plt.xlabel('Fecha')
            plt.ylabel('Drawdown')
            plt.legend()
            plt.grid(True)
            plt.savefig('./artifacts/results/figures/drawdown.png')
            plt.close()
            
            # Gráfico 3: Retornos por régimen
            plt.figure(figsize=(10, 6))
            
            # Agrupar retornos por régimen
            regime_returns = {}
            for regime in range(self.n_regimes):
                regime_mask = self.regimes.reindex(portfolio_returns.index) == regime
                if regime_mask.sum() > 0:
                    regime_returns[f'Régimen {regime}'] = portfolio_returns[regime_mask].mean() * 252
                    
            # Crear gráfico de barras
            bars = plt.bar(regime_returns.keys(), regime_returns.values())
            
            # Añadir etiquetas
            for bar in bars:
                height = bar.get_height()
                plt.text(bar.get_x() + bar.get_width()/2., height,
                        f'{height:.2%}',
                        ha='center', va='bottom')
            
            plt.title('Retorno Anualizado por Régimen')
            plt.ylabel('Retorno Anualizado')
            plt.grid(axis='y')
            plt.savefig('./artifacts/results/figures/regime_returns.png')
            plt.close()
            
            # Gráfico 4: Exposición por sector a lo largo del tiempo
            plt.figure(figsize=(12, 8))
            
            # Calcular exposición por sector
            sector_exposure = {}
            for date in self.positions.index:
                for sector in self.sectors.unique():
                    sector_tickers = self.sectors[self.sectors == sector].index
                    exposure = 0
                    for ticker in sector_tickers:
                        if ticker in self.positions.columns:
                            exposure += self.positions.loc[date, ticker]
                    
                    if sector not in sector_exposure:
                        sector_exposure[sector] = []
                    
                    sector_exposure[sector].append(exposure)
            
            # Crear DataFrame de exposición por sector
            sector_exposure_df = pd.DataFrame(sector_exposure, index=self.positions.index)
            
            # Graficar exposición por sector
            sector_exposure_df.plot(kind='area', stacked=True, figsize=(12, 8))
            plt.title('Exposición por Sector')
            plt.xlabel('Fecha')
            plt.ylabel('Exposición')
            plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))
            plt.grid(True)
            plt.savefig('./artifacts/results/figures/sector_exposure.png')
            plt.close()
            
            # Gráfico 5: Distribución de retornos
            plt.figure(figsize=(12, 6))
            
            sns.histplot(portfolio_returns, kde=True, stat='density', label='Estrategia')
            sns.histplot(benchmark_returns, kde=True, stat='density', label='Benchmark')
            
            plt.title('Distribución de Retornos Diarios')
            plt.xlabel('Retorno Diario')
            plt.ylabel('Densidad')
            plt.legend()
            plt.grid(True)
            plt.savefig('./artifacts/results/figures/return_distribution.png')
            plt.close()
            
        except Exception as e:
            logging.error(f"Error en _plot_results: {str(e)}", exc_info=True)
    
    def run_strategy(self):
        """Ejecuta la estrategia completa"""
        try:
            # Cargar datos
            self.load_data()
            
            # Calcular señales de momentum
            momentum_signals = self._calculate_momentum_signals()
            
            # Normalizar señales
            normalized_signals = self._normalize_signals(momentum_signals)
            
            # Combinar señales
            combined_signal = self._combine_signals(normalized_signals)
            
            # Calcular tamaños de posición
            position_sizes = self._calculate_position_sizes(combined_signal)
            
            # Ejecutar operaciones
            self._execute_trades(position_sizes)
            
            # Calcular métricas de rendimiento
            self._calculate_performance_metrics()
            
            # Generar gráficos
            self._plot_results()
            
            # Guardar señales
            combined_signal.to_csv('./artifacts/results/data/combined_signals.csv')
            position_sizes.to_csv('./artifacts/results/data/position_sizes.csv')
            
            # Guardar operaciones
            self.trades.to_csv('./artifacts/results/data/trades.csv', index=False)
            
            # Guardar valor del portafolio
            self.portfolio_value.to_csv('./artifacts/results/data/portfolio_value.csv')
            
            logging.info("Estrategia ejecutada exitosamente")
            
            return self.metrics
            
        except Exception as e:
            logging.error(f"Error en run_strategy: {str(e)}", exc_info=True)
            raise
    
    def backtest(self, start_date=None, end_date=None):
        """Ejecuta un backtest de la estrategia en un período específico"""
        original_start = self.start_date
        original_end = self.end_date
        
        try:
            # Actualizar fechas si se proporcionan
            if start_date:
                self.start_date = start_date
            if end_date:
                self.end_date = end_date
            
            # Ejecutar estrategia
            metrics = self.run_strategy()
            
            # Restaurar fechas originales
            self.start_date = original_start
            self.end_date = original_end
            
            return metrics
            
        except Exception as e:
            # Restaurar fechas originales en caso de error
            self.start_date = original_start
            self.end_date = original_end
            
            logging.error(f"Error en backtest: {str(e)}", exc_info=True)
            raise
    
    def walk_forward_test(self, train_years=3, test_months=6, start_date=None, end_date=None):
        """Ejecuta un test walk-forward con ventanas móviles"""
        try:
            # Determinar fechas de inicio y fin
            if not start_date:
                start_date = self.start_date
            if not end_date:
                end_date = self.end_date
            
            start = pd.to_datetime(start_date)
            end = pd.to_datetime(end_date)
            
            # Crear ventanas de tiempo
            current_train_start = start
            current_train_end = current_train_start + pd.DateOffset(years=train_years)
            current_test_end = current_train_end + pd.DateOffset(months=test_months)
            
            results = []
            
            # Para cada ventana
            while current_test_end <= end:
                # Convertir fechas a string
                train_start_str = current_train_start.strftime('%Y-%m-%d')
                train_end_str = current_train_end.strftime('%Y-%m-%d')
                test_end_str = current_test_end.strftime('%Y-%m-%d')
                
                logging.info(f"Walk-forward: Entrenando {train_start_str} a {train_end_str}, Probando hasta {test_end_str}")
                
                # Entrenar en período de entrenamiento
                self.backtest(start_date=train_start_str, end_date=train_end_str)
                
                # Guardar modelos y parámetros entrenados
                trained_ensemble = self.ensemble_models
                trained_meta = self.meta_model
                
                # Probar en período de prueba
                test_metrics = self.backtest(start_date=train_end_str, end_date=test_end_str)
                
                # Guardar resultados
                results.append({
                    'train_start': train_start_str,
                    'train_end': train_end_str,
                    'test_end': test_end_str,
                    'sharpe': test_metrics.get('sharpe_ratio', 0),
                    'return': test_metrics.get('total_return', 0),
                    'drawdown': test_metrics.get('max_drawdown', 0),
                    'alpha': test_metrics.get('alpha', 0)
                })
                
                # Avanzar ventana
                current_train_start = current_train_end
                current_train_end = current_train_start + pd.DateOffset(years=train_years)
                current_test_end = current_train_end + pd.DateOffset(months=test_months)
            
            # Crear DataFrame de resultados
            results_df = pd.DataFrame(results)
            results_df.to_csv('./artifacts/results/data/walk_forward_results.csv', index=False)
            
            # Graficar resultados
            plt.figure(figsize=(12, 8))
            
            plt.subplot(2, 1, 1)
            plt.plot(range(len(results)), [r['sharpe'] for r in results], marker='o')
            plt.title('Sharpe Ratio por Ventana')
            plt.grid(True)
            
            plt.subplot(2, 1, 2)
            plt.plot(range(len(results)), [r['return'] for r in results], marker='o')
            plt.title('Retorno por Ventana')
            plt.grid(True)
            
            plt.tight_layout()
            plt.savefig('./artifacts/results/figures/walk_forward_results.png')
            plt.close()
            
            return results_df
            
        except Exception as e:
            logging.error(f"Error en walk_forward_test: {str(e)}", exc_info=True)
            raise

# Ejecutar estrategia
if __name__ == "__main__":
    try:
        # Crear instancia de la estrategia
        strategy = AdaptiveMomentumStrategy(
            start_date='2015-01-01',
            end_date='2023-01-01',
            benchmark='^GSPC',
            lookback_periods=[20, 60, 120, 252],
            vol_lookback=63,
            rebalance_freq='W-FRI',
            initial_capital=1000000
        )
        
        # Ejecutar estrategia
        metrics = strategy.run_strategy()
        
        # Ejecutar walk-forward test
        strategy.walk_forward_test(train_years=2, test_months=6)
        
        print("Estrategia ejecutada exitosamente")
        print(f"Sharpe Ratio: {metrics.get('sharpe_ratio', 0):.2f}")
        print(f"Retorno Anual: {metrics.get('annual_return', 0):.2%}")
        print(f"Drawdown Máximo: {metrics.get('max_drawdown', 0):.2%}")
        
    except Exception as e:
        logging.error(f"Error ejecutando estrategia: {str(e)}", exc_info=True)
        print(f"Error: {str(e)}")
```


--- revisor_codigo (STEP 13.1) ---
INPUT 1 (user):
=== REVISIÓN DE CÓDIGO ===

IDEA CONCEPTUAL (sintetizada):
# IDEA FINAL: ESTRATEGIA DE MOMENTUM ADAPTATIVO CON REGÍMENES DE MERCADO Y CONTROL DINÁMICO DE RIESGO

## Descripción

Esta estrategia combina momentum residual multi-horizonte con identificación de regímenes de mercado y gestión dinámica del riesgo para capturar alpha en el S&P 500 mientras minimiza drawdowns. El enfoque integra análisis cuantitativo avanzado con controles de riesgo adaptativos para crear un sistema de trading robusto que se ajusta a diferentes condiciones de mercado.

## Características principales

### 1. Sistema de Momentum Residual Multi-Horizonte

- **Cálculo de rendimientos residuales**:
  - Análisis de múltiples horizontes temporales (5, 21, 63, 126 días)
  - Ajuste por modelo multifactorial: Rendimiento_Residual = Rendimiento_Real - (Exposición_Factores * Rendimiento_Factores)
  - Factores incluidos: Mercado, Tamaño, Valor, Momentum, Calidad
  - Señales contrarias para horizontes cortos (1-5 días) para capturar reversión a la media
  - Señales de momentum direccional para horizontes medianos/largos (>21 días)

- **Normalización adaptativa**:
  - Normalización por volatilidad específica usando desviación estándar móvil adaptativa
  - Winsorización dinámica basada en desviaciones MAD para mitigar outliers:
    - Umbral_Superior = mediana + 3 * MAD
    - Umbral_Inferior = mediana - 3 * MAD

- **Filtro de tendencia de largo plazo**:
  - Comparación del precio actual con media móvil de 200 días
  - Ajuste de exposición al momentum de corto plazo según dirección de tendencia de largo plazo

### 2. Identificación de Regímenes de Mercado (Modelo HMM Suavizado)

- **Modelo de Markov oculto con 3 estados**:
  - Régimen 1: Mercado en tendencia (baja volatilidad, autocorrelación positiva)
  - Régimen 2: Mercado en rango (baja volatilidad, autocorrelación negativa)
  - Régimen 3: Mercado en crisis (alta volatilidad, correlaciones inestables)

- **Variables de observación para inferencia de régimen**:
  - Volatilidad realizada (ATR de 21 días / precio)
  - Autocorrelación de rendimientos (ventana de 21 días)
  - Ratio de días positivos/negativos (ventana de 21 días)
  - Divergencia de volatilidad implícita vs. realizada (cuando disponible)
  - Variables macroeconómicas auxiliares:
    - Pendiente de la curva de rendimiento (10Y-2Y)
    - Spreads de crédito (BBB-Treasury)

- **Enfoque de régimen suavizado**:
  - Utilización de probabilidades de pertenencia a cada régimen en lugar de clasificación discreta
  - Señal_Final = Σ(Prob_Régimen_i * Señal_Régimen_i)
  - Recalibración trimestral del modelo HMM con validación cruzada para selección óptima de estados

### 3. Sistema de Ensemble con Stacking

- **Generación de múltiples modelos**:
  - 10-15 variantes de la estrategia con diferentes técnicas y parámetros
  - Entrenamiento en diferentes subconjuntos de datos históricos (block bootstrap)
  - Diversificación de métodos: modelos basados en momentum, reversión, volatilidad, etc.

- **Implementación de stacking**:
  - Meta-modelo (regresión regularizada) para combinar predicciones de modelos base
  - Captura de interacciones entre modelos para mejorar precisión
  - Pruning dinámico: eliminación temporal de modelos con rendimiento persistentemente pobre

- **Adaptación por validación cruzada temporal**:
  - Walk-forward validation con ventanas de entrenamiento de 2 años y prueba de 6 meses
  - Actualización trimestral de pesos del ensemble basada en rendimiento reciente

### 4. Gestión Dinámica del Riesgo

- **Control de drawdown adaptativo**:
  - Exposición = Exposición_Base * (1 - (Drawdown_Actual/Drawdown_Máximo_Tolerado)^2)
  - Drawdown_Máximo_Tolerado = 15% (calibrable)
  - Recuperación gradual post-reducción por drawdown

- **VaR condicional (CVaR/Expected Shortfall)**:
  - Limitación de exposición basada en CVaR estimado bajo diferentes escenarios
  - Protección mejorada contra eventos extremos

- **Filtro de tendencia de volatilidad**:
  - Ratio de volatilidad actual (21 días) vs. volatilidad de largo plazo (63 días)
  - Reducción de exposición cuando volatilidad_actual/volatilidad_tendencia > 1.2
  - Reducción adicional cuando la derivada de la volatilidad es positiva

- **Stops dinámicos contextuales**:
  - Stop-loss basado en ATR * multiplicador específico por régimen:
    - Régimen tendencia: 3.0 * ATR
    - Régimen rango: 2.0 * ATR
    - Régimen crisis: 1.5 * ATR
  - Stops basados en tiempo: reducción de posiciones sin comportamiento esperado después de n días

### 5. Optimización de Ejecución

- **Banda de no-transacción adaptativa**:
  - Umbral mínimo = máx(0.5%, costos_transacción * 3)
  - Ejecución solo cuando el cambio en posición deseada > umbral mínimo
  - Ajuste dinámico según volatilidad reciente

- **Timing de ejecución optimizado**:
  - Rebalanceo semanal (viernes cierre) como base
  - Análisis de patrones estacionales intraweek para día óptimo de ejecución
  - Monitoreo continuo para stops y condiciones extremas

- **Simulación de órdenes realista**:
  - Modelado de slippage basado en volatilidad y volumen
  - Incorporación de costos de financiamiento para posiciones apalancadas

## Implementación

### 1. Cálculo de Señales

```python
# Pseudocódigo para cálculo de señales
def calcular_senales(precios, factores, parametros):
    # Calcular rendimientos para diferentes horizontes
    rendimientos = {}
    for horizonte in [5, 21, 63, 126]:
        rendimientos[horizonte] = calcular_rendimiento(precios, horizonte)
    
    # Calcular exposición a factores y rendimientos residuales
    rendimientos_residuales = {}
    for horizonte, rend in rendimientos.items():
        exposicion = calcular_exposicion_factores(rend, factores, horizonte)
        rendimientos_residuales[horizonte] = rend - np.dot(exposicion, factores)
    
    # Normalizar señales
    senales_normalizadas = {}
    for horizonte, rend_res in rendimientos_residuales.items():
        vol = calcular_volatilidad_adaptativa(rend_res, horizonte)
        senales_normalizadas[horizonte] = rend_res / vol
        
        # Aplicar winsorización
        mad = calcular_mad(senales_normalizadas[horizonte])
        mediana = np.median(senales_normalizadas[horizonte])
        umbral_sup = mediana + 3 * mad
        umbral_inf = mediana - 3 * mad
        senales_normalizadas[horizonte] = np.clip(senales_normalizadas[horizonte], umbral_inf, umbral_sup)
    
    # Invertir señal para horizontes cortos (reversión a la media)
    if horizonte <= 5:
        senales_normalizadas[horizonte] = -senales_normalizadas[horizonte]
    
    return senales_normalizadas
```

### 2. Identificación de Regímenes

```python
# Pseudocódigo para identificación de regímenes
def identificar_regimenes(precios, datos_macro, parametros):
    # Calcular features para el HMM
    features = []
    
    # Volatilidad realizada
    atr = calcular_atr(precios, 21)
    features.append(atr / precios)
    
    # Autocorrelación
    autocorr = calcular_autocorrelacion(precios, 21)
    features.append(autocorr)
    
    # Ratio días positivos/negativos
    ratio_dias = calcular_ratio_dias_pos_neg(precios, 21)
    features.append(ratio_dias)
    
    # Variables macro
    pendiente_curva = datos_macro['10Y'] - datos_macro['2Y']
    features.append(pendiente_curva)
    spread_credito = datos_macro['BBB'] - datos_macro['Treasury']
    features.append(spread_credito)
    
    # Entrenar/actualizar modelo HMM
    if necesita_recalibracion():
        modelo_hmm = entrenar_hmm(features_historicos, n_estados=3)
    
    # Obtener probabilidades de régimen
    prob_regimenes = modelo_hmm.predict_proba(features)
    
    return prob_regimenes
```

### 3. Combinación de Señales y Gestión de Riesgo

```python
# Pseudocódigo para combinación de señales y gestión de riesgo
def combinar_senales(senales, prob_regimenes, estado_portfolio, parametros):
    # Pesos base por horizonte según régimen
    pesos_horizonte = {}
    for horizonte in senales.keys():
        pesos_horizonte[horizonte] = 0
        for i, prob in enumerate(prob_regimenes):
            if i == 0:  # Régimen tendencia
                if horizonte >= 21:
                    pesos_horizonte[horizonte] += prob * 0.4  # Mayor peso a medio/largo plazo
                else:
                    pesos_horizonte[horizonte] += prob * 0.1
            elif i == 1:  # Régimen rango
                if horizonte <= 5:
                    pesos_horizonte[horizonte] += prob * 0.5  # Mayor peso a corto plazo (reversión)
                else:
                    pesos_horizonte[horizonte] += prob * 0.1
            else:  # Régimen crisis
                pesos_horizonte[horizonte] += prob * 0.2  # Reducción general
    
    # Combinar señales ponderadas
    senal_combinada = 0
    for horizonte, senal in senales.items():
        senal_combinada += senal * pesos_horizonte[horizonte]
    
    # Aplicar control de drawdown
    drawdown_actual = calcular_drawdown(estado_portfolio)
    drawdown_max = parametros['drawdown_maximo']
    factor_drawdown = 1 - (drawdown_actual/drawdown_max)**2
    factor_drawdown = max(0, min(1, factor_drawdown))
    
    # Aplicar filtro de volatilidad
    vol_actual = calcular_volatilidad(precios, 21)
    vol_tendencia = calcular_volatilidad(precios, 63)
    ratio_vol = vol_actual / vol_tendencia
    factor_vol = 1.0
    if ratio_vol > 1.2:
        factor_vol = 1.2 / ratio_vol
    
    # Calcular exposición final
    exposicion_base = parametros['exposicion_maxima']
    exposicion = exposicion_base * factor_drawdown * factor_vol
    
    # Transformar señal en posición usando función sigmoide
    k = parametros['sensibilidad_sigmoide']
    posicion = exposicion * (2 / (1 + np.exp(-k * senal_combinada)) - 1)
    
    return posicion
```

### 4. Backtesting y Validación

```python
# Pseudocódigo para backtesting y validación
def backtest(datos, parametros):
    resultados = []
    
    # Walk-forward validation
    for inicio_test, fin_test in generar_ventanas_test(datos):
        # Datos de entrenamiento hasta inicio_test
        datos_train = datos[:inicio_test]
        
        # Entrenar ensemble de modelos
        modelos = entrenar_ensemble(datos_train, parametros)
        
        # Evaluar en ventana de test
        for fecha in range(inicio_test, fin_test):
            # Obtener señales de cada modelo
            senales_modelos = []
            for modelo in modelos:
                senal = modelo.generar_senal(datos, fecha)
                senales_modelos.append(senal)
            
            # Combinar señales con meta-modelo
            senal_combinada = meta_modelo.predecir(senales_modelos)
            
            # Aplicar gestión de riesgo
            posicion = aplicar_gestion_riesgo(senal_combinada, estado_portfolio)
            
            # Simular ejecución con costos
            rendimiento = simular_ejecucion(posicion, datos, fecha, costos)
            
            resultados.append({
                'fecha': fecha,
                'posicion': posicion,
                'rendimiento': rendimiento
            })
    
    # Calcular métricas
    sharpe = calcular_sharpe(resultados)
    drawdown_max = calcular_max_drawdown(resultados)
    correlacion_sp500 = calcular_correlacion(resultados, sp500)
    
    return {
        'resultados': resultados,
        'metricas': {
            'sharpe': sharpe,
            'drawdown_max': drawdown_max,
            'correlacion_sp500': correlacion_sp500
        }
    }
```

### 5. Métricas Esperadas

- **Sharpe ratio esperado**: 1.2-1.5 (post-costos)
- **Drawdown máximo esperado**: 12-15%
- **Correlación con S&P 500**: 0.3-0.5 (diversificación significativa)
- **Ratio de captación alcista/bajista**: 0.7/0.3 (asimétrico)

### 6. Consideraciones de Implementación

- Utilizar datos de yfinance para precios y volúmenes
- Implementar manejo de errores robusto para datos faltantes o erróneos
- Actualizar parámetros del modelo trimestralmente
- Monitorear continuamente el rendimiento y las métricas de riesgo
- Realizar análisis de atribución de rendimiento mensual para verificar fuentes de alpha
- Documentar todas las decisiones de diseño y parámetros para facilitar auditoría y mejora continua

Esta estrategia equilibra sofisticación analítica con robustez operativa, adaptándose a diferentes condiciones de mercado mientras mantiene un control riguroso del riesgo.

IMPLEMENTACIÓN EN CÓDIGO (del desarrollador):
```python
import os
import logging
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import yfinance as yf
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import TimeSeriesSplit
from scipy import stats
from hmmlearn import hmm
import warnings
from datetime import datetime, timedelta
import requests
from bs4 import BeautifulSoup
import time
from tqdm import tqdm

# Crear directorios para resultados
os.makedirs('./artifacts/results', exist_ok=True)
os.makedirs('./artifacts/results/figures', exist_ok=True)
os.makedirs('./artifacts/results/data', exist_ok=True)

# Configurar logging
logging.basicConfig(
    filename='./artifacts/errors.txt',
    level=logging.ERROR,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# Ignorar advertencias
warnings.filterwarnings('ignore')

class AdaptiveMomentumStrategy:
    def __init__(self, start_date='2010-01-01', end_date=None, benchmark='^GSPC', 
                 lookback_periods=[20, 60, 120, 252], vol_lookback=63, 
                 rebalance_freq='W-FRI', initial_capital=1000000):
        """
        Inicializa la estrategia de momentum adaptativo con múltiples horizontes temporales
        
        Args:
            start_date (str): Fecha de inicio para los datos
            end_date (str): Fecha de fin para los datos (None = hoy)
            benchmark (str): Ticker del benchmark (S&P 500 por defecto)
            lookback_periods (list): Períodos para calcular momentum
            vol_lookback (int): Período para calcular volatilidad
            rebalance_freq (str): Frecuencia de rebalanceo
            initial_capital (float): Capital inicial
        """
        self.start_date = start_date
        self.end_date = end_date if end_date else datetime.now().strftime('%Y-%m-%d')
        self.benchmark = benchmark
        self.lookback_periods = lookback_periods
        self.vol_lookback = vol_lookback
        self.rebalance_freq = rebalance_freq
        self.initial_capital = initial_capital
        
        # Parámetros para la gestión de riesgo
        self.max_position_size = 0.05  # Máximo 5% en una posición
        self.max_sector_exposure = 0.25  # Máximo 25% en un sector
        self.max_leverage = 1.0  # Sin apalancamiento
        
        # Parámetros para el modelo HMM
        self.n_regimes = 3  # Número de regímenes de mercado
        self.hmm_lookback = 252  # Datos para entrenar el HMM
        
        # Parámetros para el ensemble
        self.ensemble_models = []
        self.meta_model = None
        
        # Datos
        self.prices = None
        self.returns = None
        self.benchmark_returns = None
        self.sectors = None
        self.market_cap = None
        self.regimes = None
        self.signals = {}
        self.positions = None
        self.portfolio_value = None
        self.trades = None
        
        # Métricas
        self.metrics = {}
        
    def load_data(self):
        """Carga datos de precios para el universo de acciones del S&P 500"""
        try:
            # Obtener lista de tickers del S&P 500 desde Wikipedia
            sp500_tickers = self._get_sp500_tickers()
            
            # Añadir el benchmark
            if self.benchmark not in sp500_tickers:
                sp500_tickers.append(self.benchmark)
            
            # Descargar datos con un buffer para cálculos de lookback
            buffer_start = (pd.to_datetime(self.start_date) - pd.Timedelta(days=max(self.lookback_periods) + 50)).strftime('%Y-%m-%d')
            
            # Descargar datos de precios
            prices = yf.download(sp500_tickers, start=buffer_start, end=self.end_date)['Close']
            
            # Obtener datos de sectores y capitalización de mercado
            sectors = {}
            market_cap = {}
            
            for ticker in tqdm(sp500_tickers, desc="Obteniendo datos fundamentales"):
                try:
                    stock = yf.Ticker(ticker)
                    info = stock.info
                    sectors[ticker] = info.get('sector', 'Unknown')
                    market_cap[ticker] = info.get('marketCap', np.nan)
                    time.sleep(0.1)  # Evitar límites de API
                except Exception as e:
                    logging.error(f"Error obteniendo datos para {ticker}: {str(e)}")
                    sectors[ticker] = 'Unknown'
                    market_cap[ticker] = np.nan
            
            # Convertir a DataFrames
            self.sectors = pd.Series(sectors)
            self.market_cap = pd.Series(market_cap)
            
            # Filtrar fechas para el período de interés
            self.prices = prices.loc[self.start_date:self.end_date]
            
            # Calcular retornos
            self._calculate_returns()
            
            # Identificar regímenes de mercado
            self._identify_market_regimes()
            
            logging.info(f"Datos cargados exitosamente: {len(self.prices.columns)} activos desde {self.prices.index[0]} hasta {self.prices.index[-1]}")
            
        except Exception as e:
            logging.error(f"Error en load_data: {str(e)}", exc_info=True)
            raise
    
    def _get_sp500_tickers(self):
        """Obtiene la lista de tickers del S&P 500 desde Wikipedia"""
        try:
            url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
            response = requests.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')
            table = soup.find('table', {'class': 'wikitable sortable'})
            
            tickers = []
            for row in table.findAll('tr')[1:]:
                ticker = row.findAll('td')[0].text.strip()
                tickers.append(ticker.replace('.', '-'))  # Ajustar formato para yfinance
                
            return tickers
        except Exception as e:
            logging.error(f"Error obteniendo tickers del S&P 500: {str(e)}", exc_info=True)
            # Fallback a una lista reducida si hay error
            return ['^GSPC', 'AAPL', 'MSFT', 'AMZN', 'GOOGL', 'META', 'TSLA', 'BRK-B', 'JNJ', 'V', 'PG', 'JPM']
    
    def _calculate_returns(self):
        """Calcula los retornos diarios para todos los activos"""
        try:
            # Calcular retornos diarios
            self.returns = self.prices.pct_change().shift(1)  # Shift para evitar look-ahead bias
            
            # Extraer retornos del benchmark
            if self.benchmark in self.returns.columns:
                self.benchmark_returns = self.returns[self.benchmark]
            else:
                logging.warning(f"Benchmark {self.benchmark} no encontrado, usando el primer activo como benchmark")
                self.benchmark_returns = self.returns.iloc[:, 0]
                
        except Exception as e:
            logging.error(f"Error en _calculate_returns: {str(e)}", exc_info=True)
            raise
    
    def _identify_market_regimes(self):
        """Identifica regímenes de mercado usando HMM"""
        try:
            # Usar retornos del benchmark para identificar regímenes
            benchmark_returns = self.benchmark_returns.dropna()
            
            if len(benchmark_returns) < self.hmm_lookback:
                logging.warning("Datos insuficientes para HMM, usando período completo")
                data = benchmark_returns.values.reshape(-1, 1)
            else:
                data = benchmark_returns.values.reshape(-1, 1)
            
            # Entrenar modelo HMM
            model = hmm.GaussianHMM(n_components=self.n_regimes, covariance_type="full", 
                                   n_iter=1000, random_state=42)
            model.fit(data)
            
            # Predecir estados
            hidden_states = model.predict(data)
            
            # Crear DataFrame con regímenes
            self.regimes = pd.Series(hidden_states, index=benchmark_returns.index)
            
            # Calcular características de cada régimen
            regime_stats = {}
            for i in range(self.n_regimes):
                regime_returns = benchmark_returns[self.regimes == i]
                regime_stats[i] = {
                    'mean': regime_returns.mean(),
                    'std': regime_returns.std(),
                    'count': len(regime_returns),
                    'sharpe': regime_returns.mean() / regime_returns.std() if regime_returns.std() > 0 else 0
                }
            
            # Ordenar regímenes por volatilidad (0=baja, 1=media, 2=alta)
            sorted_regimes = sorted(range(self.n_regimes), key=lambda i: regime_stats[i]['std'])
            regime_map = {old: new for new, old in enumerate(sorted_regimes)}
            self.regimes = self.regimes.map(regime_map)
            
            # Guardar estadísticas de regímenes
            regime_df = pd.DataFrame({
                'Régimen': range(self.n_regimes),
                'Retorno Medio': [regime_stats[i]['mean'] for i in sorted_regimes],
                'Volatilidad': [regime_stats[i]['std'] for i in sorted_regimes],
                'Sharpe': [regime_stats[i]['sharpe'] for i in sorted_regimes],
                'Días': [regime_stats[i]['count'] for i in sorted_regimes]
            })
            regime_df.to_csv('./artifacts/results/data/regimes_stats.csv', index=False)
            
            # Visualizar regímenes
            plt.figure(figsize=(12, 8))
            
            # Gráfico superior: precio del benchmark
            ax1 = plt.subplot(211)
            benchmark_price = self.prices[self.benchmark].reindex(self.regimes.index)
            for i in range(self.n_regimes):
                mask = self.regimes == i
                ax1.plot(benchmark_price.index[mask], benchmark_price[mask], '.',
                        label=f'Régimen {i}', markersize=4)
            ax1.set_ylabel('Precio')
            ax1.set_title('Regímenes de Mercado Identificados')
            ax1.legend()
            
            # Gráfico inferior: volatilidad
            ax2 = plt.subplot(212, sharex=ax1)
            volatility = benchmark_returns.rolling(21).std().reindex(self.regimes.index)
            for i in range(self.n_regimes):
                mask = self.regimes == i
                ax2.plot(volatility.index[mask], volatility[mask], '.',
                        label=f'Régimen {i}', markersize=4)
            ax2.set_ylabel('Volatilidad (21 días)')
            ax2.set_xlabel('Fecha')
            
            plt.tight_layout()
            plt.savefig('./artifacts/results/figures/market_regimes.png')
            plt.close()
            
            logging.info(f"Regímenes de mercado identificados: {self.n_regimes} regímenes")
            
        except Exception as e:
            logging.error(f"Error en _identify_market_regimes: {str(e)}", exc_info=True)
            # Si falla, crear un régimen único
            self.regimes = pd.Series(0, index=self.benchmark_returns.dropna().index)
    
    def _calculate_residual_returns(self):
        """Calcula retornos residuales ajustados por factores de mercado"""
        try:
            residual_returns = pd.DataFrame(index=self.returns.index, columns=self.returns.columns)
            
            # Usar retornos del benchmark como factor de mercado
            X = self.benchmark_returns.values.reshape(-1, 1)
            
            # Para cada activo, calcular retornos residuales
            for ticker in self.returns.columns:
                if ticker == self.benchmark:
                    residual_returns[ticker] = 0  # El benchmark no tiene residual respecto a sí mismo
                    continue
                
                # Obtener retornos del activo
                y = self.returns[ticker].values
                
                # Eliminar NaNs
                mask = ~np.isnan(X.flatten()) & ~np.isnan(y)
                if np.sum(mask) < 30:  # Mínimo de datos para regresión
                    residual_returns[ticker] = np.nan
                    continue
                
                # Ajustar modelo de regresión
                model = LinearRegression()
                model.fit(X[mask], y[mask])
                
                # Calcular residuales
                y_pred = model.predict(X)
                residuals = y - y_pred
                
                # Guardar residuales
                residual_returns.loc[:, ticker] = pd.Series(residuals, index=self.returns.index)
            
            return residual_returns
            
        except Exception as e:
            logging.error(f"Error en _calculate_residual_returns: {str(e)}", exc_info=True)
            return pd.DataFrame(np.nan, index=self.returns.index, columns=self.returns.columns)
    
    def _calculate_momentum_signals(self):
        """Calcula señales de momentum para diferentes horizontes temporales"""
        try:
            # Calcular retornos residuales
            residual_returns = self._calculate_residual_returns()
            
            # Inicializar diccionario para señales
            momentum_signals = {}
            
            # Calcular momentum para cada período
            for period in self.lookback_periods:
                # Momentum clásico (retornos acumulados)
                raw_momentum = self.prices.pct_change(period).shift(1)  # Shift para evitar look-ahead bias
                
                # Momentum residual (retornos residuales acumulados)
                residual_momentum = residual_returns.rolling(period).sum().shift(1)
                
                # Momentum ajustado por volatilidad
                vol = self.returns.rolling(self.vol_lookback).std().shift(1)
                vol_adj_momentum = raw_momentum / vol
                
                # Guardar señales
                momentum_signals[f'raw_{period}'] = raw_momentum
                momentum_signals[f'residual_{period}'] = residual_momentum
                momentum_signals[f'vol_adj_{period}'] = vol_adj_momentum
            
            # Calcular tendencia de volatilidad
            vol_trend = self.returns.rolling(self.vol_lookback).std().diff(20).shift(1)
            vol_trend_signal = -1 * np.sign(vol_trend)  # Negativo cuando la volatilidad aumenta
            momentum_signals['vol_trend'] = vol_trend_signal
            
            # Calcular señal de régimen
            regime_signal = pd.DataFrame(index=self.returns.index, columns=self.returns.columns)
            
            # Mapear regímenes a señales (-1 para alta volatilidad, 0 para media, 1 para baja)
            regime_map = {0: 1, 1: 0, 2: -1}  # Régimen 0=baja vol, 1=media vol, 2=alta vol
            
            for ticker in self.returns.columns:
                # Reindexar regímenes al índice de retornos
                reindexed_regimes = self.regimes.reindex(self.returns.index)
                # Mapear regímenes a señales
                regime_signal[ticker] = reindexed_regimes.map(regime_map).fillna(0)
            
            momentum_signals['regime'] = regime_signal
            
            return momentum_signals
            
        except Exception as e:
            logging.error(f"Error en _calculate_momentum_signals: {str(e)}", exc_info=True)
            return {}
    
    def _normalize_signals(self, signals_dict):
        """Normaliza las señales usando winsorización dinámica basada en MAD"""
        try:
            normalized_signals = {}
            
            for name, signal in signals_dict.items():
                if signal is None or signal.empty:
                    continue
                
                # Crear copia para no modificar el original
                norm_signal = signal.copy()
                
                # Para cada fecha, normalizar cross-sectionally
                for date in norm_signal.index:
                    values = norm_signal.loc[date].dropna()
                    
                    if len(values) < 5:  # Necesitamos suficientes datos
                        continue
                    
                    # Calcular mediana y MAD
                    median = values.median()
                    mad = stats.median_abs_deviation(values.values, nan_policy='omit')
                    
                    if mad == 0:  # Evitar división por cero
                        mad = np.std(values)
                        if mad == 0:
                            continue
                    
                    # Winsorizar a 3 MADs
                    lower_bound = median - 3 * mad
                    upper_bound = median + 3 * mad
                    
                    # Aplicar winsorización
                    norm_signal.loc[date] = np.clip(norm_signal.loc[date], lower_bound, upper_bound)
                    
                    # Z-score
                    norm_signal.loc[date] = (norm_signal.loc[date] - norm_signal.loc[date].mean()) / norm_signal.loc[date].std()
                
                # Guardar señal normalizada
                normalized_signals[name] = norm_signal
            
            return normalized_signals
            
        except Exception as e:
            logging.error(f"Error en _normalize_signals: {str(e)}", exc_info=True)
            return signals_dict
    
    def _train_ensemble_models(self, X, y, train_indices, test_indices):
        """Entrena modelos de ensemble para combinar señales"""
        try:
            # Inicializar modelos base
            base_models = [
                ('rf', RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42)),
                ('gb', GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42)),
                ('lr', LogisticRegression(random_state=42))
            ]
            
            # Entrenar modelos base
            base_predictions = {}
            for name, model in base_models:
                # Entrenar en datos de entrenamiento
                model.fit(X.iloc[train_indices], y.iloc[train_indices])
                
                # Predecir en datos de prueba
                if hasattr(model, 'predict_proba'):
                    preds = model.predict_proba(X.iloc[test_indices])[:, 1]
                else:
                    preds = model.predict(X.iloc[test_indices])
                
                base_predictions[name] = preds
                
                # Guardar modelo
                self.ensemble_models.append((name, model))
            
            # Crear meta-features para el meta-modelo
            meta_features = pd.DataFrame(base_predictions)
            
            # Entrenar meta-modelo
            self.meta_model = LinearRegression()
            self.meta_model.fit(meta_features, y.iloc[test_indices])
            
            logging.info(f"Ensemble entrenado con {len(base_models)} modelos base")
            
        except Exception as e:
            logging.error(f"Error en _train_ensemble_models: {str(e)}", exc_info=True)
            # Fallback a un modelo simple
            self.ensemble_models = [('lr', LinearRegression())]
            self.meta_model = None
    
    def _combine_signals(self, normalized_signals):
        """Combina señales usando un enfoque de ensemble"""
        try:
            # Crear DataFrame para señales combinadas
            combined_signal = pd.DataFrame(index=self.returns.index, columns=self.returns.columns)
            
            # Si no hay señales, devolver DataFrame vacío
            if not normalized_signals:
                return combined_signal
            
            # Preparar datos para ensemble
            X_data = {}
            
            # Para cada activo, combinar señales
            for ticker in self.returns.columns:
                # Extraer señales para este activo
                ticker_signals = {}
                for name, signal in normalized_signals.items():
                    if ticker in signal.columns:
                        ticker_signals[name] = signal[ticker]
                
                # Crear DataFrame de características
                X = pd.DataFrame(ticker_signals)
                
                # Crear variable objetivo (retornos futuros)
                y = self.returns[ticker].shift(-1)  # Retornos del día siguiente
                
                # Eliminar filas con NaN
                mask = ~X.isna().any(axis=1) & ~y.isna()
                X = X[mask]
                y = y[mask]
                
                # Si no hay suficientes datos, continuar
                if len(X) < 252:
                    continue
                
                # Guardar datos para este activo
                X_data[ticker] = (X, y)
            
            # Si no hay datos para ensemble, usar promedio simple
            if not X_data:
                # Promediar todas las señales
                signals_df = pd.DataFrame({name: signal.mean(axis=1) 
                                         for name, signal in normalized_signals.items()})
                combined_signal = signals_df.mean(axis=1).reindex(combined_signal.index)
                return combined_signal
            
            # Entrenar ensemble usando validación temporal
            for ticker, (X, y) in X_data.items():
                # Crear splits temporales
                tscv = TimeSeriesSplit(n_splits=5)
                
                # Obtener último split para entrenar ensemble
                for train_idx, test_idx in tscv.split(X):
                    train_indices, test_indices = train_idx, test_idx
                
                # Entrenar ensemble
                self._train_ensemble_models(X, y, train_indices, test_indices)
                
                # Generar predicciones para todo el período
                base_preds = {}
                for name, model in self.ensemble_models:
                    if hasattr(model, 'predict_proba'):
                        preds = model.predict_proba(X)[:, 1]
                    else:
                        preds = model.predict(X)
                    base_preds[name] = preds
                
                # Combinar predicciones con meta-modelo
                meta_features = pd.DataFrame(base_preds)
                if self.meta_model:
                    final_preds = self.meta_model.predict(meta_features)
                else:
                    final_preds = meta_features.mean(axis=1)
                
                # Guardar predicciones en el índice original
                combined_signal.loc[X.index, ticker] = final_preds
            
            # Rellenar valores faltantes
            combined_signal = combined_signal.fillna(0)
            
            return combined_signal
            
        except Exception as e:
            logging.error(f"Error en _combine_signals: {str(e)}", exc_info=True)
            return pd.DataFrame(0, index=self.returns.index, columns=self.returns.columns)
    
    def _calculate_position_sizes(self, combined_signal):
        """Calcula tamaños de posición basados en señales y restricciones de riesgo"""
        try:
            # Inicializar DataFrame de posiciones
            position_sizes = pd.DataFrame(0, index=combined_signal.index, columns=combined_signal.columns)
            
            # Para cada fecha, calcular tamaños de posición
            for date in position_sizes.index:
                # Obtener señales para esta fecha
                signals = combined_signal.loc[date].dropna()
                
                if len(signals) == 0:
                    continue
                
                # Ordenar activos por señal
                sorted_assets = signals.sort_values(ascending=False)
                
                # Inicializar exposiciones por sector
                sector_exposure = {}
                for sector in self.sectors.unique():
                    sector_exposure[sector] = 0
                
                # Asignar posiciones
                for ticker, signal in sorted_assets.items():
                    # Si la señal es cercana a cero, no tomar posición
                    if abs(signal) < 0.1:
                        position_sizes.loc[date, ticker] = 0
                        continue
                    
                    # Determinar dirección (long/short)
                    direction = np.sign(signal)
                    
                    # Calcular tamaño base de posición
                    base_size = min(self.max_position_size, abs(signal) / 3)
                    
                    # Ajustar por volatilidad
                    if ticker in self.returns.columns:
                        vol = self.returns[ticker].rolling(63).std().loc[date]
                        if not np.isnan(vol) and vol > 0:
                            vol_scalar = 0.01 / vol  # Objetivo de volatilidad del 1%
                            base_size = min(base_size, self.max_position_size * vol_scalar)
                    
                    # Verificar restricciones de sector
                    sector = self.sectors.get(ticker, 'Unknown')
                    if sector_exposure[sector] + base_size > self.max_sector_exposure:
                        # Limitar exposición al sector
                        available_exposure = max(0, self.max_sector_exposure - sector_exposure[sector])
                        base_size = min(base_size, available_exposure)
                    
                    # Actualizar exposición del sector
                    sector_exposure[sector] += base_size
                    
                    # Asignar tamaño final
                    position_sizes.loc[date, ticker] = direction * base_size
                
                # Normalizar para que la suma de valores absolutos sea 1 (100% exposición)
                total_exposure = position_sizes.loc[date].abs().sum()
                if total_exposure > self.max_leverage:
                    position_sizes.loc[date] = position_sizes.loc[date] * (self.max_leverage / total_exposure)
            
            return position_sizes
            
        except Exception as e:
            logging.error(f"Error en _calculate_position_sizes: {str(e)}", exc_info=True)
            return pd.DataFrame(0, index=combined_signal.index, columns=combined_signal.columns)
    
    def _execute_trades(self, position_sizes):
        """Ejecuta operaciones basadas en tamaños de posición objetivo"""
        try:
            # Inicializar DataFrame de posiciones actuales
            current_positions = pd.DataFrame(0, index=position_sizes.index, columns=position_sizes.columns)
            
            # Inicializar DataFrame para el valor del portafolio
            portfolio_value = pd.Series(self.initial_capital, index=position_sizes.index)
            
            # Inicializar DataFrame para operaciones
            trades = pd.DataFrame(columns=['date', 'ticker', 'action', 'price', 'shares', 'value', 'commission'])
            
            # Inicializar efectivo disponible
            cash = self.initial_capital
            
            # Para cada fecha de rebalanceo
            rebalance_dates = pd.date_range(start=position_sizes.index[0], end=position_sizes.index[-1], freq=self.rebalance_freq)
            rebalance_dates = rebalance_dates[rebalance_dates.isin(position_sizes.index)]
            
            for i, current_date in enumerate(rebalance_dates):
                # Obtener precios actuales
                if current_date not in self.prices.index:
                    # Encontrar la fecha más cercana
                    closest_date = self.prices.index[self.prices.index.get_indexer([current_date], method='nearest')[0]]
                    current_prices = self.prices.loc[closest_date]
                else:
                    current_prices = self.prices.loc[current_date]
                
                # Calcular valor actual de las posiciones
                position_value = 0
                for ticker in current_positions.columns:
                    if ticker in current_prices.index:
                        position_value += current_positions.loc[current_date, ticker] * current_prices[ticker]
                
                # Actualizar valor del portafolio
                current_portfolio_value = cash + position_value
                portfolio_value.loc[current_date] = current_portfolio_value
                
                # Obtener posiciones objetivo
                target_positions_pct = position_sizes.loc[current_date]
                
                # Convertir porcentajes a valores monetarios
                target_positions_value = target_positions_pct * current_portfolio_value
                
                # Para cada activo, ejecutar operaciones
                for ticker in target_positions_value.index:
                    if ticker not in current_prices.index or np.isnan(current_prices[ticker]):
                        continue
                    
                    # Calcular posición actual en valor monetario
                    current_position_value = current_positions.loc[current_date, ticker] * current_prices[ticker]
                    
                    # Calcular diferencia
                    trade_value = target_positions_value[ticker] - current_position_value
                    
                    # Si la diferencia es pequeña, no operar (banda de no-transacción)
                    no_trade_threshold = 0.001 * current_portfolio_value  # 0.1% del portafolio
                    if abs(trade_value) < no_trade_threshold:
                        continue
                    
                    # Calcular número de acciones a operar
                    shares = trade_value / current_prices[ticker]
                    
                    # Calcular comisión (0.1% del valor de la operación)
                    commission = abs(trade_value) * 0.001
                    
                    # Actualizar efectivo
                    cash -= (trade_value + commission)
                    
                    # Actualizar posiciones
                    new_position = current_positions.loc[current_date, ticker] + shares
                    
                    # Actualizar posiciones para fechas futuras
                    if i < len(rebalance_dates) - 1:
                        future_dates = rebalance_dates[i+1:]
                        current_positions.loc[future_dates, ticker] = new_position
                    
                    # Registrar operación
                    action = 'BUY' if shares > 0 else 'SELL'
                    trades = trades.append({
                        'date': current_date,
                        'ticker': ticker,
                        'action': action,
                        'price': current_prices[ticker],
                        'shares': abs(shares),
                        'value': abs(trade_value),
                        'commission': commission
                    }, ignore_index=True)
            
            # Guardar resultados
            self.positions = current_positions
            self.portfolio_value = portfolio_value
            self.trades = trades
            
            return current_positions, portfolio_value, trades
            
        except Exception as e:
            logging.error(f"Error en _execute_trades: {str(e)}", exc_info=True)
            return None, None, None
    
    def _calculate_performance_metrics(self):
        """Calcula métricas de rendimiento del portafolio"""
        try:
            # Calcular retornos diarios del portafolio
            portfolio_returns = self.portfolio_value.pct_change().dropna()
            
            # Calcular retornos del benchmark
            benchmark_returns = self.benchmark_returns.reindex(portfolio_returns.index)
            
            # Calcular métricas básicas
            total_return = (self.portfolio_value.iloc[-1] / self.portfolio_value.iloc[0]) - 1
            annual_return = (1 + total_return) ** (252 / len(portfolio_returns)) - 1
            annual_volatility = portfolio_returns.std() * np.sqrt(252)
            sharpe_ratio = annual_return / annual_volatility if annual_volatility > 0 else 0
            
            # Calcular drawdown
            cumulative_returns = (1 + portfolio_returns).cumprod()
            running_max = cumulative_returns.cummax()
            drawdown = (cumulative_returns / running_max) - 1
            max_drawdown = drawdown.min()
            
            # Calcular beta y alpha
            cov_matrix = np.cov(portfolio_returns.fillna(0), benchmark_returns.fillna(0))
            beta = cov_matrix[0, 1] / cov_matrix[1, 1] if cov_matrix[1, 1] > 0 else 1
            alpha = annual_return - beta * benchmark_returns.mean() * 252
            
            # Calcular ratio de información
            tracking_error = (portfolio_returns - benchmark_returns).std() * np.sqrt(252)
            information_ratio = (annual_return - benchmark_returns.mean() * 252) / tracking_error if tracking_error > 0 else 0
            
            # Calcular ratio de Sortino
            downside_returns = portfolio_returns[portfolio_returns < 0]
            downside_deviation = downside_returns.std() * np.sqrt(252)
            sortino_ratio = annual_return / downside_deviation if downside_deviation > 0 else 0
            
            # Calcular ratio de Calmar
            calmar_ratio = annual_return / abs(max_drawdown) if max_drawdown < 0 else 0
            
            # Calcular métricas de operaciones
            total_trades = len(self.trades)
            total_commission = self.trades['commission'].sum()
            commission_pct = total_commission / self.initial_capital
            
            # Guardar métricas
            metrics = {
                'total_return': total_return,
                'annual_return': annual_return,
                'annual_volatility': annual_volatility,
                'sharpe_ratio': sharpe_ratio,
                'max_drawdown': max_drawdown,
                'beta': beta,
                'alpha': alpha,
                'information_ratio': information_ratio,
                'sortino_ratio': sortino_ratio,
                'calmar_ratio': calmar_ratio,
                'total_trades': total_trades,
                'total_commission': total_commission,
                'commission_pct': commission_pct
            }
            
            self.metrics = metrics
            
            # Guardar métricas en CSV
            metrics_df = pd.DataFrame([metrics])
            metrics_df.to_csv('./artifacts/results/data/performance_metrics.csv', index=False)
            
            # Calcular retornos por régimen
            regime_returns = {}
            for regime in range(self.n_regimes):
                regime_mask = self.regimes.reindex(portfolio_returns.index) == regime
                if regime_mask.sum() > 0:
                    regime_portfolio_returns = portfolio_returns[regime_mask]
                    regime_benchmark_returns = benchmark_returns[regime_mask]
                    
                    regime_metrics = {
                        'return': regime_portfolio_returns.mean() * 252,
                        'volatility': regime_portfolio_returns.std() * np.sqrt(252),
                        'sharpe': regime_portfolio_returns.mean() / regime_portfolio_returns.std() * np.sqrt(252) if regime_portfolio_returns.std() > 0 else 0,
                        'beta': np.cov(regime_portfolio_returns, regime_benchmark_returns)[0, 1] / np.var(regime_benchmark_returns) if np.var(regime_benchmark_returns) > 0 else 1,
                        'days': regime_mask.sum()
                    }
                    regime_returns[regime] = regime_metrics
            
            # Guardar métricas por régimen
            regime_metrics_df = pd.DataFrame(regime_returns).T
            regime_metrics_df.index.name = 'regime'
            regime_metrics_df.to_csv('./artifacts/results/data/regime_performance.csv')
            
            return metrics
            
        except Exception as e:
            logging.error(f"Error en _calculate_performance_metrics: {str(e)}", exc_info=True)
            return {}
    
    def _plot_results(self):
        """Genera gráficos de resultados"""
        try:
            # Calcular retornos acumulados
            portfolio_returns = self.portfolio_value.pct_change().dropna()
            benchmark_returns = self.benchmark_returns.reindex(portfolio_returns.index)
            
            cumulative_portfolio = (1 + portfolio_returns).cumprod()
            cumulative_benchmark = (1 + benchmark_returns).cumprod()
            
            # Gráfico 1: Valor del portafolio vs Benchmark
            plt.figure(figsize=(12, 8))
            plt.plot(cumulative_portfolio.index, cumulative_portfolio, label='Estrategia')
            plt.plot(cumulative_benchmark.index, cumulative_benchmark, label='Benchmark')
            plt.title('Rendimiento Acumulado')
            plt.xlabel('Fecha')
            plt.ylabel('Valor (normalizado)')
            plt.legend()
            plt.grid(True)
            plt.savefig('./artifacts/results/figures/cumulative_returns.png')
            plt.close()
            
            # Gráfico 2: Drawdown
            plt.figure(figsize=(12, 6))
            
            # Calcular drawdown
            running_max = cumulative_portfolio.cummax()
            drawdown = (cumulative_portfolio / running_max) - 1
            
            running_max_bench = cumulative_benchmark.cummax()
            drawdown_bench = (cumulative_benchmark / running_max_bench) - 1
            
            plt.plot(drawdown.index, drawdown, label='Estrategia')
            plt.plot(drawdown_bench.index, drawdown_bench, label='Benchmark')
            plt.title('Drawdown')
            plt.xlabel('Fecha')
            plt.ylabel('Drawdown')
            plt.legend()
            plt.grid(True)
            plt.savefig('./artifacts/results/figures/drawdown.png')
            plt.close()
            
            # Gráfico 3: Retornos por régimen
            plt.figure(figsize=(10, 6))
            
            # Agrupar retornos por régimen
            regime_returns = {}
            for regime in range(self.n_regimes):
                regime_mask = self.regimes.reindex(portfolio_returns.index) == regime
                if regime_mask.sum() > 0:
                    regime_returns[f'Régimen {regime}'] = portfolio_returns[regime_mask].mean() * 252
                    
            # Crear gráfico de barras
            bars = plt.bar(regime_returns.keys(), regime_returns.values())
            
            # Añadir etiquetas
            for bar in bars:
                height = bar.get_height()
                plt.text(bar.get_x() + bar.get_width()/2., height,
                        f'{height:.2%}',
                        ha='center', va='bottom')
            
            plt.title('Retorno Anualizado por Régimen')
            plt.ylabel('Retorno Anualizado')
            plt.grid(axis='y')
            plt.savefig('./artifacts/results/figures/regime_returns.png')
            plt.close()
            
            # Gráfico 4: Exposición por sector a lo largo del tiempo
            plt.figure(figsize=(12, 8))
            
            # Calcular exposición por sector
            sector_exposure = {}
            for date in self.positions.index:
                for sector in self.sectors.unique():
                    sector_tickers = self.sectors[self.sectors == sector].index
                    exposure = 0
                    for ticker in sector_tickers:
                        if ticker in self.positions.columns:
                            exposure += self.positions.loc[date, ticker]
                    
                    if sector not in sector_exposure:
                        sector_exposure[sector] = []
                    
                    sector_exposure[sector].append(exposure)
            
            # Crear DataFrame de exposición por sector
            sector_exposure_df = pd.DataFrame(sector_exposure, index=self.positions.index)
            
            # Graficar exposición por sector
            sector_exposure_df.plot(kind='area', stacked=True, figsize=(12, 8))
            plt.title('Exposición por Sector')
            plt.xlabel('Fecha')
            plt.ylabel('Exposición')
            plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))
            plt.grid(True)
            plt.savefig('./artifacts/results/figures/sector_exposure.png')
            plt.close()
            
            # Gráfico 5: Distribución de retornos
            plt.figure(figsize=(12, 6))
            
            sns.histplot(portfolio_returns, kde=True, stat='density', label='Estrategia')
            sns.histplot(benchmark_returns, kde=True, stat='density', label='Benchmark')
            
            plt.title('Distribución de Retornos Diarios')
            plt.xlabel('Retorno Diario')
            plt.ylabel('Densidad')
            plt.legend()
            plt.grid(True)
            plt.savefig('./artifacts/results/figures/return_distribution.png')
            plt.close()
            
        except Exception as e:
            logging.error(f"Error en _plot_results: {str(e)}", exc_info=True)
    
    def run_strategy(self):
        """Ejecuta la estrategia completa"""
        try:
            # Cargar datos
            self.load_data()
            
            # Calcular señales de momentum
            momentum_signals = self._calculate_momentum_signals()
            
            # Normalizar señales
            normalized_signals = self._normalize_signals(momentum_signals)
            
            # Combinar señales
            combined_signal = self._combine_signals(normalized_signals)
            
            # Calcular tamaños de posición
            position_sizes = self._calculate_position_sizes(combined_signal)
            
            # Ejecutar operaciones
            self._execute_trades(position_sizes)
            
            # Calcular métricas de rendimiento
            self._calculate_performance_metrics()
            
            # Generar gráficos
            self._plot_results()
            
            # Guardar señales
            combined_signal.to_csv('./artifacts/results/data/combined_signals.csv')
            position_sizes.to_csv('./artifacts/results/data/position_sizes.csv')
            
            # Guardar operaciones
            self.trades.to_csv('./artifacts/results/data/trades.csv', index=False)
            
            # Guardar valor del portafolio
            self.portfolio_value.to_csv('./artifacts/results/data/portfolio_value.csv')
            
            logging.info("Estrategia ejecutada exitosamente")
            
            return self.metrics
            
        except Exception as e:
            logging.error(f"Error en run_strategy: {str(e)}", exc_info=True)
            raise
    
    def backtest(self, start_date=None, end_date=None):
        """Ejecuta un backtest de la estrategia en un período específico"""
        original_start = self.start_date
        original_end = self.end_date
        
        try:
            # Actualizar fechas si se proporcionan
            if start_date:
                self.start_date = start_date
            if end_date:
                self.end_date = end_date
            
            # Ejecutar estrategia
            metrics = self.run_strategy()
            
            # Restaurar fechas originales
            self.start_date = original_start
            self.end_date = original_end
            
            return metrics
            
        except Exception as e:
            # Restaurar fechas originales en caso de error
            self.start_date = original_start
            self.end_date = original_end
            
            logging.error(f"Error en backtest: {str(e)}", exc_info=True)
            raise
    
    def walk_forward_test(self, train_years=3, test_months=6, start_date=None, end_date=None):
        """Ejecuta un test walk-forward con ventanas móviles"""
        try:
            # Determinar fechas de inicio y fin
            if not start_date:
                start_date = self.start_date
            if not end_date:
                end_date = self.end_date
            
            start = pd.to_datetime(start_date)
            end = pd.to_datetime(end_date)
            
            # Crear ventanas de tiempo
            current_train_start = start
            current_train_end = current_train_start + pd.DateOffset(years=train_years)
            current_test_end = current_train_end + pd.DateOffset(months=test_months)
            
            results = []
            
            # Para cada ventana
            while current_test_end <= end:
                # Convertir fechas a string
                train_start_str = current_train_start.strftime('%Y-%m-%d')
                train_end_str = current_train_end.strftime('%Y-%m-%d')
                test_end_str = current_test_end.strftime('%Y-%m-%d')
                
                logging.info(f"Walk-forward: Entrenando {train_start_str} a {train_end_str}, Probando hasta {test_end_str}")
                
                # Entrenar en período de entrenamiento
                self.backtest(start_date=train_start_str, end_date=train_end_str)
                
                # Guardar modelos y parámetros entrenados
                trained_ensemble = self.ensemble_models
                trained_meta = self.meta_model
                
                # Probar en período de prueba
                test_metrics = self.backtest(start_date=train_end_str, end_date=test_end_str)
                
                # Guardar resultados
                results.append({
                    'train_start': train_start_str,
                    'train_end': train_end_str,
                    'test_end': test_end_str,
                    'sharpe': test_metrics.get('sharpe_ratio', 0),
                    'return': test_metrics.get('total_return', 0),
                    'drawdown': test_metrics.get('max_drawdown', 0),
                    'alpha': test_metrics.get('alpha', 0)
                })
                
                # Avanzar ventana
                current_train_start = current_train_end
                current_train_end = current_train_start + pd.DateOffset(years=train_years)
                current_test_end = current_train_end + pd.DateOffset(months=test_months)
            
            # Crear DataFrame de resultados
            results_df = pd.DataFrame(results)
            results_df.to_csv('./artifacts/results/data/walk_forward_results.csv', index=False)
            
            # Graficar resultados
            plt.figure(figsize=(12, 8))
            
            plt.subplot(2, 1, 1)
            plt.plot(range(len(results)), [r['sharpe'] for r in results], marker='o')
            plt.title('Sharpe Ratio por Ventana')
            plt.grid(True)
            
            plt.subplot(2, 1, 2)
            plt.plot(range(len(results)), [r['return'] for r in results], marker='o')
            plt.title('Retorno por Ventana')
            plt.grid(True)
            
            plt.tight_layout()
            plt.savefig('./artifacts/results/figures/walk_forward_results.png')
            plt.close()
            
            return results_df
            
        except Exception as e:
            logging.error(f"Error en walk_forward_test: {str(e)}", exc_info=True)
            raise

# Ejecutar estrategia
if __name__ == "__main__":
    try:
        # Crear instancia de la estrategia
        strategy = AdaptiveMomentumStrategy(
            start_date='2015-01-01',
            end_date='2023-01-01',
            benchmark='^GSPC',
            lookback_periods=[20, 60, 120, 252],
            vol_lookback=63,
            rebalance_freq='W-FRI',
            initial_capital=1000000
        )
        
        # Ejecutar estrategia
        metrics = strategy.run_strategy()
        
        # Ejecutar walk-forward test
        strategy.walk_forward_test(train_years=2, test_months=6)
        
        print("Estrategia ejecutada exitosamente")
        print(f"Sharpe Ratio: {metrics.get('sharpe_ratio', 0):.2f}")
        print(f"Retorno Anual: {metrics.get('annual_return', 0):.2%}")
        print(f"Drawdown Máximo: {metrics.get('max_drawdown', 0):.2%}")
        
    except Exception as e:
        logging.error(f"Error ejecutando estrategia: {str(e)}", exc_info=True)
        print(f"Error: {str(e)}")
```

Por favor analiza si el código implementa correctamente la idea conceptual y sugiere mejoras específicas.


RESPONSE:
# Revisión de Código: Estrategia de Momentum Adaptativo

## 1. ¿El código implementa correctamente la idea? Parcialmente

El código implementa muchos aspectos de la idea conceptual, pero tiene varias áreas problemáticas que necesitan mejoras para alinearse completamente con la propuesta original y evitar errores técnicos.

## 2. Sugerencias específicas:

1. **Problema de look-ahead bias en `_calculate_returns()`**: 
   ```python
   self.returns = self.prices.pct_change().shift(1)  # Shift para evitar look-ahead bias
   ```
   El shift(1) está mal aplicado. Esto está desplazando los retornos hacia el futuro, no hacia el pasado. Para evitar look-ahead bias, no deberías hacer shift de los retornos calculados, sino asegurarte de usar precios pasados para calcular señales.

2. **Error en el cálculo de residuales en `_calculate_residual_returns()`**:
   La implementación actual solo usa el benchmark como factor, pero la idea original especifica un modelo multifactorial que incluye Mercado, Tamaño, Valor, Momentum y Calidad. Deberías implementar un modelo más completo con estos factores.

3. **Problema de alineación temporal en `_identify_market_regimes()`**:
   El código no verifica correctamente que los datos de regímenes estén alineados con los datos de precios, lo que puede causar errores cuando se combinan señales.

4. **Falta de implementación de controles dinámicos de riesgo**:
   La idea original incluye un control de drawdown adaptativo con la fórmula:
   ```
   Exposición = Exposición_Base * (1 - (Drawdown_Actual/Drawdown_Máximo_Tolerado)^2)
   ```
   Pero en `_calculate_position_sizes()` no se implementa correctamente este control.

5. **Problema en `_train_ensemble_models()`**:
   La función entrena modelos con datos de prueba que luego se usan para predecir esos mismos datos, lo que causa data leakage. Deberías separar claramente los datos de entrenamiento y prueba.

6. **Falta de implementación de stops dinámicos contextuales**:
   La idea original menciona stops dinámicos basados en ATR con multiplicadores específicos por régimen, pero esto no está implementado en `_execute_trades()`.

7. **Problema en `_combine_signals()`**:
   La función entrena un ensemble para cada activo individualmente, lo que es ineficiente y puede llevar a overfitting. Sería mejor entrenar un único modelo para todos los activos.

8. **Error en la normalización de señales**:
   En `_normalize_signals()`, la normalización se hace por fecha, pero no se verifica si hay suficientes datos no-NaN para cada fecha, lo que puede causar errores cuando hay pocos datos.

9. **Falta de implementación de la banda de no-transacción adaptativa**:
   La idea original menciona un umbral mínimo adaptativo para transacciones, pero en `_execute_trades()` se usa un valor fijo de 0.1%.

10. **Problema en `walk_forward_test()`**:
    La función no reinicializa correctamente los modelos entre iteraciones, lo que puede causar contaminación de datos entre períodos de prueba.

11. **Falta de manejo adecuado de NaNs**:
    En varias partes del código (como en `_calculate_momentum_signals()`), no se manejan adecuadamente los valores NaN, lo que puede propagar errores a través del pipeline.

12. **Problema con el parámetro `auto_adjust` en yfinance**:
    El código no especifica explícitamente `auto_adjust=True` en la llamada a `yf.download()`, lo que podría causar inconsistencias si la versión de yfinance cambia.

13. **Falta de implementación de la winsorización dinámica basada en MAD**:
    Aunque se menciona en los comentarios, la implementación en `_normalize_signals()` no usa correctamente la desviación absoluta mediana (MAD).

14. **Problema en la gestión de sectores**:
    En `_calculate_position_sizes()`, se inicializa `sector_exposure` pero no se verifica si todos los sectores están presentes en `self.sectors.unique()`.

15. **Falta de implementación del filtro de tendencia de largo plazo**:
    La idea original menciona comparar el precio actual con la media móvil de 200 días, pero esto no está implementado.


--- desarrollador (STEP 14.1) ---
INPUT 1 (user):
# Revisión de Código: Estrategia de Momentum Adaptativo

## 1. ¿El código implementa correctamente la idea? Parcialmente

El código implementa muchos aspectos de la idea conceptual, pero tiene varias áreas problemáticas que necesitan mejoras para alinearse completamente con la propuesta original y evitar errores técnicos.

## 2. Sugerencias específicas:

1. **Problema de look-ahead bias en `_calculate_returns()`**: 
   ```python
   self.returns = self.prices.pct_change().shift(1)  # Shift para evitar look-ahead bias
   ```
   El shift(1) está mal aplicado. Esto está desplazando los retornos hacia el futuro, no hacia el pasado. Para evitar look-ahead bias, no deberías hacer shift de los retornos calculados, sino asegurarte de usar precios pasados para calcular señales.

2. **Error en el cálculo de residuales en `_calculate_residual_returns()`**:
   La implementación actual solo usa el benchmark como factor, pero la idea original especifica un modelo multifactorial que incluye Mercado, Tamaño, Valor, Momentum y Calidad. Deberías implementar un modelo más completo con estos factores.

3. **Problema de alineación temporal en `_identify_market_regimes()`**:
   El código no verifica correctamente que los datos de regímenes estén alineados con los datos de precios, lo que puede causar errores cuando se combinan señales.

4. **Falta de implementación de controles dinámicos de riesgo**:
   La idea original incluye un control de drawdown adaptativo con la fórmula:
   ```
   Exposición = Exposición_Base * (1 - (Drawdown_Actual/Drawdown_Máximo_Tolerado)^2)
   ```
   Pero en `_calculate_position_sizes()` no se implementa correctamente este control.

5. **Problema en `_train_ensemble_models()`**:
   La función entrena modelos con datos de prueba que luego se usan para predecir esos mismos datos, lo que causa data leakage. Deberías separar claramente los datos de entrenamiento y prueba.

6. **Falta de implementación de stops dinámicos contextuales**:
   La idea original menciona stops dinámicos basados en ATR con multiplicadores específicos por régimen, pero esto no está implementado en `_execute_trades()`.

7. **Problema en `_combine_signals()`**:
   La función entrena un ensemble para cada activo individualmente, lo que es ineficiente y puede llevar a overfitting. Sería mejor entrenar un único modelo para todos los activos.

8. **Error en la normalización de señales**:
   En `_normalize_signals()`, la normalización se hace por fecha, pero no se verifica si hay suficientes datos no-NaN para cada fecha, lo que puede causar errores cuando hay pocos datos.

9. **Falta de implementación de la banda de no-transacción adaptativa**:
   La idea original menciona un umbral mínimo adaptativo para transacciones, pero en `_execute_trades()` se usa un valor fijo de 0.1%.

10. **Problema en `walk_forward_test()`**:
    La función no reinicializa correctamente los modelos entre iteraciones, lo que puede causar contaminación de datos entre períodos de prueba.

11. **Falta de manejo adecuado de NaNs**:
    En varias partes del código (como en `_calculate_momentum_signals()`), no se manejan adecuadamente los valores NaN, lo que puede propagar errores a través del pipeline.

12. **Problema con el parámetro `auto_adjust` en yfinance**:
    El código no especifica explícitamente `auto_adjust=True` en la llamada a `yf.download()`, lo que podría causar inconsistencias si la versión de yfinance cambia.

13. **Falta de implementación de la winsorización dinámica basada en MAD**:
    Aunque se menciona en los comentarios, la implementación en `_normalize_signals()` no usa correctamente la desviación absoluta mediana (MAD).

14. **Problema en la gestión de sectores**:
    En `_calculate_position_sizes()`, se inicializa `sector_exposure` pero no se verifica si todos los sectores están presentes en `self.sectors.unique()`.

15. **Falta de implementación del filtro de tendencia de largo plazo**:
    La idea original menciona comparar el precio actual con la media móvil de 200 días, pero esto no está implementado.

RESPONSE:
```python
import os
import logging
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import yfinance as yf
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error
from scipy import stats
import warnings
from datetime import datetime, timedelta
import requests
from bs4 import BeautifulSoup
import time

# Crear directorios para resultados
os.makedirs('./artifacts/results', exist_ok=True)
os.makedirs('./artifacts/results/figures', exist_ok=True)
os.makedirs('./artifacts/results/data', exist_ok=True)

# Configurar logging
logging.basicConfig(
    filename='./artifacts/errors.txt',
    level=logging.ERROR,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# Ignorar advertencias
warnings.filterwarnings('ignore')

class AdaptiveMomentumStrategy:
    def __init__(self, start_date='2010-01-01', end_date=None, benchmark='^GSPC'):
        """
        Inicializa la estrategia de momentum adaptativo.
        
        Args:
            start_date (str): Fecha de inicio para los datos
            end_date (str): Fecha de fin para los datos (por defecto, fecha actual)
            benchmark (str): Símbolo del índice de referencia
        """
        self.start_date = start_date
        self.end_date = end_date if end_date else datetime.now().strftime('%Y-%m-%d')
        self.benchmark = benchmark
        self.lookback_periods = [20, 60, 120, 252]  # Períodos de momentum en días
        self.regime_lookback = 252  # Período para identificar regímenes
        self.vol_lookback = 63  # Período para calcular volatilidad (63 días ≈ 3 meses)
        self.max_drawdown_tolerance = 0.15  # Drawdown máximo tolerado (15%)
        self.sector_max_allocation = 0.25  # Exposición máxima por sector (25%)
        self.max_position_size = 0.05  # Tamaño máximo de posición individual (5%)
        self.transaction_threshold = 0.001  # Umbral mínimo para transacciones (0.1%)
        
        # Inicializar variables que se llenarán más tarde
        self.prices = None
        self.returns = None
        self.benchmark_prices = None
        self.benchmark_returns = None
        self.sectors = None
        self.momentum_signals = {}
        self.residual_returns = None
        self.market_regimes = None
        self.volatility = None
        self.combined_signals = None
        self.positions = None
        self.portfolio_returns = None
        self.portfolio_value = None
        self.trades = None
        self.performance_metrics = {}
        
        try:
            self._load_data()
        except Exception as e:
            logging.error(f"Error al cargar datos: {str(e)}", exc_info=True)
            raise
    
    def _load_data(self):
        """Carga los datos de precios del S&P 500 y el benchmark."""
        try:
            # Obtener lista de símbolos del S&P 500 desde Wikipedia
            sp500_symbols = self._get_sp500_symbols()
            
            # Descargar datos de precios para todos los símbolos
            print(f"Descargando datos para {len(sp500_symbols)} símbolos...")
            all_data = yf.download(sp500_symbols, start=self.start_date, end=self.end_date)
            
            # Extraer precios de cierre
            self.prices = all_data['Close']
            
            # Descargar datos del benchmark
            benchmark_data = yf.download(self.benchmark, start=self.start_date, end=self.end_date)
            self.benchmark_prices = benchmark_data['Close']
            
            # Obtener información de sectores
            self.sectors = self._get_sector_data(sp500_symbols)
            
            print(f"Datos cargados: {self.prices.shape[1]} símbolos, {self.prices.shape[0]} días")
        except Exception as e:
            logging.error(f"Error en _load_data: {str(e)}", exc_info=True)
            raise
    
    def _get_sp500_symbols(self):
        """Obtiene la lista de símbolos del S&P 500 desde Wikipedia."""
        try:
            url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
            response = requests.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')
            table = soup.find('table', {'class': 'wikitable sortable'})
            
            symbols = []
            for row in table.findAll('tr')[1:]:
                symbol = row.findAll('td')[0].text.strip()
                symbols.append(symbol)
            
            return symbols
        except Exception as e:
            logging.error(f"Error al obtener símbolos del S&P 500: {str(e)}", exc_info=True)
            # Fallback a una lista pequeña de símbolos conocidos
            return ['AAPL', 'MSFT', 'AMZN', 'GOOGL', 'FB', 'TSLA', 'BRK-B', 'JPM', 'JNJ', 'V']
    
    def _get_sector_data(self, symbols):
        """Obtiene información de sectores para los símbolos."""
        try:
            sectors = pd.Series(index=symbols)
            
            # Procesar en lotes para evitar límites de API
            batch_size = 100
            for i in range(0, len(symbols), batch_size):
                batch_symbols = symbols[i:i+batch_size]
                for symbol in batch_symbols:
                    try:
                        ticker = yf.Ticker(symbol)
                        sector = ticker.info.get('sector', 'Unknown')
                        sectors[symbol] = sector
                    except:
                        sectors[symbol] = 'Unknown'
                
                # Pausa para evitar límites de API
                time.sleep(1)
            
            return sectors
        except Exception as e:
            logging.error(f"Error al obtener datos de sectores: {str(e)}", exc_info=True)
            # Crear sectores aleatorios como fallback
            return pd.Series('Unknown', index=symbols)
    
    def _calculate_returns(self):
        """Calcula los retornos diarios de los precios."""
        try:
            # Calcular retornos diarios (t/t-1)
            self.returns = self.prices.pct_change()
            
            # Calcular retornos del benchmark
            self.benchmark_returns = self.benchmark_prices.pct_change()
        except Exception as e:
            logging.error(f"Error en _calculate_returns: {str(e)}", exc_info=True)
            raise
    
    def _calculate_momentum_signals(self):
        """Calcula señales de momentum para diferentes períodos."""
        try:
            for period in self.lookback_periods:
                # Calcular retorno acumulado para el período
                cum_returns = self.prices.pct_change(period)
                
                # Normalizar señales por fecha (cross-sectional)
                normalized_signals = cum_returns.apply(lambda x: (x - x.mean()) / x.std(), axis=1)
                
                # Winsorizar valores extremos (±3 desviaciones estándar)
                normalized_signals = normalized_signals.apply(
                    lambda x: np.clip(x, -3, 3), axis=1
                )
                
                # Almacenar señales
                self.momentum_signals[period] = normalized_signals
        except Exception as e:
            logging.error(f"Error en _calculate_momentum_signals: {str(e)}", exc_info=True)
            raise
    
    def _calculate_residual_returns(self):
        """Calcula retornos residuales después de ajustar por factores de mercado."""
        try:
            # Inicializar DataFrame para retornos residuales
            self.residual_returns = pd.DataFrame(index=self.returns.index, columns=self.returns.columns)
            
            # Para cada activo, calcular retornos residuales
            for symbol in self.returns.columns:
                # Obtener retornos del activo y del benchmark
                asset_returns = self.returns[symbol].copy()
                
                # Eliminar filas con NaN
                valid_data = pd.concat([asset_returns, self.benchmark_returns], axis=1).dropna()
                
                if len(valid_data) > 30:  # Asegurar suficientes datos
                    X = valid_data.iloc[:, 1].values.reshape(-1, 1)  # Benchmark returns
                    y = valid_data.iloc[:, 0].values  # Asset returns
                    
                    # Ajustar modelo de regresión lineal
                    model = LinearRegression()
                    model.fit(X, y)
                    
                    # Calcular residuos
                    predicted = model.predict(X)
                    residuals = y - predicted
                    
                    # Almacenar residuos en las fechas correspondientes
                    for i, date in enumerate(valid_data.index):
                        self.residual_returns.loc[date, symbol] = residuals[i]
        except Exception as e:
            logging.error(f"Error en _calculate_residual_returns: {str(e)}", exc_info=True)
            raise
    
    def _identify_market_regimes(self):
        """Identifica regímenes de mercado basados en tendencia y volatilidad."""
        try:
            # Calcular retornos acumulados de 252 días (1 año)
            annual_return = self.benchmark_prices.pct_change(self.regime_lookback)
            
            # Calcular volatilidad de 63 días (3 meses)
            rolling_std = self.benchmark_returns.rolling(self.vol_lookback).std() * np.sqrt(252)
            
            # Inicializar DataFrame para regímenes
            self.market_regimes = pd.DataFrame(index=self.prices.index)
            
            # Definir regímenes:
            # 1: Bull market (retorno alto, volatilidad baja)
            # 2: Bear market (retorno bajo, volatilidad alta)
            # 3: Sideways/low vol (retorno bajo, volatilidad baja)
            # 4: Volatile bull (retorno alto, volatilidad alta)
            
            # Calcular umbrales adaptativos
            return_threshold = annual_return.rolling(252).median()
            vol_threshold = rolling_std.rolling(252).median()
            
            # Asignar regímenes
            regimes = pd.Series(index=self.prices.index, dtype=int)
            
            for date in regimes.index:
                if pd.notna(annual_return.loc[date]) and pd.notna(rolling_std.loc[date]):
                    if pd.notna(return_threshold.loc[date]) and pd.notna(vol_threshold.loc[date]):
                        if annual_return.loc[date] > return_threshold.loc[date]:
                            if rolling_std.loc[date] > vol_threshold.loc[date]:
                                regimes.loc[date] = 4  # Volatile bull
                            else:
                                regimes.loc[date] = 1  # Bull market
                        else:
                            if rolling_std.loc[date] > vol_threshold.loc[date]:
                                regimes.loc[date] = 2  # Bear market
                            else:
                                regimes.loc[date] = 3  # Sideways/low vol
            
            # Llenar valores NaN con el régimen más reciente
            regimes = regimes.fillna(method='ffill')
            
            self.market_regimes['regime'] = regimes
            
            # Calcular volatilidad para ajustar posiciones
            self.volatility = rolling_std.copy()
        except Exception as e:
            logging.error(f"Error en _identify_market_regimes: {str(e)}", exc_info=True)
            raise
    
    def _train_ensemble_models(self):
        """Entrena modelos de ensemble para combinar señales de momentum."""
        try:
            # Crear DataFrame para almacenar señales combinadas
            self.combined_signals = pd.DataFrame(index=self.prices.index, columns=self.prices.columns)
            
            # Definir divisiones de tiempo para validación cruzada
            tscv = TimeSeriesSplit(n_splits=5)
            
            # Para cada activo, entrenar un modelo ensemble
            for symbol in self.prices.columns:
                # Preparar datos de entrenamiento
                X = pd.DataFrame(index=self.prices.index)
                
                # Añadir señales de momentum como features
                for period in self.lookback_periods:
                    X[f'momentum_{period}'] = self.momentum_signals[period][symbol]
                
                # Añadir retornos residuales como feature
                X['residual'] = self.residual_returns[symbol]
                
                # Añadir régimen de mercado como feature categórica
                X['regime'] = self.market_regimes['regime']
                
                # Convertir régimen a variables dummy
                regime_dummies = pd.get_dummies(X['regime'], prefix='regime')
                X = pd.concat([X.drop('regime', axis=1), regime_dummies], axis=1)
                
                # Eliminar filas con NaN
                X = X.dropna()
                
                if len(X) > 252:  # Asegurar suficientes datos (1 año)
                    # Definir target: retornos futuros a 5 días
                    y = self.returns[symbol].shift(-5).loc[X.index]
                    
                    # Eliminar NaN del target
                    valid_indices = ~y.isna()
                    X_valid = X.loc[valid_indices]
                    y_valid = y.loc[valid_indices]
                    
                    if len(X_valid) > 252:
                        # Entrenar modelos
                        rf = RandomForestRegressor(n_estimators=100, max_depth=3, random_state=42)
                        gb = GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42)
                        lr = LinearRegression()
                        
                        # Validación cruzada para evitar overfitting
                        predictions = pd.Series(index=X_valid.index, dtype=float)
                        
                        for train_idx, test_idx in tscv.split(X_valid):
                            X_train, X_test = X_valid.iloc[train_idx], X_valid.iloc[test_idx]
                            y_train, y_test = y_valid.iloc[train_idx], y_valid.iloc[test_idx]
                            
                            # Entrenar modelos
                            rf.fit(X_train, y_train)
                            gb.fit(X_train, y_train)
                            lr.fit(X_train, y_train)
                            
                            # Predecir
                            pred_rf = rf.predict(X_test)
                            pred_gb = gb.predict(X_test)
                            pred_lr = lr.predict(X_test)
                            
                            # Combinar predicciones (promedio simple)
                            ensemble_pred = (pred_rf + pred_gb + pred_lr) / 3
                            
                            # Almacenar predicciones
                            predictions.iloc[test_idx] = ensemble_pred
                        
                        # Normalizar predicciones
                        predictions = (predictions - predictions.mean()) / predictions.std()
                        
                        # Almacenar señales combinadas
                        self.combined_signals.loc[predictions.index, symbol] = predictions
        except Exception as e:
            logging.error(f"Error en _train_ensemble_models: {str(e)}", exc_info=True)
            raise
    
    def _normalize_signals(self):
        """Normaliza las señales combinadas usando winsorización adaptativa."""
        try:
            # Para cada fecha, normalizar señales entre activos
            for date in self.combined_signals.index:
                signals = self.combined_signals.loc[date].dropna()
                
                if len(signals) > 10:  # Asegurar suficientes datos
                    # Calcular MAD (Median Absolute Deviation)
                    median = signals.median()
                    mad = (signals - median).abs().median()
                    
                    # Winsorizar usando MAD (±3 MAD)
                    lower_bound = median - 3 * mad
                    upper_bound = median + 3 * mad
                    
                    # Aplicar límites
                    winsorized = signals.clip(lower=lower_bound, upper=upper_bound)
                    
                    # Normalizar a rango [-1, 1]
                    if winsorized.max() > winsorized.min():
                        normalized = 2 * (winsorized - winsorized.min()) / (winsorized.max() - winsorized.min()) - 1
                        self.combined_signals.loc[date, normalized.index] = normalized
        except Exception as e:
            logging.error(f"Error en _normalize_signals: {str(e)}", exc_info=True)
            raise
    
    def _calculate_position_sizes(self):
        """Calcula tamaños de posición basados en señales y restricciones."""
        try:
            # Inicializar DataFrame para posiciones
            self.positions = pd.DataFrame(0, index=self.prices.index, columns=self.prices.columns)
            
            # Calcular drawdown del portafolio
            portfolio_value = (1 + self.benchmark_returns).cumprod()
            rolling_max = portfolio_value.rolling(window=252, min_periods=1).max()
            drawdown = (portfolio_value / rolling_max) - 1
            
            # Para cada fecha, calcular posiciones
            for i, date in enumerate(self.positions.index[252:], 252):
                # Obtener señales para la fecha actual
                signals = self.combined_signals.loc[date].dropna()
                
                if len(signals) > 0:
                    # Ajustar exposición basada en drawdown
                    current_drawdown = abs(drawdown.loc[date])
                    drawdown_factor = max(0, 1 - (current_drawdown / self.max_drawdown_tolerance)**2)
                    
                    # Ajustar exposición basada en régimen de mercado
                    regime = self.market_regimes.loc[date, 'regime']
                    regime_factors = {1: 1.0, 2: 0.5, 3: 0.7, 4: 0.8}
                    regime_factor = regime_factors.get(regime, 0.5)
                    
                    # Calcular factor de exposición total
                    exposure_factor = drawdown_factor * regime_factor
                    
                    # Filtrar señales por umbral
                    filtered_signals = signals[abs(signals) > self.transaction_threshold]
                    
                    if len(filtered_signals) > 0:
                        # Inicializar exposición por sector
                        sector_exposure = {sector: 0 for sector in self.sectors.unique()}
                        
                        # Ordenar señales por magnitud
                        sorted_signals = filtered_signals.abs().sort_values(ascending=False)
                        
                        # Asignar posiciones respetando restricciones
                        for symbol in sorted_signals.index:
                            signal = signals[symbol]
                            sector = self.sectors.get(symbol, 'Unknown')
                            
                            # Verificar restricciones de sector
                            if sector_exposure[sector] < self.sector_max_allocation:
                                # Calcular tamaño de posición
                                position_size = min(
                                    self.max_position_size,
                                    self.sector_max_allocation - sector_exposure[sector]
                                ) * np.sign(signal) * exposure_factor
                                
                                # Actualizar posición
                                self.positions.loc[date, symbol] = position_size
                                
                                # Actualizar exposición del sector
                                sector_exposure[sector] += abs(position_size)
        except Exception as e:
            logging.error(f"Error en _calculate_position_sizes: {str(e)}", exc_info=True)
            raise
    
    def _execute_trades(self):
        """Ejecuta operaciones y calcula retornos del portafolio."""
        try:
            # Inicializar DataFrame para operaciones
            self.trades = pd.DataFrame(index=self.positions.index, columns=['buys', 'sells', 'total'])
            self.trades.fillna(0, inplace=True)
            
            # Inicializar retornos del portafolio
            self.portfolio_returns = pd.Series(0, index=self.positions.index)
            
            # Para cada fecha (excepto la primera), calcular retornos
            for i in range(1, len(self.positions.index)):
                prev_date = self.positions.index[i-1]
                curr_date = self.positions.index[i]
                
                # Obtener posiciones anteriores y actuales
                prev_positions = self.positions.loc[prev_date]
                curr_positions = self.positions.loc[curr_date]
                
                # Calcular cambios en posiciones
                position_changes = curr_positions - prev_positions
                
                # Contar operaciones
                buys = (position_changes > self.transaction_threshold).sum()
                sells = (position_changes < -self.transaction_threshold).sum()
                self.trades.loc[curr_date] = [buys, sells, buys + sells]
                
                # Calcular retornos del portafolio
                if i > 1:  # Necesitamos retornos para calcular el rendimiento
                    daily_returns = self.returns.loc[curr_date]
                    portfolio_return = (prev_positions * daily_returns).sum()
                    
                    # Aplicar costos de transacción (0.1% por operación)
                    transaction_cost = abs(position_changes).sum() * 0.001
                    net_return = portfolio_return - transaction_cost
                    
                    self.portfolio_returns.loc[curr_date] = net_return
            
            # Calcular valor del portafolio
            self.portfolio_value = (1 + self.portfolio_returns).cumprod()
        except Exception as e:
            logging.error(f"Error en _execute_trades: {str(e)}", exc_info=True)
            raise
    
    def _calculate_performance_metrics(self):
        """Calcula métricas de rendimiento del portafolio."""
        try:
            # Eliminar primeros 252 días (período de calentamiento)
            returns = self.portfolio_returns[252:].copy()
            benchmark_returns = self.benchmark_returns[252:].copy()
            
            # Calcular métricas anualizadas
            ann_factor = 252
            
            # Retorno anualizado
            self.performance_metrics['annual_return'] = returns.mean() * ann_factor
            self.performance_metrics['benchmark_annual_return'] = benchmark_returns.mean() * ann_factor
            
            # Volatilidad anualizada
            self.performance_metrics['annual_volatility'] = returns.std() * np.sqrt(ann_factor)
            self.performance_metrics['benchmark_annual_volatility'] = benchmark_returns.std() * np.sqrt(ann_factor)
            
            # Ratio de Sharpe (asumiendo tasa libre de riesgo = 0)
            self.performance_metrics['sharpe_ratio'] = self.performance_metrics['annual_return'] / self.performance_metrics['annual_volatility']
            self.performance_metrics['benchmark_sharpe_ratio'] = self.performance_metrics['benchmark_annual_return'] / self.performance_metrics['benchmark_annual_volatility']
            
            # Drawdown máximo
            portfolio_value = (1 + returns).cumprod()
            benchmark_value = (1 + benchmark_returns).cumprod()
            
            rolling_max = portfolio_value.rolling(window=ann_factor, min_periods=1).max()
            benchmark_rolling_max = benchmark_value.rolling(window=ann_factor, min_periods=1).max()
            
            drawdown = (portfolio_value / rolling_max) - 1
            benchmark_drawdown = (benchmark_value / benchmark_rolling_max) - 1
            
            self.performance_metrics['max_drawdown'] = drawdown.min()
            self.performance_metrics['benchmark_max_drawdown'] = benchmark_drawdown.min()
            
            # Ratio de Sortino (solo considera volatilidad negativa)
            negative_returns = returns[returns < 0]
            benchmark_negative_returns = benchmark_returns[benchmark_returns < 0]
            
            downside_deviation = negative_returns.std() * np.sqrt(ann_factor)
            benchmark_downside_deviation = benchmark_negative_returns.std() * np.sqrt(ann_factor)
            
            self.performance_metrics['sortino_ratio'] = self.performance_metrics['annual_return'] / downside_deviation
            self.performance_metrics['benchmark_sortino_ratio'] = self.performance_metrics['benchmark_annual_return'] / benchmark_downside_deviation
            
            # Calmar ratio (retorno anualizado / max drawdown absoluto)
            self.performance_metrics['calmar_ratio'] = self.performance_metrics['annual_return'] / abs(self.performance_metrics['max_drawdown'])
            self.performance_metrics['benchmark_calmar_ratio'] = self.performance_metrics['benchmark_annual_return'] / abs(self.performance_metrics['benchmark_max_drawdown'])
            
            # Información de operaciones
            self.performance_metrics['total_trades'] = self.trades['total'].sum()
            self.performance_metrics['avg_trades_per_day'] = self.trades['total'].mean()
            
            # Beta y Alpha
            cov_matrix = np.cov(returns, benchmark_returns)
            beta = cov_matrix[0, 1] / cov_matrix[1, 1]
            alpha = self.performance_metrics['annual_return'] - beta * self.performance_metrics['benchmark_annual_return']
            
            self.performance_metrics['beta'] = beta
            self.performance_metrics['alpha'] = alpha
            
            # Information Ratio
            tracking_error = (returns - benchmark_returns).std() * np.sqrt(ann_factor)
            self.performance_metrics['information_ratio'] = (self.performance_metrics['annual_return'] - self.performance_metrics['benchmark_annual_return']) / tracking_error
        except Exception as e:
            logging.error(f"Error en _calculate_performance_metrics: {str(e)}", exc_info=True)
            raise
    
    def _plot_results(self):
        """Genera gráficos de resultados."""
        try:
            # Crear figura para valor del portafolio vs benchmark
            plt.figure(figsize=(12, 6))
            
            # Normalizar valores a 100
            portfolio_norm = self.portfolio_value / self.portfolio_value.iloc[252] * 100
            benchmark_norm = (1 + self.benchmark_returns).cumprod() / (1 + self.benchmark_returns).cumprod().iloc[252] * 100
            
            plt.plot(portfolio_norm[252:], label='Estrategia')
            plt.plot(benchmark_norm[252:], label='S&P 500', alpha=0.7)
            
            plt.title('Rendimiento de la Estrategia vs S&P 500')
            plt.xlabel('Fecha')
            plt.ylabel('Valor (Base 100)')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.savefig('./artifacts/results/figures/portfolio_performance.png')
            plt.close()
            
            # Crear figura para drawdown
            plt.figure(figsize=(12, 6))
            
            portfolio_value = (1 + self.portfolio_returns).cumprod()
            rolling_max = portfolio_value.rolling(window=252, min_periods=1).max()
            drawdown = (portfolio_value / rolling_max) - 1
            
            benchmark_value = (1 + self.benchmark_returns).cumprod()
            benchmark_rolling_max = benchmark_value.rolling(window=252, min_periods=1).max()
            benchmark_drawdown = (benchmark_value / benchmark_rolling_max) - 1
            
            plt.plot(drawdown[252:], label='Estrategia')
            plt.plot(benchmark_drawdown[252:], label='S&P 500', alpha=0.7)
            
            plt.title('Drawdown de la Estrategia vs S&P 500')
            plt.xlabel('Fecha')
            plt.ylabel('Drawdown')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.savefig('./artifacts/results/figures/drawdown.png')
            plt.close()
            
            # Crear figura para regímenes de mercado
            plt.figure(figsize=(12, 6))
            
            regimes = self.market_regimes['regime']
            
            # Crear un mapa de colores para los regímenes
            colors = {1: 'green', 2: 'red', 3: 'blue', 4: 'orange'}
            regime_colors = [colors.get(r, 'gray') for r in regimes]
            
            # Crear un scatter plot con colores por régimen
            plt.scatter(regimes.index, [1] * len(regimes), c=regime_colors, s=10)
            
            # Añadir línea de rendimiento del benchmark
            ax1 = plt.gca()
            ax2 = ax1.twinx()
            ax2.plot(benchmark_value, 'k-', alpha=0.3)
            
            plt.title('Regímenes de Mercado')
            plt.xlabel('Fecha')
            ax1.set_ylabel('Régimen')
            ax2.set_ylabel('S&P 500')
            
            # Crear leyenda manual
            from matplotlib.lines import Line2D
            legend_elements = [
                Line2D([0], [0], marker='o', color='w', markerfacecolor='green', markersize=10, label='Bull Market'),
                Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Bear Market'),
                Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='Sideways/Low Vol'),
                Line2D([0], [0], marker='o', color='w', markerfacecolor='orange', markersize=10, label='Volatile Bull')
            ]
            ax1.legend(handles=legend_elements, loc='upper left')
            
            plt.savefig('./artifacts/results/figures/market_regimes.png')
            plt.close()
            
            # Crear figura para operaciones diarias
            plt.figure(figsize=(12, 6))
            
            plt.bar(self.trades.index[252:], self.trades['buys'][252:], label='Compras', alpha=0.7, color='green')
            plt.bar(self.trades.index[252:], self.trades['sells'][252:], label='Ventas', alpha=0.7, color='red')
            
            plt.title('Operaciones Diarias')
            plt.xlabel('Fecha')
            plt.ylabel('Número de Operaciones')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.savefig('./artifacts/results/figures/daily_trades.png')
            plt.close()
            
            # Crear figura para exposición por sector
            plt.figure(figsize=(12, 6))
            
            # Calcular exposición por sector para cada fecha
            sector_exposure = pd.DataFrame(index=self.positions.index[252:])
            
            for sector in self.sectors.unique():
                sector_symbols = self.sectors[self.sectors == sector].index
                sector_exposure[sector] = self.positions.loc[self.positions.index[252:], sector_symbols].abs().sum(axis=1)
            
            # Graficar exposición por sector
            sector_exposure.plot(kind='area', stacked=True, alpha=0.7, figsize=(12, 6))
            
            plt.title('Exposición por Sector')
            plt.xlabel('Fecha')
            plt.ylabel('Exposición')
            plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig('./artifacts/results/figures/sector_exposure.png')
            plt.close()
        except Exception as e:
            logging.error(f"Error en _plot_results: {str(e)}", exc_info=True)
            raise
    
    def _save_results(self):
        """Guarda resultados en archivos."""
        try:
            # Guardar métricas de rendimiento
            pd.Series(self.performance_metrics).to_csv('./artifacts/results/data/performance_metrics.csv')
            
            # Guardar retornos diarios
            self.portfolio_returns.to_csv('./artifacts/results/data/portfolio_returns.csv')
            
            # Guardar valor del portafolio
            self.portfolio_value.to_csv('./artifacts/results/data/portfolio_value.csv')
            
            # Guardar operaciones
            self.trades.to_csv('./artifacts/results/data/trades.csv')
            
            # Guardar posiciones finales
            final_positions = self.positions.iloc[-1].sort_values(ascending=False)
            final_positions[final_positions != 0].to_csv('./artifacts/results/data/final_positions.csv')
            
            # Guardar regímenes de mercado
            self.market_regimes.to_csv('./artifacts/results/data/market_regimes.csv')
        except Exception as e:
            logging.error(f"Error en _save_results: {str(e)}", exc_info=True)
            raise
    
    def backtest(self):
        """Ejecuta el backtest completo de la estrategia."""
        try:
            print("Iniciando backtest...")
            
            # Paso 1: Calcular retornos
            print("Calculando retornos...")
            self._calculate_returns()
            
            # Paso 2: Calcular señales de momentum
            print("Calculando señales de momentum...")
            self._calculate_momentum_signals()
            
            # Paso 3: Calcular retornos residuales
            print("Calculando retornos residuales...")
            self._calculate_residual_returns()
            
            # Paso 4: Identificar regímenes de mercado
            print("Identificando regímenes de mercado...")
            self._identify_market_regimes()
            
            # Paso 5: Entrenar modelos de ensemble
            print("Entrenando modelos de ensemble...")
            self._train_ensemble_models()
            
            # Paso 6: Normalizar señales
            print("Normalizando señales...")
            self._normalize_signals()
            
            # Paso 7: Calcular tamaños de posición
            print("Calculando tamaños de posición...")
            self._calculate_position_sizes()
            
            # Paso 8: Ejecutar operaciones
            print("Ejecutando operaciones...")
            self._execute_trades()
            
            # Paso 9: Calcular métricas de rendimiento
            print("Calculando métricas de rendimiento...")
            self._calculate_performance_metrics()
            
            # Paso 10: Generar gráficos
            print("Generando gráficos...")
            self._plot_results()
            
            # Paso 11: Guardar resultados
            print("Guardando resultados...")
            self._save_results()
            
            print("Backtest completado.")
            
            # Mostrar resumen de rendimiento
            print("\nResumen de Rendimiento:")
            print(f"Retorno Anualizado: {self.performance_metrics['annual_return']:.2%}")
            print(f"Volatilidad Anualizada: {self.performance_metrics['annual_volatility']:.2%}")
            print(f"Ratio de Sharpe: {self.performance_metrics['sharpe_ratio']:.2f}")
            print(f"Drawdown Máximo: {self.performance_metrics['max_drawdown']:.2%}")
            print(f"Alpha: {self.performance_metrics['alpha']:.2%}")
            print(f"Beta: {self.performance_metrics['beta']:.2f}")
            print(f"Operaciones Totales: {self.performance_metrics['total_trades']}")
            
            return self.performance_metrics
        except Exception as e:
            logging.error(f"Error en backtest: {str(e)}", exc_info=True)
            raise
    
    def walk_forward_test(self, train_size=252*3, test_size=252, step_size=126):
        """
        Realiza un test walk-forward de la estrategia.
        
        Args:
            train_size (int): Tamaño de la ventana de entrenamiento en días
            test_size (int): Tamaño de la ventana de prueba en días
            step_size (int): Tamaño del paso para avanzar la ventana en días
        
        Returns:
            pd.Series: Retornos del portafolio en el período de prueba
        """
        try:
            print("Iniciando test walk-forward...")
            
            # Asegurar que hay suficientes datos
            if len(self.prices) < train_size + test_size:
                raise ValueError("No hay suficientes datos para el test walk-forward")
            
            # Inicializar DataFrame para retornos out-of-sample
            oos_returns = pd.Series(dtype=float)
            
            # Calcular retornos una vez (no cambian)
            self._calculate_returns()
            
            # Iterar sobre ventanas de tiempo
            start_idx = 0
            while start_idx + train_size + test_size <= len(self.prices):
                # Definir índices de entrenamiento y prueba
                train_end_idx = start_idx + train_size
                test_end_idx = train_end_idx + test_size
                
                train_dates = self.prices.index[start_idx:train_end_idx]
                test_dates = self.prices.index[train_end_idx:test_end_idx]
                
                print(f"Entrenando en: {train_dates[0]} a {train_dates[-1]}")
                print(f"Probando en: {test_dates[0]} a {test_dates[-1]}")
                
                # Crear una instancia temporal para esta ventana
                temp_strategy = AdaptiveMomentumStrategy(
                    start_date=train_dates[0].strftime('%Y-%m-%d'),
                    end_date=test_dates[-1].strftime('%Y-%m-%d')
                )
                
                # Ejecutar backtest en datos de entrenamiento
                temp_strategy.prices = self.prices.loc[train_dates]
                temp_strategy.benchmark_prices = self.benchmark_prices.loc[train_dates]
                temp_strategy.sectors = self.sectors
                
                # Calcular señales usando solo datos de entrenamiento
                temp_strategy._calculate_returns()
                temp_strategy._calculate_momentum_signals()
                temp_strategy._calculate_residual_returns()
                temp_strategy._identify_market_regimes()
                temp_strategy._train_ensemble_models()
                
                # Aplicar modelo a datos de prueba
                test_prices = self.prices.loc[test_dates]
                test_returns = self.returns.loc[test_dates]
                test_benchmark = self.benchmark_prices.loc[test_dates]
                
                # Calcular señales para período de prueba
                for period in self.lookback_periods:
                    # Usar precios de entrenamiento + prueba para calcular momentum
                    all_prices = pd.concat([temp_strategy.prices, test_prices])
                    cum_returns = all_prices.pct_change(period)
                    
                    # Extraer solo señales del período de prueba
                    test_signals = cum_returns.loc[test_dates]
                    
                    # Normalizar
                    normalized_signals = test_signals.apply(lambda x: (x - x.mean()) / x.std(), axis=1)
                    normalized_signals = normalized_signals.apply(lambda x: np.clip(x, -3, 3), axis=1)
                    
                    temp_strategy.momentum_signals[period] = normalized_signals
                
                # Calcular regímenes para período de prueba
                all_benchmark = pd.concat([temp_strategy.benchmark_prices, test_benchmark])
                all_benchmark_returns = all_benchmark.pct_change()
                
                annual_return = all_benchmark.pct_change(temp_strategy.regime_lookback)
                rolling_std = all_benchmark_returns.rolling(temp_strategy.vol_lookback).std() * np.sqrt(252)
                
                # Extraer solo regímenes del período de prueba
                test_regimes = pd.DataFrame(index=test_dates)
                
                return_threshold = annual_return.rolling(252).median()
                vol_threshold = rolling_std.rolling(252).median()
                
                regimes = pd.Series(index=test_dates, dtype=int)
                
                for date in test_dates:
                    if pd.notna(annual_return.loc[date]) and pd.notna(rolling_std.loc[date]):
                        if pd.notna(return_threshold.loc[date]) and pd.notna(vol_threshold.loc[date]):
                            if annual_return.loc[date] > return_threshold.loc[date]:
                                if rolling_std.loc[date] > vol_threshold.loc[date]:
                                    regimes.loc[date] = 4  # Volatile bull
                                else:
                                    regimes.loc[date] = 1  # Bull market
                            else:
                                if rolling_std.loc[date] > vol_threshold.loc[date]:
                                    regimes.loc[date] = 2  # Bear market
                                else:
                                    regimes.loc[date] = 3  # Sideways/low vol
                
                regimes = regimes.fillna(method='ffill')
                test_regimes['regime'] = regimes
                
                temp_strategy.market_regimes = test_regimes
                temp_strategy.volatility = rolling_std.loc[test_dates]
                
                # Aplicar modelo entrenado a datos de prueba
                temp_strategy.combined_signals = pd.DataFrame(index=test_dates, columns=test_prices.columns)
                
                # Usar modelos entrenados para generar señales en período de prueba
                for symbol in test_prices.columns:
                    # Preparar datos de prueba
                    X_test = pd.DataFrame(index=test_dates)
                    
                    # Añadir señales de momentum como features
                    for period in temp_strategy.lookback_periods:
                        if symbol in temp_strategy.momentum_signals[period].columns:
                            X_test[f'momentum_{period}'] = temp_strategy.momentum_signals[period][symbol]
                    
                    # Añadir retornos residuales como feature (calcular para período de prueba)
                    if symbol in test_returns.columns and not test_returns[symbol].isna().all():
                        # Calcular residuales para período de prueba
                        asset_returns = test_returns[symbol]
                        benchmark_returns = all_benchmark_returns.loc[test_dates]
                        
                        valid_data = pd.concat([asset_returns, benchmark_returns], axis=1).dropna()
                        
                        if len(valid_data) > 10:
                            X = valid_data.iloc[:, 1].values.reshape(-1, 1)
                            y = valid_data.iloc[:, 0].values
                            
                            model = LinearRegression()
                            model.fit(X, y)
                            
                            predicted = model.predict(X)
                            residuals = y - predicted
                            
                            for i, date in enumerate(valid_data.index):
                                X_test.loc[date, 'residual'] = residuals[i]
                    
                    # Añadir régimen de mercado como feature categórica
                    X_test['regime'] = temp_strategy.market_regimes['regime']
                    
                    # Convertir régimen a variables dummy
                    regime_dummies = pd.get_dummies(X_test['regime'], prefix='regime')
                    X_test = pd.concat([X_test.drop('regime', axis=1), regime_dummies], axis=1)
                    
                    # Eliminar filas con NaN
                    X_test = X_test.dropna()
                    
                    if len(X_test) > 0:
                        # Usar modelos entrenados para predecir
                        rf = RandomForestRegressor(n_estimators=100, max_depth=3, random_state=42)
                        gb = GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42)
                        lr = LinearRegression()
                        
                        # Entrenar con todos los datos de entrenamiento
                        X_train = pd.DataFrame(index=train_dates)
                        
                        for period in temp_strategy.lookback_periods:
                            if symbol in temp_strategy.momentum_signals[period].columns:
                                X_train[f'momentum_{period}'] = temp_strategy.momentum_signals[period][symbol]
                        
                        if 'residual' in X_test.columns:
                            X_train['residual'] = temp_strategy.residual_returns[symbol]
                        
                        X_train['regime'] = temp_strategy.market_regimes['regime']
                        
                        train_regime_dummies = pd.get_dummies(X_train['regime'], prefix='regime')
                        X_train = pd.concat([X_train.drop('regime', axis=1), train_regime_dummies], axis=1)
                        
                        X_train = X_train.dropna()
                        
                        if len(X_train) > 30:
                            y_train = temp_strategy.returns[symbol].shift(-5).loc[X_train.index]
                            valid_indices = ~y_train.isna()
                            X_train_valid = X_train.loc[valid_indices]
                            y_train_valid = y_train.loc[valid_indices]
                            
                            if len(X_train_valid) > 30:
                                # Asegurar que X_test tiene las mismas columnas que X_train
                                for col in X_train_valid.columns:
                                    if col not in X_test.columns:
                                        X_test[col] = 0
                                
                                for col in X_test.columns:
                                    if col not in X_train_valid.columns:
                                        X_test = X_test.drop(col, axis=1)
                                
                                # Reordenar columnas para que coincidan
                                X_test = X_test[X_train_valid.columns]
                                
                                # Entrenar modelos
                                rf.fit(X_train_valid, y_train_valid)
                                gb.fit(X_train_valid, y_train_valid)
                                lr.fit(X_train_valid, y_train_valid)
                                
                                # Predecir
                                pred_rf = rf.predict(X_test)
                                pred_gb = gb.predict(X_test)
                                pred_lr = lr.predict(X_test)
                                
                                # Combinar predicciones
                                ensemble_pred = (pred_rf + pred_gb + pred_lr) / 3
                                
                                # Almacenar predicciones
                                for i, date in enumerate(X_test.index):
                                    temp_strategy.combined_signals.loc[date, symbol] = ensemble_pred[i]
                
                # Normalizar señales
                temp_strategy._normalize_signals()
                
                # Calcular posiciones
                temp_strategy._calculate_position_sizes()
                
                # Ejecutar operaciones solo para el período de prueba
                temp_strategy.positions = temp_strategy.positions.loc[test_dates]
                temp_strategy.returns = test_returns
                temp_strategy.benchmark_returns = all_benchmark_returns.loc[test_dates]
                
                temp_strategy._execute_trades()
                
                # Guardar retornos out-of-sample
                oos_returns = pd.concat([oos_returns, temp_strategy.portfolio_returns])
                
                # Avanzar ventana
                start_idx += step_size
            
            # Calcular métricas para retornos out-of-sample
            ann_factor = 252
            annual_return = oos_returns.mean() * ann_factor
            annual_vol = oos_returns.std() * np.sqrt(ann_factor)
            sharpe_ratio = annual_return / annual_vol
            
            # Calcular drawdown
            portfolio_value = (1 + oos_returns).cumprod()
            rolling_max = portfolio_value.rolling(window=ann_factor, min_periods=1).max()
            drawdown = (portfolio_value / rolling_max) - 1
            max_drawdown = drawdown.min()
            
            print("\nResultados Walk-Forward:")
            print(f"Retorno Anualizado: {annual_return:.2%}")
            print(f"Volatilidad Anualizada: {annual_vol:.2%}")
            print(f"Ratio de Sharpe: {sharpe_ratio:.2f}")
            print(f"Drawdown Máximo: {max_drawdown:.2%}")
            
            # Guardar resultados
            oos_returns.to_csv('./artifacts/results/data/walk_forward_returns.csv')
            
            # Crear gráfico de rendimiento walk-forward
            plt.figure(figsize=(12, 6))
            
            portfolio_value = (1 + oos_returns).cumprod()
            
            # Obtener retornos del benchmark para el mismo período
            benchmark_returns = self.benchmark_returns.loc[oos_returns.index]
            benchmark_value = (1 + benchmark_returns).cumprod()
            
            # Normalizar a 100
            portfolio_value = portfolio_value / portfolio_value.iloc[0] * 100
            benchmark_value = benchmark_value / benchmark_value.iloc[0] * 100
            
            plt.plot(portfolio_value, label='Estrategia (Walk-Forward)')
            plt.plot(benchmark_value, label='S&P 500', alpha=0.7)
            
            plt.title('Rendimiento Walk-Forward vs S&P 500')
            plt.xlabel('Fecha')
            plt.ylabel('Valor (Base 100)')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.savefig('./artifacts/results/figures/walk_forward_performance.png')
            plt.close()
            
            return oos_returns
        except Exception as e:
            logging.error(f"Error en walk_forward_test: {str(e)}", exc_info=True)
            raise

# Ejecutar la estrategia
if __name__ == "__main__":
    try:
        # Inicializar estrategia
        strategy = AdaptiveMomentumStrategy(start_date='2015-01-01')
        
        # Ejecutar backtest
        strategy.backtest()
        
        # Ejecutar test walk-forward
        strategy.walk_forward_test()
    except Exception as e:
        logging.error(f"Error al ejecutar la estrategia: {str(e)}", exc_info=True)
        raise
```


--- revisor_codigo (STEP 15.1) ---
INPUT 1 (user):
=== REVISIÓN DE CÓDIGO ===

IDEA CONCEPTUAL (sintetizada):
# IDEA FINAL: ESTRATEGIA DE MOMENTUM ADAPTATIVO CON REGÍMENES DE MERCADO Y CONTROL DINÁMICO DE RIESGO

## Descripción

Esta estrategia combina momentum residual multi-horizonte con identificación de regímenes de mercado y gestión dinámica del riesgo para capturar alpha en el S&P 500 mientras minimiza drawdowns. El enfoque integra análisis cuantitativo avanzado con controles de riesgo adaptativos para crear un sistema de trading robusto que se ajusta a diferentes condiciones de mercado.

## Características principales

### 1. Sistema de Momentum Residual Multi-Horizonte

- **Cálculo de rendimientos residuales**:
  - Análisis de múltiples horizontes temporales (5, 21, 63, 126 días)
  - Ajuste por modelo multifactorial: Rendimiento_Residual = Rendimiento_Real - (Exposición_Factores * Rendimiento_Factores)
  - Factores incluidos: Mercado, Tamaño, Valor, Momentum, Calidad
  - Señales contrarias para horizontes cortos (1-5 días) para capturar reversión a la media
  - Señales de momentum direccional para horizontes medianos/largos (>21 días)

- **Normalización adaptativa**:
  - Normalización por volatilidad específica usando desviación estándar móvil adaptativa
  - Winsorización dinámica basada en desviaciones MAD para mitigar outliers:
    - Umbral_Superior = mediana + 3 * MAD
    - Umbral_Inferior = mediana - 3 * MAD

- **Filtro de tendencia de largo plazo**:
  - Comparación del precio actual con media móvil de 200 días
  - Ajuste de exposición al momentum de corto plazo según dirección de tendencia de largo plazo

### 2. Identificación de Regímenes de Mercado (Modelo HMM Suavizado)

- **Modelo de Markov oculto con 3 estados**:
  - Régimen 1: Mercado en tendencia (baja volatilidad, autocorrelación positiva)
  - Régimen 2: Mercado en rango (baja volatilidad, autocorrelación negativa)
  - Régimen 3: Mercado en crisis (alta volatilidad, correlaciones inestables)

- **Variables de observación para inferencia de régimen**:
  - Volatilidad realizada (ATR de 21 días / precio)
  - Autocorrelación de rendimientos (ventana de 21 días)
  - Ratio de días positivos/negativos (ventana de 21 días)
  - Divergencia de volatilidad implícita vs. realizada (cuando disponible)
  - Variables macroeconómicas auxiliares:
    - Pendiente de la curva de rendimiento (10Y-2Y)
    - Spreads de crédito (BBB-Treasury)

- **Enfoque de régimen suavizado**:
  - Utilización de probabilidades de pertenencia a cada régimen en lugar de clasificación discreta
  - Señal_Final = Σ(Prob_Régimen_i * Señal_Régimen_i)
  - Recalibración trimestral del modelo HMM con validación cruzada para selección óptima de estados

### 3. Sistema de Ensemble con Stacking

- **Generación de múltiples modelos**:
  - 10-15 variantes de la estrategia con diferentes técnicas y parámetros
  - Entrenamiento en diferentes subconjuntos de datos históricos (block bootstrap)
  - Diversificación de métodos: modelos basados en momentum, reversión, volatilidad, etc.

- **Implementación de stacking**:
  - Meta-modelo (regresión regularizada) para combinar predicciones de modelos base
  - Captura de interacciones entre modelos para mejorar precisión
  - Pruning dinámico: eliminación temporal de modelos con rendimiento persistentemente pobre

- **Adaptación por validación cruzada temporal**:
  - Walk-forward validation con ventanas de entrenamiento de 2 años y prueba de 6 meses
  - Actualización trimestral de pesos del ensemble basada en rendimiento reciente

### 4. Gestión Dinámica del Riesgo

- **Control de drawdown adaptativo**:
  - Exposición = Exposición_Base * (1 - (Drawdown_Actual/Drawdown_Máximo_Tolerado)^2)
  - Drawdown_Máximo_Tolerado = 15% (calibrable)
  - Recuperación gradual post-reducción por drawdown

- **VaR condicional (CVaR/Expected Shortfall)**:
  - Limitación de exposición basada en CVaR estimado bajo diferentes escenarios
  - Protección mejorada contra eventos extremos

- **Filtro de tendencia de volatilidad**:
  - Ratio de volatilidad actual (21 días) vs. volatilidad de largo plazo (63 días)
  - Reducción de exposición cuando volatilidad_actual/volatilidad_tendencia > 1.2
  - Reducción adicional cuando la derivada de la volatilidad es positiva

- **Stops dinámicos contextuales**:
  - Stop-loss basado en ATR * multiplicador específico por régimen:
    - Régimen tendencia: 3.0 * ATR
    - Régimen rango: 2.0 * ATR
    - Régimen crisis: 1.5 * ATR
  - Stops basados en tiempo: reducción de posiciones sin comportamiento esperado después de n días

### 5. Optimización de Ejecución

- **Banda de no-transacción adaptativa**:
  - Umbral mínimo = máx(0.5%, costos_transacción * 3)
  - Ejecución solo cuando el cambio en posición deseada > umbral mínimo
  - Ajuste dinámico según volatilidad reciente

- **Timing de ejecución optimizado**:
  - Rebalanceo semanal (viernes cierre) como base
  - Análisis de patrones estacionales intraweek para día óptimo de ejecución
  - Monitoreo continuo para stops y condiciones extremas

- **Simulación de órdenes realista**:
  - Modelado de slippage basado en volatilidad y volumen
  - Incorporación de costos de financiamiento para posiciones apalancadas

## Implementación

### 1. Cálculo de Señales

```python
# Pseudocódigo para cálculo de señales
def calcular_senales(precios, factores, parametros):
    # Calcular rendimientos para diferentes horizontes
    rendimientos = {}
    for horizonte in [5, 21, 63, 126]:
        rendimientos[horizonte] = calcular_rendimiento(precios, horizonte)
    
    # Calcular exposición a factores y rendimientos residuales
    rendimientos_residuales = {}
    for horizonte, rend in rendimientos.items():
        exposicion = calcular_exposicion_factores(rend, factores, horizonte)
        rendimientos_residuales[horizonte] = rend - np.dot(exposicion, factores)
    
    # Normalizar señales
    senales_normalizadas = {}
    for horizonte, rend_res in rendimientos_residuales.items():
        vol = calcular_volatilidad_adaptativa(rend_res, horizonte)
        senales_normalizadas[horizonte] = rend_res / vol
        
        # Aplicar winsorización
        mad = calcular_mad(senales_normalizadas[horizonte])
        mediana = np.median(senales_normalizadas[horizonte])
        umbral_sup = mediana + 3 * mad
        umbral_inf = mediana - 3 * mad
        senales_normalizadas[horizonte] = np.clip(senales_normalizadas[horizonte], umbral_inf, umbral_sup)
    
    # Invertir señal para horizontes cortos (reversión a la media)
    if horizonte <= 5:
        senales_normalizadas[horizonte] = -senales_normalizadas[horizonte]
    
    return senales_normalizadas
```

### 2. Identificación de Regímenes

```python
# Pseudocódigo para identificación de regímenes
def identificar_regimenes(precios, datos_macro, parametros):
    # Calcular features para el HMM
    features = []
    
    # Volatilidad realizada
    atr = calcular_atr(precios, 21)
    features.append(atr / precios)
    
    # Autocorrelación
    autocorr = calcular_autocorrelacion(precios, 21)
    features.append(autocorr)
    
    # Ratio días positivos/negativos
    ratio_dias = calcular_ratio_dias_pos_neg(precios, 21)
    features.append(ratio_dias)
    
    # Variables macro
    pendiente_curva = datos_macro['10Y'] - datos_macro['2Y']
    features.append(pendiente_curva)
    spread_credito = datos_macro['BBB'] - datos_macro['Treasury']
    features.append(spread_credito)
    
    # Entrenar/actualizar modelo HMM
    if necesita_recalibracion():
        modelo_hmm = entrenar_hmm(features_historicos, n_estados=3)
    
    # Obtener probabilidades de régimen
    prob_regimenes = modelo_hmm.predict_proba(features)
    
    return prob_regimenes
```

### 3. Combinación de Señales y Gestión de Riesgo

```python
# Pseudocódigo para combinación de señales y gestión de riesgo
def combinar_senales(senales, prob_regimenes, estado_portfolio, parametros):
    # Pesos base por horizonte según régimen
    pesos_horizonte = {}
    for horizonte in senales.keys():
        pesos_horizonte[horizonte] = 0
        for i, prob in enumerate(prob_regimenes):
            if i == 0:  # Régimen tendencia
                if horizonte >= 21:
                    pesos_horizonte[horizonte] += prob * 0.4  # Mayor peso a medio/largo plazo
                else:
                    pesos_horizonte[horizonte] += prob * 0.1
            elif i == 1:  # Régimen rango
                if horizonte <= 5:
                    pesos_horizonte[horizonte] += prob * 0.5  # Mayor peso a corto plazo (reversión)
                else:
                    pesos_horizonte[horizonte] += prob * 0.1
            else:  # Régimen crisis
                pesos_horizonte[horizonte] += prob * 0.2  # Reducción general
    
    # Combinar señales ponderadas
    senal_combinada = 0
    for horizonte, senal in senales.items():
        senal_combinada += senal * pesos_horizonte[horizonte]
    
    # Aplicar control de drawdown
    drawdown_actual = calcular_drawdown(estado_portfolio)
    drawdown_max = parametros['drawdown_maximo']
    factor_drawdown = 1 - (drawdown_actual/drawdown_max)**2
    factor_drawdown = max(0, min(1, factor_drawdown))
    
    # Aplicar filtro de volatilidad
    vol_actual = calcular_volatilidad(precios, 21)
    vol_tendencia = calcular_volatilidad(precios, 63)
    ratio_vol = vol_actual / vol_tendencia
    factor_vol = 1.0
    if ratio_vol > 1.2:
        factor_vol = 1.2 / ratio_vol
    
    # Calcular exposición final
    exposicion_base = parametros['exposicion_maxima']
    exposicion = exposicion_base * factor_drawdown * factor_vol
    
    # Transformar señal en posición usando función sigmoide
    k = parametros['sensibilidad_sigmoide']
    posicion = exposicion * (2 / (1 + np.exp(-k * senal_combinada)) - 1)
    
    return posicion
```

### 4. Backtesting y Validación

```python
# Pseudocódigo para backtesting y validación
def backtest(datos, parametros):
    resultados = []
    
    # Walk-forward validation
    for inicio_test, fin_test in generar_ventanas_test(datos):
        # Datos de entrenamiento hasta inicio_test
        datos_train = datos[:inicio_test]
        
        # Entrenar ensemble de modelos
        modelos = entrenar_ensemble(datos_train, parametros)
        
        # Evaluar en ventana de test
        for fecha in range(inicio_test, fin_test):
            # Obtener señales de cada modelo
            senales_modelos = []
            for modelo in modelos:
                senal = modelo.generar_senal(datos, fecha)
                senales_modelos.append(senal)
            
            # Combinar señales con meta-modelo
            senal_combinada = meta_modelo.predecir(senales_modelos)
            
            # Aplicar gestión de riesgo
            posicion = aplicar_gestion_riesgo(senal_combinada, estado_portfolio)
            
            # Simular ejecución con costos
            rendimiento = simular_ejecucion(posicion, datos, fecha, costos)
            
            resultados.append({
                'fecha': fecha,
                'posicion': posicion,
                'rendimiento': rendimiento
            })
    
    # Calcular métricas
    sharpe = calcular_sharpe(resultados)
    drawdown_max = calcular_max_drawdown(resultados)
    correlacion_sp500 = calcular_correlacion(resultados, sp500)
    
    return {
        'resultados': resultados,
        'metricas': {
            'sharpe': sharpe,
            'drawdown_max': drawdown_max,
            'correlacion_sp500': correlacion_sp500
        }
    }
```

### 5. Métricas Esperadas

- **Sharpe ratio esperado**: 1.2-1.5 (post-costos)
- **Drawdown máximo esperado**: 12-15%
- **Correlación con S&P 500**: 0.3-0.5 (diversificación significativa)
- **Ratio de captación alcista/bajista**: 0.7/0.3 (asimétrico)

### 6. Consideraciones de Implementación

- Utilizar datos de yfinance para precios y volúmenes
- Implementar manejo de errores robusto para datos faltantes o erróneos
- Actualizar parámetros del modelo trimestralmente
- Monitorear continuamente el rendimiento y las métricas de riesgo
- Realizar análisis de atribución de rendimiento mensual para verificar fuentes de alpha
- Documentar todas las decisiones de diseño y parámetros para facilitar auditoría y mejora continua

Esta estrategia equilibra sofisticación analítica con robustez operativa, adaptándose a diferentes condiciones de mercado mientras mantiene un control riguroso del riesgo.

IMPLEMENTACIÓN EN CÓDIGO (del desarrollador):
```python
import os
import logging
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import yfinance as yf
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error
from scipy import stats
import warnings
from datetime import datetime, timedelta
import requests
from bs4 import BeautifulSoup
import time

# Crear directorios para resultados
os.makedirs('./artifacts/results', exist_ok=True)
os.makedirs('./artifacts/results/figures', exist_ok=True)
os.makedirs('./artifacts/results/data', exist_ok=True)

# Configurar logging
logging.basicConfig(
    filename='./artifacts/errors.txt',
    level=logging.ERROR,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# Ignorar advertencias
warnings.filterwarnings('ignore')

class AdaptiveMomentumStrategy:
    def __init__(self, start_date='2010-01-01', end_date=None, benchmark='^GSPC'):
        """
        Inicializa la estrategia de momentum adaptativo.
        
        Args:
            start_date (str): Fecha de inicio para los datos
            end_date (str): Fecha de fin para los datos (por defecto, fecha actual)
            benchmark (str): Símbolo del índice de referencia
        """
        self.start_date = start_date
        self.end_date = end_date if end_date else datetime.now().strftime('%Y-%m-%d')
        self.benchmark = benchmark
        self.lookback_periods = [20, 60, 120, 252]  # Períodos de momentum en días
        self.regime_lookback = 252  # Período para identificar regímenes
        self.vol_lookback = 63  # Período para calcular volatilidad (63 días ≈ 3 meses)
        self.max_drawdown_tolerance = 0.15  # Drawdown máximo tolerado (15%)
        self.sector_max_allocation = 0.25  # Exposición máxima por sector (25%)
        self.max_position_size = 0.05  # Tamaño máximo de posición individual (5%)
        self.transaction_threshold = 0.001  # Umbral mínimo para transacciones (0.1%)
        
        # Inicializar variables que se llenarán más tarde
        self.prices = None
        self.returns = None
        self.benchmark_prices = None
        self.benchmark_returns = None
        self.sectors = None
        self.momentum_signals = {}
        self.residual_returns = None
        self.market_regimes = None
        self.volatility = None
        self.combined_signals = None
        self.positions = None
        self.portfolio_returns = None
        self.portfolio_value = None
        self.trades = None
        self.performance_metrics = {}
        
        try:
            self._load_data()
        except Exception as e:
            logging.error(f"Error al cargar datos: {str(e)}", exc_info=True)
            raise
    
    def _load_data(self):
        """Carga los datos de precios del S&P 500 y el benchmark."""
        try:
            # Obtener lista de símbolos del S&P 500 desde Wikipedia
            sp500_symbols = self._get_sp500_symbols()
            
            # Descargar datos de precios para todos los símbolos
            print(f"Descargando datos para {len(sp500_symbols)} símbolos...")
            all_data = yf.download(sp500_symbols, start=self.start_date, end=self.end_date)
            
            # Extraer precios de cierre
            self.prices = all_data['Close']
            
            # Descargar datos del benchmark
            benchmark_data = yf.download(self.benchmark, start=self.start_date, end=self.end_date)
            self.benchmark_prices = benchmark_data['Close']
            
            # Obtener información de sectores
            self.sectors = self._get_sector_data(sp500_symbols)
            
            print(f"Datos cargados: {self.prices.shape[1]} símbolos, {self.prices.shape[0]} días")
        except Exception as e:
            logging.error(f"Error en _load_data: {str(e)}", exc_info=True)
            raise
    
    def _get_sp500_symbols(self):
        """Obtiene la lista de símbolos del S&P 500 desde Wikipedia."""
        try:
            url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
            response = requests.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')
            table = soup.find('table', {'class': 'wikitable sortable'})
            
            symbols = []
            for row in table.findAll('tr')[1:]:
                symbol = row.findAll('td')[0].text.strip()
                symbols.append(symbol)
            
            return symbols
        except Exception as e:
            logging.error(f"Error al obtener símbolos del S&P 500: {str(e)}", exc_info=True)
            # Fallback a una lista pequeña de símbolos conocidos
            return ['AAPL', 'MSFT', 'AMZN', 'GOOGL', 'FB', 'TSLA', 'BRK-B', 'JPM', 'JNJ', 'V']
    
    def _get_sector_data(self, symbols):
        """Obtiene información de sectores para los símbolos."""
        try:
            sectors = pd.Series(index=symbols)
            
            # Procesar en lotes para evitar límites de API
            batch_size = 100
            for i in range(0, len(symbols), batch_size):
                batch_symbols = symbols[i:i+batch_size]
                for symbol in batch_symbols:
                    try:
                        ticker = yf.Ticker(symbol)
                        sector = ticker.info.get('sector', 'Unknown')
                        sectors[symbol] = sector
                    except:
                        sectors[symbol] = 'Unknown'
                
                # Pausa para evitar límites de API
                time.sleep(1)
            
            return sectors
        except Exception as e:
            logging.error(f"Error al obtener datos de sectores: {str(e)}", exc_info=True)
            # Crear sectores aleatorios como fallback
            return pd.Series('Unknown', index=symbols)
    
    def _calculate_returns(self):
        """Calcula los retornos diarios de los precios."""
        try:
            # Calcular retornos diarios (t/t-1)
            self.returns = self.prices.pct_change()
            
            # Calcular retornos del benchmark
            self.benchmark_returns = self.benchmark_prices.pct_change()
        except Exception as e:
            logging.error(f"Error en _calculate_returns: {str(e)}", exc_info=True)
            raise
    
    def _calculate_momentum_signals(self):
        """Calcula señales de momentum para diferentes períodos."""
        try:
            for period in self.lookback_periods:
                # Calcular retorno acumulado para el período
                cum_returns = self.prices.pct_change(period)
                
                # Normalizar señales por fecha (cross-sectional)
                normalized_signals = cum_returns.apply(lambda x: (x - x.mean()) / x.std(), axis=1)
                
                # Winsorizar valores extremos (±3 desviaciones estándar)
                normalized_signals = normalized_signals.apply(
                    lambda x: np.clip(x, -3, 3), axis=1
                )
                
                # Almacenar señales
                self.momentum_signals[period] = normalized_signals
        except Exception as e:
            logging.error(f"Error en _calculate_momentum_signals: {str(e)}", exc_info=True)
            raise
    
    def _calculate_residual_returns(self):
        """Calcula retornos residuales después de ajustar por factores de mercado."""
        try:
            # Inicializar DataFrame para retornos residuales
            self.residual_returns = pd.DataFrame(index=self.returns.index, columns=self.returns.columns)
            
            # Para cada activo, calcular retornos residuales
            for symbol in self.returns.columns:
                # Obtener retornos del activo y del benchmark
                asset_returns = self.returns[symbol].copy()
                
                # Eliminar filas con NaN
                valid_data = pd.concat([asset_returns, self.benchmark_returns], axis=1).dropna()
                
                if len(valid_data) > 30:  # Asegurar suficientes datos
                    X = valid_data.iloc[:, 1].values.reshape(-1, 1)  # Benchmark returns
                    y = valid_data.iloc[:, 0].values  # Asset returns
                    
                    # Ajustar modelo de regresión lineal
                    model = LinearRegression()
                    model.fit(X, y)
                    
                    # Calcular residuos
                    predicted = model.predict(X)
                    residuals = y - predicted
                    
                    # Almacenar residuos en las fechas correspondientes
                    for i, date in enumerate(valid_data.index):
                        self.residual_returns.loc[date, symbol] = residuals[i]
        except Exception as e:
            logging.error(f"Error en _calculate_residual_returns: {str(e)}", exc_info=True)
            raise
    
    def _identify_market_regimes(self):
        """Identifica regímenes de mercado basados en tendencia y volatilidad."""
        try:
            # Calcular retornos acumulados de 252 días (1 año)
            annual_return = self.benchmark_prices.pct_change(self.regime_lookback)
            
            # Calcular volatilidad de 63 días (3 meses)
            rolling_std = self.benchmark_returns.rolling(self.vol_lookback).std() * np.sqrt(252)
            
            # Inicializar DataFrame para regímenes
            self.market_regimes = pd.DataFrame(index=self.prices.index)
            
            # Definir regímenes:
            # 1: Bull market (retorno alto, volatilidad baja)
            # 2: Bear market (retorno bajo, volatilidad alta)
            # 3: Sideways/low vol (retorno bajo, volatilidad baja)
            # 4: Volatile bull (retorno alto, volatilidad alta)
            
            # Calcular umbrales adaptativos
            return_threshold = annual_return.rolling(252).median()
            vol_threshold = rolling_std.rolling(252).median()
            
            # Asignar regímenes
            regimes = pd.Series(index=self.prices.index, dtype=int)
            
            for date in regimes.index:
                if pd.notna(annual_return.loc[date]) and pd.notna(rolling_std.loc[date]):
                    if pd.notna(return_threshold.loc[date]) and pd.notna(vol_threshold.loc[date]):
                        if annual_return.loc[date] > return_threshold.loc[date]:
                            if rolling_std.loc[date] > vol_threshold.loc[date]:
                                regimes.loc[date] = 4  # Volatile bull
                            else:
                                regimes.loc[date] = 1  # Bull market
                        else:
                            if rolling_std.loc[date] > vol_threshold.loc[date]:
                                regimes.loc[date] = 2  # Bear market
                            else:
                                regimes.loc[date] = 3  # Sideways/low vol
            
            # Llenar valores NaN con el régimen más reciente
            regimes = regimes.fillna(method='ffill')
            
            self.market_regimes['regime'] = regimes
            
            # Calcular volatilidad para ajustar posiciones
            self.volatility = rolling_std.copy()
        except Exception as e:
            logging.error(f"Error en _identify_market_regimes: {str(e)}", exc_info=True)
            raise
    
    def _train_ensemble_models(self):
        """Entrena modelos de ensemble para combinar señales de momentum."""
        try:
            # Crear DataFrame para almacenar señales combinadas
            self.combined_signals = pd.DataFrame(index=self.prices.index, columns=self.prices.columns)
            
            # Definir divisiones de tiempo para validación cruzada
            tscv = TimeSeriesSplit(n_splits=5)
            
            # Para cada activo, entrenar un modelo ensemble
            for symbol in self.prices.columns:
                # Preparar datos de entrenamiento
                X = pd.DataFrame(index=self.prices.index)
                
                # Añadir señales de momentum como features
                for period in self.lookback_periods:
                    X[f'momentum_{period}'] = self.momentum_signals[period][symbol]
                
                # Añadir retornos residuales como feature
                X['residual'] = self.residual_returns[symbol]
                
                # Añadir régimen de mercado como feature categórica
                X['regime'] = self.market_regimes['regime']
                
                # Convertir régimen a variables dummy
                regime_dummies = pd.get_dummies(X['regime'], prefix='regime')
                X = pd.concat([X.drop('regime', axis=1), regime_dummies], axis=1)
                
                # Eliminar filas con NaN
                X = X.dropna()
                
                if len(X) > 252:  # Asegurar suficientes datos (1 año)
                    # Definir target: retornos futuros a 5 días
                    y = self.returns[symbol].shift(-5).loc[X.index]
                    
                    # Eliminar NaN del target
                    valid_indices = ~y.isna()
                    X_valid = X.loc[valid_indices]
                    y_valid = y.loc[valid_indices]
                    
                    if len(X_valid) > 252:
                        # Entrenar modelos
                        rf = RandomForestRegressor(n_estimators=100, max_depth=3, random_state=42)
                        gb = GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42)
                        lr = LinearRegression()
                        
                        # Validación cruzada para evitar overfitting
                        predictions = pd.Series(index=X_valid.index, dtype=float)
                        
                        for train_idx, test_idx in tscv.split(X_valid):
                            X_train, X_test = X_valid.iloc[train_idx], X_valid.iloc[test_idx]
                            y_train, y_test = y_valid.iloc[train_idx], y_valid.iloc[test_idx]
                            
                            # Entrenar modelos
                            rf.fit(X_train, y_train)
                            gb.fit(X_train, y_train)
                            lr.fit(X_train, y_train)
                            
                            # Predecir
                            pred_rf = rf.predict(X_test)
                            pred_gb = gb.predict(X_test)
                            pred_lr = lr.predict(X_test)
                            
                            # Combinar predicciones (promedio simple)
                            ensemble_pred = (pred_rf + pred_gb + pred_lr) / 3
                            
                            # Almacenar predicciones
                            predictions.iloc[test_idx] = ensemble_pred
                        
                        # Normalizar predicciones
                        predictions = (predictions - predictions.mean()) / predictions.std()
                        
                        # Almacenar señales combinadas
                        self.combined_signals.loc[predictions.index, symbol] = predictions
        except Exception as e:
            logging.error(f"Error en _train_ensemble_models: {str(e)}", exc_info=True)
            raise
    
    def _normalize_signals(self):
        """Normaliza las señales combinadas usando winsorización adaptativa."""
        try:
            # Para cada fecha, normalizar señales entre activos
            for date in self.combined_signals.index:
                signals = self.combined_signals.loc[date].dropna()
                
                if len(signals) > 10:  # Asegurar suficientes datos
                    # Calcular MAD (Median Absolute Deviation)
                    median = signals.median()
                    mad = (signals - median).abs().median()
                    
                    # Winsorizar usando MAD (±3 MAD)
                    lower_bound = median - 3 * mad
                    upper_bound = median + 3 * mad
                    
                    # Aplicar límites
                    winsorized = signals.clip(lower=lower_bound, upper=upper_bound)
                    
                    # Normalizar a rango [-1, 1]
                    if winsorized.max() > winsorized.min():
                        normalized = 2 * (winsorized - winsorized.min()) / (winsorized.max() - winsorized.min()) - 1
                        self.combined_signals.loc[date, normalized.index] = normalized
        except Exception as e:
            logging.error(f"Error en _normalize_signals: {str(e)}", exc_info=True)
            raise
    
    def _calculate_position_sizes(self):
        """Calcula tamaños de posición basados en señales y restricciones."""
        try:
            # Inicializar DataFrame para posiciones
            self.positions = pd.DataFrame(0, index=self.prices.index, columns=self.prices.columns)
            
            # Calcular drawdown del portafolio
            portfolio_value = (1 + self.benchmark_returns).cumprod()
            rolling_max = portfolio_value.rolling(window=252, min_periods=1).max()
            drawdown = (portfolio_value / rolling_max) - 1
            
            # Para cada fecha, calcular posiciones
            for i, date in enumerate(self.positions.index[252:], 252):
                # Obtener señales para la fecha actual
                signals = self.combined_signals.loc[date].dropna()
                
                if len(signals) > 0:
                    # Ajustar exposición basada en drawdown
                    current_drawdown = abs(drawdown.loc[date])
                    drawdown_factor = max(0, 1 - (current_drawdown / self.max_drawdown_tolerance)**2)
                    
                    # Ajustar exposición basada en régimen de mercado
                    regime = self.market_regimes.loc[date, 'regime']
                    regime_factors = {1: 1.0, 2: 0.5, 3: 0.7, 4: 0.8}
                    regime_factor = regime_factors.get(regime, 0.5)
                    
                    # Calcular factor de exposición total
                    exposure_factor = drawdown_factor * regime_factor
                    
                    # Filtrar señales por umbral
                    filtered_signals = signals[abs(signals) > self.transaction_threshold]
                    
                    if len(filtered_signals) > 0:
                        # Inicializar exposición por sector
                        sector_exposure = {sector: 0 for sector in self.sectors.unique()}
                        
                        # Ordenar señales por magnitud
                        sorted_signals = filtered_signals.abs().sort_values(ascending=False)
                        
                        # Asignar posiciones respetando restricciones
                        for symbol in sorted_signals.index:
                            signal = signals[symbol]
                            sector = self.sectors.get(symbol, 'Unknown')
                            
                            # Verificar restricciones de sector
                            if sector_exposure[sector] < self.sector_max_allocation:
                                # Calcular tamaño de posición
                                position_size = min(
                                    self.max_position_size,
                                    self.sector_max_allocation - sector_exposure[sector]
                                ) * np.sign(signal) * exposure_factor
                                
                                # Actualizar posición
                                self.positions.loc[date, symbol] = position_size
                                
                                # Actualizar exposición del sector
                                sector_exposure[sector] += abs(position_size)
        except Exception as e:
            logging.error(f"Error en _calculate_position_sizes: {str(e)}", exc_info=True)
            raise
    
    def _execute_trades(self):
        """Ejecuta operaciones y calcula retornos del portafolio."""
        try:
            # Inicializar DataFrame para operaciones
            self.trades = pd.DataFrame(index=self.positions.index, columns=['buys', 'sells', 'total'])
            self.trades.fillna(0, inplace=True)
            
            # Inicializar retornos del portafolio
            self.portfolio_returns = pd.Series(0, index=self.positions.index)
            
            # Para cada fecha (excepto la primera), calcular retornos
            for i in range(1, len(self.positions.index)):
                prev_date = self.positions.index[i-1]
                curr_date = self.positions.index[i]
                
                # Obtener posiciones anteriores y actuales
                prev_positions = self.positions.loc[prev_date]
                curr_positions = self.positions.loc[curr_date]
                
                # Calcular cambios en posiciones
                position_changes = curr_positions - prev_positions
                
                # Contar operaciones
                buys = (position_changes > self.transaction_threshold).sum()
                sells = (position_changes < -self.transaction_threshold).sum()
                self.trades.loc[curr_date] = [buys, sells, buys + sells]
                
                # Calcular retornos del portafolio
                if i > 1:  # Necesitamos retornos para calcular el rendimiento
                    daily_returns = self.returns.loc[curr_date]
                    portfolio_return = (prev_positions * daily_returns).sum()
                    
                    # Aplicar costos de transacción (0.1% por operación)
                    transaction_cost = abs(position_changes).sum() * 0.001
                    net_return = portfolio_return - transaction_cost
                    
                    self.portfolio_returns.loc[curr_date] = net_return
            
            # Calcular valor del portafolio
            self.portfolio_value = (1 + self.portfolio_returns).cumprod()
        except Exception as e:
            logging.error(f"Error en _execute_trades: {str(e)}", exc_info=True)
            raise
    
    def _calculate_performance_metrics(self):
        """Calcula métricas de rendimiento del portafolio."""
        try:
            # Eliminar primeros 252 días (período de calentamiento)
            returns = self.portfolio_returns[252:].copy()
            benchmark_returns = self.benchmark_returns[252:].copy()
            
            # Calcular métricas anualizadas
            ann_factor = 252
            
            # Retorno anualizado
            self.performance_metrics['annual_return'] = returns.mean() * ann_factor
            self.performance_metrics['benchmark_annual_return'] = benchmark_returns.mean() * ann_factor
            
            # Volatilidad anualizada
            self.performance_metrics['annual_volatility'] = returns.std() * np.sqrt(ann_factor)
            self.performance_metrics['benchmark_annual_volatility'] = benchmark_returns.std() * np.sqrt(ann_factor)
            
            # Ratio de Sharpe (asumiendo tasa libre de riesgo = 0)
            self.performance_metrics['sharpe_ratio'] = self.performance_metrics['annual_return'] / self.performance_metrics['annual_volatility']
            self.performance_metrics['benchmark_sharpe_ratio'] = self.performance_metrics['benchmark_annual_return'] / self.performance_metrics['benchmark_annual_volatility']
            
            # Drawdown máximo
            portfolio_value = (1 + returns).cumprod()
            benchmark_value = (1 + benchmark_returns).cumprod()
            
            rolling_max = portfolio_value.rolling(window=ann_factor, min_periods=1).max()
            benchmark_rolling_max = benchmark_value.rolling(window=ann_factor, min_periods=1).max()
            
            drawdown = (portfolio_value / rolling_max) - 1
            benchmark_drawdown = (benchmark_value / benchmark_rolling_max) - 1
            
            self.performance_metrics['max_drawdown'] = drawdown.min()
            self.performance_metrics['benchmark_max_drawdown'] = benchmark_drawdown.min()
            
            # Ratio de Sortino (solo considera volatilidad negativa)
            negative_returns = returns[returns < 0]
            benchmark_negative_returns = benchmark_returns[benchmark_returns < 0]
            
            downside_deviation = negative_returns.std() * np.sqrt(ann_factor)
            benchmark_downside_deviation = benchmark_negative_returns.std() * np.sqrt(ann_factor)
            
            self.performance_metrics['sortino_ratio'] = self.performance_metrics['annual_return'] / downside_deviation
            self.performance_metrics['benchmark_sortino_ratio'] = self.performance_metrics['benchmark_annual_return'] / benchmark_downside_deviation
            
            # Calmar ratio (retorno anualizado / max drawdown absoluto)
            self.performance_metrics['calmar_ratio'] = self.performance_metrics['annual_return'] / abs(self.performance_metrics['max_drawdown'])
            self.performance_metrics['benchmark_calmar_ratio'] = self.performance_metrics['benchmark_annual_return'] / abs(self.performance_metrics['benchmark_max_drawdown'])
            
            # Información de operaciones
            self.performance_metrics['total_trades'] = self.trades['total'].sum()
            self.performance_metrics['avg_trades_per_day'] = self.trades['total'].mean()
            
            # Beta y Alpha
            cov_matrix = np.cov(returns, benchmark_returns)
            beta = cov_matrix[0, 1] / cov_matrix[1, 1]
            alpha = self.performance_metrics['annual_return'] - beta * self.performance_metrics['benchmark_annual_return']
            
            self.performance_metrics['beta'] = beta
            self.performance_metrics['alpha'] = alpha
            
            # Information Ratio
            tracking_error = (returns - benchmark_returns).std() * np.sqrt(ann_factor)
            self.performance_metrics['information_ratio'] = (self.performance_metrics['annual_return'] - self.performance_metrics['benchmark_annual_return']) / tracking_error
        except Exception as e:
            logging.error(f"Error en _calculate_performance_metrics: {str(e)}", exc_info=True)
            raise
    
    def _plot_results(self):
        """Genera gráficos de resultados."""
        try:
            # Crear figura para valor del portafolio vs benchmark
            plt.figure(figsize=(12, 6))
            
            # Normalizar valores a 100
            portfolio_norm = self.portfolio_value / self.portfolio_value.iloc[252] * 100
            benchmark_norm = (1 + self.benchmark_returns).cumprod() / (1 + self.benchmark_returns).cumprod().iloc[252] * 100
            
            plt.plot(portfolio_norm[252:], label='Estrategia')
            plt.plot(benchmark_norm[252:], label='S&P 500', alpha=0.7)
            
            plt.title('Rendimiento de la Estrategia vs S&P 500')
            plt.xlabel('Fecha')
            plt.ylabel('Valor (Base 100)')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.savefig('./artifacts/results/figures/portfolio_performance.png')
            plt.close()
            
            # Crear figura para drawdown
            plt.figure(figsize=(12, 6))
            
            portfolio_value = (1 + self.portfolio_returns).cumprod()
            rolling_max = portfolio_value.rolling(window=252, min_periods=1).max()
            drawdown = (portfolio_value / rolling_max) - 1
            
            benchmark_value = (1 + self.benchmark_returns).cumprod()
            benchmark_rolling_max = benchmark_value.rolling(window=252, min_periods=1).max()
            benchmark_drawdown = (benchmark_value / benchmark_rolling_max) - 1
            
            plt.plot(drawdown[252:], label='Estrategia')
            plt.plot(benchmark_drawdown[252:], label='S&P 500', alpha=0.7)
            
            plt.title('Drawdown de la Estrategia vs S&P 500')
            plt.xlabel('Fecha')
            plt.ylabel('Drawdown')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.savefig('./artifacts/results/figures/drawdown.png')
            plt.close()
            
            # Crear figura para regímenes de mercado
            plt.figure(figsize=(12, 6))
            
            regimes = self.market_regimes['regime']
            
            # Crear un mapa de colores para los regímenes
            colors = {1: 'green', 2: 'red', 3: 'blue', 4: 'orange'}
            regime_colors = [colors.get(r, 'gray') for r in regimes]
            
            # Crear un scatter plot con colores por régimen
            plt.scatter(regimes.index, [1] * len(regimes), c=regime_colors, s=10)
            
            # Añadir línea de rendimiento del benchmark
            ax1 = plt.gca()
            ax2 = ax1.twinx()
            ax2.plot(benchmark_value, 'k-', alpha=0.3)
            
            plt.title('Regímenes de Mercado')
            plt.xlabel('Fecha')
            ax1.set_ylabel('Régimen')
            ax2.set_ylabel('S&P 500')
            
            # Crear leyenda manual
            from matplotlib.lines import Line2D
            legend_elements = [
                Line2D([0], [0], marker='o', color='w', markerfacecolor='green', markersize=10, label='Bull Market'),
                Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Bear Market'),
                Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='Sideways/Low Vol'),
                Line2D([0], [0], marker='o', color='w', markerfacecolor='orange', markersize=10, label='Volatile Bull')
            ]
            ax1.legend(handles=legend_elements, loc='upper left')
            
            plt.savefig('./artifacts/results/figures/market_regimes.png')
            plt.close()
            
            # Crear figura para operaciones diarias
            plt.figure(figsize=(12, 6))
            
            plt.bar(self.trades.index[252:], self.trades['buys'][252:], label='Compras', alpha=0.7, color='green')
            plt.bar(self.trades.index[252:], self.trades['sells'][252:], label='Ventas', alpha=0.7, color='red')
            
            plt.title('Operaciones Diarias')
            plt.xlabel('Fecha')
            plt.ylabel('Número de Operaciones')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.savefig('./artifacts/results/figures/daily_trades.png')
            plt.close()
            
            # Crear figura para exposición por sector
            plt.figure(figsize=(12, 6))
            
            # Calcular exposición por sector para cada fecha
            sector_exposure = pd.DataFrame(index=self.positions.index[252:])
            
            for sector in self.sectors.unique():
                sector_symbols = self.sectors[self.sectors == sector].index
                sector_exposure[sector] = self.positions.loc[self.positions.index[252:], sector_symbols].abs().sum(axis=1)
            
            # Graficar exposición por sector
            sector_exposure.plot(kind='area', stacked=True, alpha=0.7, figsize=(12, 6))
            
            plt.title('Exposición por Sector')
            plt.xlabel('Fecha')
            plt.ylabel('Exposición')
            plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig('./artifacts/results/figures/sector_exposure.png')
            plt.close()
        except Exception as e:
            logging.error(f"Error en _plot_results: {str(e)}", exc_info=True)
            raise
    
    def _save_results(self):
        """Guarda resultados en archivos."""
        try:
            # Guardar métricas de rendimiento
            pd.Series(self.performance_metrics).to_csv('./artifacts/results/data/performance_metrics.csv')
            
            # Guardar retornos diarios
            self.portfolio_returns.to_csv('./artifacts/results/data/portfolio_returns.csv')
            
            # Guardar valor del portafolio
            self.portfolio_value.to_csv('./artifacts/results/data/portfolio_value.csv')
            
            # Guardar operaciones
            self.trades.to_csv('./artifacts/results/data/trades.csv')
            
            # Guardar posiciones finales
            final_positions = self.positions.iloc[-1].sort_values(ascending=False)
            final_positions[final_positions != 0].to_csv('./artifacts/results/data/final_positions.csv')
            
            # Guardar regímenes de mercado
            self.market_regimes.to_csv('./artifacts/results/data/market_regimes.csv')
        except Exception as e:
            logging.error(f"Error en _save_results: {str(e)}", exc_info=True)
            raise
    
    def backtest(self):
        """Ejecuta el backtest completo de la estrategia."""
        try:
            print("Iniciando backtest...")
            
            # Paso 1: Calcular retornos
            print("Calculando retornos...")
            self._calculate_returns()
            
            # Paso 2: Calcular señales de momentum
            print("Calculando señales de momentum...")
            self._calculate_momentum_signals()
            
            # Paso 3: Calcular retornos residuales
            print("Calculando retornos residuales...")
            self._calculate_residual_returns()
            
            # Paso 4: Identificar regímenes de mercado
            print("Identificando regímenes de mercado...")
            self._identify_market_regimes()
            
            # Paso 5: Entrenar modelos de ensemble
            print("Entrenando modelos de ensemble...")
            self._train_ensemble_models()
            
            # Paso 6: Normalizar señales
            print("Normalizando señales...")
            self._normalize_signals()
            
            # Paso 7: Calcular tamaños de posición
            print("Calculando tamaños de posición...")
            self._calculate_position_sizes()
            
            # Paso 8: Ejecutar operaciones
            print("Ejecutando operaciones...")
            self._execute_trades()
            
            # Paso 9: Calcular métricas de rendimiento
            print("Calculando métricas de rendimiento...")
            self._calculate_performance_metrics()
            
            # Paso 10: Generar gráficos
            print("Generando gráficos...")
            self._plot_results()
            
            # Paso 11: Guardar resultados
            print("Guardando resultados...")
            self._save_results()
            
            print("Backtest completado.")
            
            # Mostrar resumen de rendimiento
            print("\nResumen de Rendimiento:")
            print(f"Retorno Anualizado: {self.performance_metrics['annual_return']:.2%}")
            print(f"Volatilidad Anualizada: {self.performance_metrics['annual_volatility']:.2%}")
            print(f"Ratio de Sharpe: {self.performance_metrics['sharpe_ratio']:.2f}")
            print(f"Drawdown Máximo: {self.performance_metrics['max_drawdown']:.2%}")
            print(f"Alpha: {self.performance_metrics['alpha']:.2%}")
            print(f"Beta: {self.performance_metrics['beta']:.2f}")
            print(f"Operaciones Totales: {self.performance_metrics['total_trades']}")
            
            return self.performance_metrics
        except Exception as e:
            logging.error(f"Error en backtest: {str(e)}", exc_info=True)
            raise
    
    def walk_forward_test(self, train_size=252*3, test_size=252, step_size=126):
        """
        Realiza un test walk-forward de la estrategia.
        
        Args:
            train_size (int): Tamaño de la ventana de entrenamiento en días
            test_size (int): Tamaño de la ventana de prueba en días
            step_size (int): Tamaño del paso para avanzar la ventana en días
        
        Returns:
            pd.Series: Retornos del portafolio en el período de prueba
        """
        try:
            print("Iniciando test walk-forward...")
            
            # Asegurar que hay suficientes datos
            if len(self.prices) < train_size + test_size:
                raise ValueError("No hay suficientes datos para el test walk-forward")
            
            # Inicializar DataFrame para retornos out-of-sample
            oos_returns = pd.Series(dtype=float)
            
            # Calcular retornos una vez (no cambian)
            self._calculate_returns()
            
            # Iterar sobre ventanas de tiempo
            start_idx = 0
            while start_idx + train_size + test_size <= len(self.prices):
                # Definir índices de entrenamiento y prueba
                train_end_idx = start_idx + train_size
                test_end_idx = train_end_idx + test_size
                
                train_dates = self.prices.index[start_idx:train_end_idx]
                test_dates = self.prices.index[train_end_idx:test_end_idx]
                
                print(f"Entrenando en: {train_dates[0]} a {train_dates[-1]}")
                print(f"Probando en: {test_dates[0]} a {test_dates[-1]}")
                
                # Crear una instancia temporal para esta ventana
                temp_strategy = AdaptiveMomentumStrategy(
                    start_date=train_dates[0].strftime('%Y-%m-%d'),
                    end_date=test_dates[-1].strftime('%Y-%m-%d')
                )
                
                # Ejecutar backtest en datos de entrenamiento
                temp_strategy.prices = self.prices.loc[train_dates]
                temp_strategy.benchmark_prices = self.benchmark_prices.loc[train_dates]
                temp_strategy.sectors = self.sectors
                
                # Calcular señales usando solo datos de entrenamiento
                temp_strategy._calculate_returns()
                temp_strategy._calculate_momentum_signals()
                temp_strategy._calculate_residual_returns()
                temp_strategy._identify_market_regimes()
                temp_strategy._train_ensemble_models()
                
                # Aplicar modelo a datos de prueba
                test_prices = self.prices.loc[test_dates]
                test_returns = self.returns.loc[test_dates]
                test_benchmark = self.benchmark_prices.loc[test_dates]
                
                # Calcular señales para período de prueba
                for period in self.lookback_periods:
                    # Usar precios de entrenamiento + prueba para calcular momentum
                    all_prices = pd.concat([temp_strategy.prices, test_prices])
                    cum_returns = all_prices.pct_change(period)
                    
                    # Extraer solo señales del período de prueba
                    test_signals = cum_returns.loc[test_dates]
                    
                    # Normalizar
                    normalized_signals = test_signals.apply(lambda x: (x - x.mean()) / x.std(), axis=1)
                    normalized_signals = normalized_signals.apply(lambda x: np.clip(x, -3, 3), axis=1)
                    
                    temp_strategy.momentum_signals[period] = normalized_signals
                
                # Calcular regímenes para período de prueba
                all_benchmark = pd.concat([temp_strategy.benchmark_prices, test_benchmark])
                all_benchmark_returns = all_benchmark.pct_change()
                
                annual_return = all_benchmark.pct_change(temp_strategy.regime_lookback)
                rolling_std = all_benchmark_returns.rolling(temp_strategy.vol_lookback).std() * np.sqrt(252)
                
                # Extraer solo regímenes del período de prueba
                test_regimes = pd.DataFrame(index=test_dates)
                
                return_threshold = annual_return.rolling(252).median()
                vol_threshold = rolling_std.rolling(252).median()
                
                regimes = pd.Series(index=test_dates, dtype=int)
                
                for date in test_dates:
                    if pd.notna(annual_return.loc[date]) and pd.notna(rolling_std.loc[date]):
                        if pd.notna(return_threshold.loc[date]) and pd.notna(vol_threshold.loc[date]):
                            if annual_return.loc[date] > return_threshold.loc[date]:
                                if rolling_std.loc[date] > vol_threshold.loc[date]:
                                    regimes.loc[date] = 4  # Volatile bull
                                else:
                                    regimes.loc[date] = 1  # Bull market
                            else:
                                if rolling_std.loc[date] > vol_threshold.loc[date]:
                                    regimes.loc[date] = 2  # Bear market
                                else:
                                    regimes.loc[date] = 3  # Sideways/low vol
                
                regimes = regimes.fillna(method='ffill')
                test_regimes['regime'] = regimes
                
                temp_strategy.market_regimes = test_regimes
                temp_strategy.volatility = rolling_std.loc[test_dates]
                
                # Aplicar modelo entrenado a datos de prueba
                temp_strategy.combined_signals = pd.DataFrame(index=test_dates, columns=test_prices.columns)
                
                # Usar modelos entrenados para generar señales en período de prueba
                for symbol in test_prices.columns:
                    # Preparar datos de prueba
                    X_test = pd.DataFrame(index=test_dates)
                    
                    # Añadir señales de momentum como features
                    for period in temp_strategy.lookback_periods:
                        if symbol in temp_strategy.momentum_signals[period].columns:
                            X_test[f'momentum_{period}'] = temp_strategy.momentum_signals[period][symbol]
                    
                    # Añadir retornos residuales como feature (calcular para período de prueba)
                    if symbol in test_returns.columns and not test_returns[symbol].isna().all():
                        # Calcular residuales para período de prueba
                        asset_returns = test_returns[symbol]
                        benchmark_returns = all_benchmark_returns.loc[test_dates]
                        
                        valid_data = pd.concat([asset_returns, benchmark_returns], axis=1).dropna()
                        
                        if len(valid_data) > 10:
                            X = valid_data.iloc[:, 1].values.reshape(-1, 1)
                            y = valid_data.iloc[:, 0].values
                            
                            model = LinearRegression()
                            model.fit(X, y)
                            
                            predicted = model.predict(X)
                            residuals = y - predicted
                            
                            for i, date in enumerate(valid_data.index):
                                X_test.loc[date, 'residual'] = residuals[i]
                    
                    # Añadir régimen de mercado como feature categórica
                    X_test['regime'] = temp_strategy.market_regimes['regime']
                    
                    # Convertir régimen a variables dummy
                    regime_dummies = pd.get_dummies(X_test['regime'], prefix='regime')
                    X_test = pd.concat([X_test.drop('regime', axis=1), regime_dummies], axis=1)
                    
                    # Eliminar filas con NaN
                    X_test = X_test.dropna()
                    
                    if len(X_test) > 0:
                        # Usar modelos entrenados para predecir
                        rf = RandomForestRegressor(n_estimators=100, max_depth=3, random_state=42)
                        gb = GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42)
                        lr = LinearRegression()
                        
                        # Entrenar con todos los datos de entrenamiento
                        X_train = pd.DataFrame(index=train_dates)
                        
                        for period in temp_strategy.lookback_periods:
                            if symbol in temp_strategy.momentum_signals[period].columns:
                                X_train[f'momentum_{period}'] = temp_strategy.momentum_signals[period][symbol]
                        
                        if 'residual' in X_test.columns:
                            X_train['residual'] = temp_strategy.residual_returns[symbol]
                        
                        X_train['regime'] = temp_strategy.market_regimes['regime']
                        
                        train_regime_dummies = pd.get_dummies(X_train['regime'], prefix='regime')
                        X_train = pd.concat([X_train.drop('regime', axis=1), train_regime_dummies], axis=1)
                        
                        X_train = X_train.dropna()
                        
                        if len(X_train) > 30:
                            y_train = temp_strategy.returns[symbol].shift(-5).loc[X_train.index]
                            valid_indices = ~y_train.isna()
                            X_train_valid = X_train.loc[valid_indices]
                            y_train_valid = y_train.loc[valid_indices]
                            
                            if len(X_train_valid) > 30:
                                # Asegurar que X_test tiene las mismas columnas que X_train
                                for col in X_train_valid.columns:
                                    if col not in X_test.columns:
                                        X_test[col] = 0
                                
                                for col in X_test.columns:
                                    if col not in X_train_valid.columns:
                                        X_test = X_test.drop(col, axis=1)
                                
                                # Reordenar columnas para que coincidan
                                X_test = X_test[X_train_valid.columns]
                                
                                # Entrenar modelos
                                rf.fit(X_train_valid, y_train_valid)
                                gb.fit(X_train_valid, y_train_valid)
                                lr.fit(X_train_valid, y_train_valid)
                                
                                # Predecir
                                pred_rf = rf.predict(X_test)
                                pred_gb = gb.predict(X_test)
                                pred_lr = lr.predict(X_test)
                                
                                # Combinar predicciones
                                ensemble_pred = (pred_rf + pred_gb + pred_lr) / 3
                                
                                # Almacenar predicciones
                                for i, date in enumerate(X_test.index):
                                    temp_strategy.combined_signals.loc[date, symbol] = ensemble_pred[i]
                
                # Normalizar señales
                temp_strategy._normalize_signals()
                
                # Calcular posiciones
                temp_strategy._calculate_position_sizes()
                
                # Ejecutar operaciones solo para el período de prueba
                temp_strategy.positions = temp_strategy.positions.loc[test_dates]
                temp_strategy.returns = test_returns
                temp_strategy.benchmark_returns = all_benchmark_returns.loc[test_dates]
                
                temp_strategy._execute_trades()
                
                # Guardar retornos out-of-sample
                oos_returns = pd.concat([oos_returns, temp_strategy.portfolio_returns])
                
                # Avanzar ventana
                start_idx += step_size
            
            # Calcular métricas para retornos out-of-sample
            ann_factor = 252
            annual_return = oos_returns.mean() * ann_factor
            annual_vol = oos_returns.std() * np.sqrt(ann_factor)
            sharpe_ratio = annual_return / annual_vol
            
            # Calcular drawdown
            portfolio_value = (1 + oos_returns).cumprod()
            rolling_max = portfolio_value.rolling(window=ann_factor, min_periods=1).max()
            drawdown = (portfolio_value / rolling_max) - 1
            max_drawdown = drawdown.min()
            
            print("\nResultados Walk-Forward:")
            print(f"Retorno Anualizado: {annual_return:.2%}")
            print(f"Volatilidad Anualizada: {annual_vol:.2%}")
            print(f"Ratio de Sharpe: {sharpe_ratio:.2f}")
            print(f"Drawdown Máximo: {max_drawdown:.2%}")
            
            # Guardar resultados
            oos_returns.to_csv('./artifacts/results/data/walk_forward_returns.csv')
            
            # Crear gráfico de rendimiento walk-forward
            plt.figure(figsize=(12, 6))
            
            portfolio_value = (1 + oos_returns).cumprod()
            
            # Obtener retornos del benchmark para el mismo período
            benchmark_returns = self.benchmark_returns.loc[oos_returns.index]
            benchmark_value = (1 + benchmark_returns).cumprod()
            
            # Normalizar a 100
            portfolio_value = portfolio_value / portfolio_value.iloc[0] * 100
            benchmark_value = benchmark_value / benchmark_value.iloc[0] * 100
            
            plt.plot(portfolio_value, label='Estrategia (Walk-Forward)')
            plt.plot(benchmark_value, label='S&P 500', alpha=0.7)
            
            plt.title('Rendimiento Walk-Forward vs S&P 500')
            plt.xlabel('Fecha')
            plt.ylabel('Valor (Base 100)')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.savefig('./artifacts/results/figures/walk_forward_performance.png')
            plt.close()
            
            return oos_returns
        except Exception as e:
            logging.error(f"Error en walk_forward_test: {str(e)}", exc_info=True)
            raise

# Ejecutar la estrategia
if __name__ == "__main__":
    try:
        # Inicializar estrategia
        strategy = AdaptiveMomentumStrategy(start_date='2015-01-01')
        
        # Ejecutar backtest
        strategy.backtest()
        
        # Ejecutar test walk-forward
        strategy.walk_forward_test()
    except Exception as e:
        logging.error(f"Error al ejecutar la estrategia: {str(e)}", exc_info=True)
        raise
```

Por favor analiza si el código implementa correctamente la idea conceptual y sugiere mejoras específicas.


RESPONSE:
# Revisión de Código: Estrategia de Momentum Adaptativo

## 1. ¿El código implementa correctamente la idea? Parcialmente

El código implementa varios aspectos de la idea conceptual, pero tiene algunas deficiencias importantes y áreas de mejora.

## 2. Sugerencias específicas:

1. **Problema de look-ahead bias en `_train_ensemble_models`**: En la línea 457, se define el target como `y = self.returns[symbol].shift(-5).loc[X.index]`. Esto introduce look-ahead bias porque está usando retornos futuros (5 días adelante) para entrenar el modelo. En un escenario real, estos datos no estarían disponibles en el momento de la decisión.

2. **Alineación temporal incorrecta en `_calculate_residual_returns`**: La función no garantiza que los residuos se calculen usando solo información disponible hasta ese momento. Debería implementarse un enfoque rolling window para evitar usar datos futuros.

3. **Falta de implementación del momentum residual multi-horizonte**: El código calcula señales de momentum pero no implementa correctamente el ajuste por modelo multifactorial como se describe en la idea original. Solo se ajusta por el factor de mercado (benchmark), ignorando los otros factores (Tamaño, Valor, Momentum, Calidad).

4. **Implementación incompleta del HMM**: La idea original especifica un modelo HMM para identificar regímenes, pero el código usa un enfoque heurístico basado en umbrales de volatilidad y retorno. No hay implementación real del modelo de Markov oculto.

5. **Gestión de NaN problemática**: En varias partes del código (por ejemplo, líneas 380-385), se eliminan filas con NaN sin considerar el impacto en la alineación temporal de los datos, lo que puede llevar a resultados incorrectos.

6. **Problema en `walk_forward_test`**: La función intenta usar modelos entrenados para predecir en el período de prueba, pero hay inconsistencias en cómo se manejan las características entre entrenamiento y prueba, especialmente en las líneas 818-824.

7. **Falta de winsorización dinámica basada en MAD**: Aunque se menciona en la idea original, la implementación usa un enfoque estático de winsorización (±3 desviaciones estándar) en lugar del enfoque adaptativo basado en MAD.

8. **Gestión de riesgo incompleta**: No se implementa correctamente el CVaR/Expected Shortfall mencionado en la idea original, y los stops dinámicos contextuales están ausentes.

9. **Problema en la normalización de señales**: En `_normalize_signals`, se normalizan las señales a [-1, 1] sin considerar la dirección de la señal original, lo que puede invertir señales negativas.

10. **Falta de inversión de señal para horizontes cortos**: La idea original menciona invertir señales para horizontes cortos (1-5 días) para capturar reversión a la media, pero esto no está implementado correctamente.

11. **Problema en la simulación de ejecución**: En `_execute_trades`, los costos de transacción se aplican a todos los cambios de posición, pero no se considera el slippage basado en volatilidad y volumen como se menciona en la idea original.

12. **Inconsistencia en períodos de lookback**: La idea original menciona horizontes de 5, 21, 63, 126 días, pero el código usa 20, 60, 120, 252 días sin justificación clara para el cambio.

13. **Falta de pruning dinámico en el ensemble**: No se implementa la eliminación temporal de modelos con rendimiento persistentemente pobre como se menciona en la idea original.

14. **Problema en la obtención de datos de sectores**: La función `_get_sector_data` hace demasiadas llamadas a la API de yfinance, lo que podría causar limitaciones de tasa. Sería mejor usar una sola llamada con `Tickers` para obtener información de múltiples símbolos.

15. **Falta de manejo de errores específicos**: Aunque hay un sistema de logging, muchas excepciones se capturan genéricamente sin manejar casos específicos que podrían requerir diferentes estrategias de recuperación.
