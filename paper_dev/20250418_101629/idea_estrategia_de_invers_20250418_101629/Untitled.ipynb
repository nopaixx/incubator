{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b8923d-1e64-4231-9b46-945fd93271f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  503 of 503 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados exitosamente. 472 símbolos, 2315 días de trading.\n",
      "En promedio, 100.0% de los activos son negociables.\n",
      "En promedio, 70.0% de los activos son susceptibles de posiciones cortas.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 2063/2063 [4:14:35<00:00,  7.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Métricas de Rendimiento:\n",
      "Gross Total Return: 0.1761\n",
      "Gross Annualized Return: 0.0200\n",
      "Net Annualized Return: 0.0120\n",
      "Annualized Volatility: 0.8489\n",
      "Gross Sharpe Ratio: 0.0236\n",
      "Net Sharpe Ratio: 0.0142\n",
      "Sortino Ratio: 0.0279\n",
      "Calmar Ratio: 0.0208\n",
      "Maximum Drawdown: 0.9602\n",
      "Annualized Turnover: 12.7341\n",
      "Estimated Transaction Costs: 0.0064\n",
      "Estimated Short Costs: 0.0016\n",
      "Positive Months (%): 0.5859\n",
      "Number of Trades: 98.0000\n",
      "Gráficos guardados en ./artifacts/results/figures/\n",
      "\n",
      "Ventana WFA: 2021-09-20 a 2022-03-18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████████████████████████████████████████████████████▊                                      | 295/504 [40:50<33:55,  9.74s/it]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Crear directorios para resultados\n",
    "os.makedirs('./artifacts/results', exist_ok=True)\n",
    "os.makedirs('./artifacts/results/figures', exist_ok=True)\n",
    "os.makedirs('./artifacts/results/data', exist_ok=True)\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    filename='./artifacts/errors.txt',\n",
    "    level=logging.ERROR,\n",
    "    format='[%(asctime)s] %(levelname)s: %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# Suprimir advertencias\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class AdaptiveMultifactorStrategy:\n",
    "    def __init__(self, start_date='2010-01-01', end_date=None, symbols=None, \n",
    "                 lookback_window=252, regime_window=126, n_regimes=3, \n",
    "                 rebalance_freq=21, vol_target=0.10, max_leverage=1.5,\n",
    "                 transaction_cost=0.0005, market_impact=0.1, borrow_cost=0.0002,\n",
    "                 execution_delay=1, use_point_in_time=True):\n",
    "        \"\"\"\n",
    "        Inicializa la estrategia de descomposición multifactorial adaptativa.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        start_date : str\n",
    "            Fecha de inicio para los datos históricos (formato 'YYYY-MM-DD')\n",
    "        end_date : str\n",
    "            Fecha de fin para los datos históricos (formato 'YYYY-MM-DD')\n",
    "        symbols : list\n",
    "            Lista de símbolos a incluir. Si es None, se usa un universo de referencia histórico\n",
    "        lookback_window : int\n",
    "            Ventana de observación para calcular factores latentes (días)\n",
    "        regime_window : int\n",
    "            Ventana para detectar regímenes de mercado (días)\n",
    "        n_regimes : int\n",
    "            Número de regímenes de mercado a detectar\n",
    "        rebalance_freq : int\n",
    "            Frecuencia de rebalanceo en días\n",
    "        vol_target : float\n",
    "            Volatilidad objetivo anualizada\n",
    "        max_leverage : float\n",
    "            Apalancamiento máximo permitido\n",
    "        transaction_cost : float\n",
    "            Costo de transacción como porcentaje del valor operado\n",
    "        market_impact : float\n",
    "            Impacto de mercado como factor de volatilidad diaria\n",
    "        borrow_cost : float\n",
    "            Costo anualizado de tomar posiciones cortas\n",
    "        execution_delay : int\n",
    "            Retraso en días entre decisión y ejecución\n",
    "        use_point_in_time : bool\n",
    "            Usar datos point-in-time para evitar sesgo de supervivencia\n",
    "        \"\"\"\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date if end_date else datetime.now().strftime('%Y-%m-%d')\n",
    "        self.symbols = symbols\n",
    "        self.lookback_window = lookback_window\n",
    "        self.regime_window = regime_window\n",
    "        self.n_regimes = n_regimes\n",
    "        self.rebalance_freq = rebalance_freq\n",
    "        self.vol_target = vol_target\n",
    "        self.max_leverage = max_leverage\n",
    "        \n",
    "        # Nuevos parámetros para implementación realista\n",
    "        self.transaction_cost = transaction_cost  # 5 puntos básicos por default\n",
    "        self.market_impact = market_impact  # Como factor de volatilidad diaria\n",
    "        self.borrow_cost = borrow_cost  # 20 puntos básicos anualizados\n",
    "        self.execution_delay = execution_delay  # 1 día de retraso por default\n",
    "        self.use_point_in_time = use_point_in_time\n",
    "        \n",
    "        # Atributos que se inicializarán más tarde\n",
    "        self.prices = None\n",
    "        self.returns = None\n",
    "        self.factor_loadings = None\n",
    "        self.factor_returns = None\n",
    "        self.regimes = None\n",
    "        self.regime_probs = None\n",
    "        self.optimal_weights = None\n",
    "        self.performance = None\n",
    "        \n",
    "        # Parámetros adicionales para simulación realista\n",
    "        self.tradable_assets = None  # Activos disponibles para trading en cada fecha\n",
    "        self.max_position_size = 0.1  # Tamaño máximo de posición como % del portafolio\n",
    "        self.liquidity_threshold = 1000000  # Volumen mínimo de negociación diario en USD\n",
    "        \n",
    "        # Cargar datos\n",
    "        self._load_data()\n",
    "        \n",
    "    def _load_data(self):\n",
    "        \"\"\"\n",
    "        Carga los datos históricos de precios y calcula retornos.\n",
    "        Incorpora consideraciones de point-in-time para evitar sesgo de supervivencia.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.symbols is None:\n",
    "                if self.use_point_in_time:\n",
    "                    # CORRECCIÓN: Usar un universo de referencia histórico para evitar sesgo de supervivencia\n",
    "                    # En la práctica, esto requeriría una base de datos point-in-time\n",
    "                    print(\"ADVERTENCIA: En una implementación real, se debería usar una base de datos point-in-time.\")\n",
    "                    print(\"Para este ejemplo, se simula usando el universo actual pero con limitaciones.\")\n",
    "                    \n",
    "                    # Obtener S&P 500 actual como sustituto (en la práctica, usar datos históricos)\n",
    "                    sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "                    self.symbols = sp500['Symbol'].tolist()\n",
    "                    \n",
    "                    # Simular cambios en el universo a lo largo del tiempo\n",
    "                    # (esto es una aproximación - idealmente se usaría una base de datos real point-in-time)\n",
    "                    np.random.seed(42)  # Para reproducibilidad\n",
    "                    self.symbols = [s for s in self.symbols if np.random.random() > 0.2]  # Eliminar algunos símbolos aleatoriamente\n",
    "                else:\n",
    "                    # Usar S&P 500 actual (con sesgo de supervivencia)\n",
    "                    sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "                    self.symbols = sp500['Symbol'].tolist()\n",
    "            \n",
    "            # Descargar datos\n",
    "            data = yf.download(self.symbols, start=self.start_date, end=self.end_date)\n",
    "            self.prices = data['Close']\n",
    "            self.volumes = data['Volume']\n",
    "            self.dollar_volumes = self.prices * self.volumes\n",
    "            \n",
    "            # Limpiar y preparar datos\n",
    "            self.prices = self.prices.dropna(axis=1, thresh=int(len(self.prices) * 0.9))  # Eliminar acciones con muchos NaN\n",
    "            self.symbols = list(self.prices.columns)\n",
    "            \n",
    "            # Calcular retornos diarios\n",
    "            self.returns = self.prices.pct_change().dropna()\n",
    "            \n",
    "            # Calcular volatilidades diarias para estimar impacto de mercado\n",
    "            self.daily_vol = self.returns.rolling(21).std().fillna(method='bfill')\n",
    "            \n",
    "            # CORRECCIÓN: Crear un DataFrame para simular disponibilidad de activos en cada fecha\n",
    "            # En una implementación real, esto se basaría en datos históricos precisos\n",
    "            self.tradable_universe = pd.DataFrame(\n",
    "                True, \n",
    "                index=self.returns.index, \n",
    "                columns=self.returns.columns\n",
    "            )\n",
    "            \n",
    "            # Simular disponibilidad para posiciones cortas (más restrictiva)\n",
    "            # En realidad, esto sería una base de datos real de disponibilidad para cortos\n",
    "            self.shortable_universe = pd.DataFrame(\n",
    "                np.random.random(self.tradable_universe.shape) > 0.3,  # Solo ~70% de activos disponibles para cortos\n",
    "                index=self.tradable_universe.index,\n",
    "                columns=self.tradable_universe.columns\n",
    "            )\n",
    "            \n",
    "            # Filtrar por liquidez mínima\n",
    "            if 'Volume' in data.columns and self.use_point_in_time:\n",
    "                volumes = data['Volume']\n",
    "                dollar_volumes = volumes * self.prices\n",
    "                \n",
    "                # Marca como no negociables los activos con baja liquidez\n",
    "                for date in self.tradable_universe.index:\n",
    "                    low_liquidity = dollar_volumes.loc[date] < self.liquidity_threshold\n",
    "                    self.tradable_universe.loc[date, low_liquidity] = False\n",
    "                    # Si no es negociable, tampoco es shortable\n",
    "                    self.shortable_universe.loc[date, low_liquidity] = False\n",
    "            \n",
    "            print(f\"Datos cargados exitosamente. {len(self.symbols)} símbolos, {len(self.returns)} días de trading.\")\n",
    "            print(f\"En promedio, {self.tradable_universe.mean().mean()*100:.1f}% de los activos son negociables.\")\n",
    "            print(f\"En promedio, {self.shortable_universe.mean().mean()*100:.1f}% de los activos son susceptibles de posiciones cortas.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al cargar datos: {str(e)}\", exc_info=True)\n",
    "            raise\n",
    "    \n",
    "    def extract_latent_factors(self, returns_window, n_components=None):\n",
    "        \"\"\"\n",
    "        Extrae factores latentes de los retornos usando PCA.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        returns_window : DataFrame\n",
    "            Ventana de retornos para extraer factores\n",
    "        n_components : int, opcional\n",
    "            Número de componentes a extraer. Si es None, se determina automáticamente.\n",
    "            \n",
    "        Retorna:\n",
    "        --------\n",
    "        factor_loadings : ndarray\n",
    "            Cargas de los factores latentes\n",
    "        factor_returns : DataFrame\n",
    "            Retornos de los factores latentes\n",
    "        n_components : int\n",
    "            Número de componentes utilizados\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Manejar valores faltantes\n",
    "            returns_filled = returns_window.copy()\n",
    "            \n",
    "            # Usar imputación por media móvil para NaNs\n",
    "            for col in returns_filled.columns:\n",
    "                mask = returns_filled[col].isna()\n",
    "                if mask.any():\n",
    "                    returns_filled.loc[mask, col] = returns_filled[col].rolling(5, min_periods=1).mean()[mask]\n",
    "            \n",
    "            # Si aún hay NaNs, rellenar con ceros\n",
    "            returns_filled = returns_filled.fillna(0)\n",
    "            \n",
    "            # Determinar número óptimo de componentes si no se especifica\n",
    "            if n_components is None:\n",
    "                n_components = self.find_optimal_components(returns_filled)\n",
    "            \n",
    "            # Aplicar PCA\n",
    "            pca = PCA(n_components=n_components)\n",
    "            factor_returns_np = pca.fit_transform(returns_filled)\n",
    "            \n",
    "            # Convertir a DataFrame\n",
    "            factor_returns = pd.DataFrame(\n",
    "                factor_returns_np, \n",
    "                index=returns_window.index,\n",
    "                columns=[f'Factor_{i+1}' for i in range(n_components)]\n",
    "            )\n",
    "            \n",
    "            return pca.components_, factor_returns, n_components\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en extract_latent_factors: {str(e)}\", exc_info=True)\n",
    "            raise\n",
    "    \n",
    "    def find_optimal_components(self, returns_window, threshold=0.80, max_components=15):\n",
    "        \"\"\"\n",
    "        Determina el número óptimo de componentes principales.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        returns_window : DataFrame\n",
    "            Ventana de retornos para analizar\n",
    "        threshold : float\n",
    "            Umbral de varianza explicada acumulada\n",
    "        max_components : int\n",
    "            Número máximo de componentes a considerar\n",
    "            \n",
    "        Retorna:\n",
    "        --------\n",
    "        n_components : int\n",
    "            Número óptimo de componentes\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Limitar el máximo posible de componentes\n",
    "            max_possible = min(returns_window.shape[1], returns_window.shape[0], max_components)\n",
    "            \n",
    "            # Calcular varianza explicada para diferentes números de componentes\n",
    "            pca = PCA(n_components=max_possible)\n",
    "            pca.fit(returns_window)\n",
    "            \n",
    "            # Encontrar el número de componentes que explican al menos threshold de la varianza\n",
    "            explained_variance_ratio_cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "            n_components = np.argmax(explained_variance_ratio_cumsum >= threshold) + 1\n",
    "            \n",
    "            # Asegurar un mínimo de componentes\n",
    "            n_components = max(n_components, 3)\n",
    "            \n",
    "            return n_components\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en find_optimal_components: {str(e)}\", exc_info=True)\n",
    "            # Valor por defecto en caso de error\n",
    "            return 5\n",
    "    \n",
    "    def detect_regimes(self, factor_returns, n_regimes=None):\n",
    "        \"\"\"\n",
    "        Detecta regímenes de mercado usando modelos de mezcla gaussiana.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        factor_returns : DataFrame\n",
    "            Retornos de los factores latentes\n",
    "        n_regimes : int, opcional\n",
    "            Número de regímenes a detectar. Si es None, se usa self.n_regimes.\n",
    "            \n",
    "        Retorna:\n",
    "        --------\n",
    "        regimes : ndarray\n",
    "            Etiquetas de régimen para cada punto temporal\n",
    "        regime_probs : ndarray\n",
    "            Probabilidades de pertenencia a cada régimen\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if n_regimes is None:\n",
    "                n_regimes = self.n_regimes\n",
    "            \n",
    "            # Calcular volatilidad y correlación\n",
    "            vol = factor_returns.rolling(21).std().dropna()\n",
    "            \n",
    "            # Crear características para el modelo de regímenes\n",
    "            features = vol.copy()\n",
    "            \n",
    "            # Estandarizar características\n",
    "            scaler = StandardScaler()\n",
    "            features_scaled = scaler.fit_transform(features)\n",
    "            \n",
    "            # Ajustar modelo de mezcla gaussiana\n",
    "            gmm = GaussianMixture(\n",
    "                n_components=n_regimes,\n",
    "                covariance_type='full',\n",
    "                random_state=42,\n",
    "                n_init=10\n",
    "            )\n",
    "            \n",
    "            # Manejar NaNs\n",
    "            features_scaled_clean = np.nan_to_num(features_scaled)\n",
    "            \n",
    "            # Ajustar modelo\n",
    "            gmm.fit(features_scaled_clean)\n",
    "            \n",
    "            # Predecir regímenes y probabilidades\n",
    "            regimes = gmm.predict(features_scaled_clean)\n",
    "            regime_probs = gmm.predict_proba(features_scaled_clean)\n",
    "            \n",
    "            return regimes, regime_probs\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en detect_regimes: {str(e)}\", exc_info=True)\n",
    "            # Valores por defecto en caso de error\n",
    "            dummy_regimes = np.zeros(len(factor_returns) - 20)\n",
    "            dummy_probs = np.ones((len(factor_returns) - 20, self.n_regimes)) / self.n_regimes\n",
    "            return dummy_regimes, dummy_probs\n",
    "    \n",
    "    def predict_expected_returns(self, returns_window, regimes, current_regime_probs, horizon=5):\n",
    "        \"\"\"\n",
    "        CORRECCIÓN: Método de predicción de retornos mejorado que elimina el look-ahead bias\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        returns_window : DataFrame\n",
    "            Ventana histórica de retornos\n",
    "        regimes : ndarray\n",
    "            Etiquetas de régimen para cada punto temporal\n",
    "        current_regime_probs : ndarray\n",
    "            Probabilidades de pertenencia a cada régimen en el momento actual\n",
    "        horizon : int\n",
    "            Horizonte de predicción en días\n",
    "            \n",
    "        Retorna:\n",
    "        --------\n",
    "        expected_returns : Series\n",
    "            Retornos esperados para cada activo\n",
    "        prediction_confidence : Series\n",
    "            Confianza en las predicciones\n",
    "        \"\"\"\n",
    "        try:\n",
    "            n_assets = returns_window.shape[1]\n",
    "            \n",
    "            # Inicializar arrays para almacenar retornos esperados por régimen\n",
    "            regime_expected_returns = np.zeros((self.n_regimes, n_assets))\n",
    "            regime_counts = np.zeros(self.n_regimes)\n",
    "            \n",
    "            # Para cada régimen, calcular retornos esperados basados en datos históricos\n",
    "            for r in range(self.n_regimes):\n",
    "                # Encontrar índices donde el régimen es r\n",
    "                regime_indices = np.where(regimes == r)[0]\n",
    "                regime_counts[r] = len(regime_indices)\n",
    "                \n",
    "                if len(regime_indices) > 0:\n",
    "                    # Calcular retornos promedio SIGUIENTES a cada régimen\n",
    "                    all_future_returns = []\n",
    "                    \n",
    "                    for idx in regime_indices:\n",
    "                        # CORRECCIÓN: Sólo considerar datos completos para evitar look-ahead bias\n",
    "                        if idx + horizon < len(returns_window) - 1:\n",
    "                            # Próximos 'horizon' días después de este régimen\n",
    "                            future_returns = returns_window.iloc[idx+1:idx+1+horizon].values\n",
    "                            # Calcular retorno acumulado\n",
    "                            cum_returns = np.prod(1 + future_returns, axis=0) - 1\n",
    "                            all_future_returns.append(cum_returns)\n",
    "                    \n",
    "                    if all_future_returns:\n",
    "                        # Promediar los retornos futuros para este régimen\n",
    "                        all_future_returns = np.array(all_future_returns)\n",
    "                        regime_expected_returns[r] = np.mean(all_future_returns, axis=0)\n",
    "            \n",
    "            # Calcular retornos esperados ponderados por régimen actual\n",
    "            expected_returns = np.zeros(n_assets)\n",
    "            for r in range(self.n_regimes):\n",
    "                regime_weight = current_regime_probs[r]\n",
    "                # Ajustar peso por la cantidad de datos (más datos = más confianza)\n",
    "                confidence_weight = min(1.0, regime_counts[r] / 30)\n",
    "                expected_returns += regime_weight * regime_expected_returns[r] * confidence_weight\n",
    "            \n",
    "            # Calcular confianza de predicción\n",
    "            regime_certainty = np.max(current_regime_probs)\n",
    "            data_sufficiency = np.min([count for count in regime_counts if count > 0]) / 30 if any(regime_counts > 0) else 0\n",
    "            prediction_confidence = regime_certainty * data_sufficiency\n",
    "            \n",
    "            # Convertir a Series\n",
    "            expected_returns_series = pd.Series(expected_returns, index=returns_window.columns)\n",
    "            confidence_series = pd.Series(prediction_confidence, index=returns_window.columns)\n",
    "            \n",
    "            return expected_returns_series, confidence_series\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en predict_expected_returns: {str(e)}\", exc_info=True)\n",
    "            # Valores por defecto en caso de error\n",
    "            dummy_returns = pd.Series(0.0001, index=returns_window.columns)\n",
    "            dummy_confidence = pd.Series(0.1, index=returns_window.columns)\n",
    "            return dummy_returns, dummy_confidence\n",
    "    \n",
    "    def optimize_portfolio(self, expected_returns, factor_loadings, prediction_confidence, \n",
    "                          current_regime, regime_certainty, current_date, previous_weights=None, risk_aversion=1.0):\n",
    "        \"\"\"\n",
    "        Optimiza el portafolio considerando restricciones realistas.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        expected_returns : Series\n",
    "            Retornos esperados para cada activo\n",
    "        factor_loadings : ndarray\n",
    "            Cargas de los factores latentes\n",
    "        prediction_confidence : Series\n",
    "            Confianza en las predicciones\n",
    "        current_regime : int\n",
    "            Régimen de mercado actual\n",
    "        regime_certainty : float\n",
    "            Certeza sobre el régimen actual\n",
    "        current_date : Timestamp\n",
    "            Fecha actual para determinar activos negociables\n",
    "        previous_weights : Series, opcional\n",
    "            Pesos del portafolio previo\n",
    "        risk_aversion : float\n",
    "            Parámetro de aversión al riesgo\n",
    "            \n",
    "        Retorna:\n",
    "        --------\n",
    "        weights : Series\n",
    "            Pesos óptimos para cada activo\n",
    "        \"\"\"\n",
    "        try:\n",
    "            n_assets = len(expected_returns)\n",
    "            assets = expected_returns.index\n",
    "            \n",
    "            # CORRECCIÓN: Usar sólo activos negociables en la fecha actual\n",
    "            if current_date in self.tradable_universe.index:\n",
    "                tradable_assets = assets[self.tradable_universe.loc[current_date, assets]]\n",
    "                shortable_assets = assets[self.shortable_universe.loc[current_date, assets]]\n",
    "            else:\n",
    "                # Si la fecha no está en el universo, usar el universo más reciente disponible\n",
    "                last_available = self.tradable_universe.index[self.tradable_universe.index <= current_date][-1]\n",
    "                tradable_assets = assets[self.tradable_universe.loc[last_available, assets]]\n",
    "                shortable_assets = assets[self.shortable_universe.loc[last_available, assets]]\n",
    "            \n",
    "            # Filtrar activos no negociables (establecer retornos esperados a NaN)\n",
    "            filtered_returns = expected_returns.copy()\n",
    "            for asset in assets:\n",
    "                if asset not in tradable_assets:\n",
    "                    filtered_returns[asset] = np.nan\n",
    "            \n",
    "            # Rellenar NaNs con valores muy negativos para evitar seleccionarlos\n",
    "            filtered_returns = filtered_returns.fillna(-999)\n",
    "            \n",
    "            # Ajustar aversión al riesgo según certeza del régimen\n",
    "            adjusted_risk_aversion = risk_aversion * (1.0 + (1.0 - regime_certainty) * 2.0)\n",
    "            \n",
    "            # Calcular matriz de covarianza basada en factores latentes\n",
    "            factor_cov = np.cov(factor_loadings)\n",
    "            asset_cov = factor_loadings.T @ factor_cov @ factor_loadings\n",
    "            \n",
    "            # Asegurar que la matriz es definida positiva\n",
    "            asset_cov = (asset_cov + asset_cov.T) / 2  # Hacer simétrica\n",
    "            min_eig = np.min(np.real(np.linalg.eigvals(asset_cov)))\n",
    "            if min_eig < 1e-6:\n",
    "                asset_cov += np.eye(n_assets) * (1e-6 - min_eig)\n",
    "            \n",
    "            # Ajustar retornos esperados por confianza y por costos de transacción estimados\n",
    "            adjusted_returns = filtered_returns * prediction_confidence\n",
    "            \n",
    "            # Considerar costos de préstamo para posiciones cortas\n",
    "            borrow_costs = pd.Series(0.0, index=assets)\n",
    "            for asset in assets:\n",
    "                if asset not in shortable_assets:\n",
    "                    borrow_costs[asset] = 0.1  # Penalización alta para activos no shortables\n",
    "                else:\n",
    "                    borrow_costs[asset] = self.borrow_cost / 252  # Costo diario de préstamo\n",
    "            \n",
    "            # CORRECCIÓN: Incluir costos de transacción estimados si tenemos pesos previos\n",
    "            if previous_weights is not None:\n",
    "                # Estimar impacto de mercado basado en volatilidad\n",
    "                market_impact_cost = pd.Series(0.0, index=assets)\n",
    "                for asset in assets:\n",
    "                    if asset in self.daily_vol.columns:\n",
    "                        # Estimar impacto como función de la volatilidad diaria y liquidez\n",
    "                        vol = self.daily_vol.loc[current_date, asset] if current_date in self.daily_vol.index else 0.02\n",
    "                        market_impact_cost[asset] = vol * self.market_impact\n",
    "            else:\n",
    "                market_impact_cost = pd.Series(0.0, index=assets)\n",
    "                previous_weights = pd.Series(0.0, index=assets)\n",
    "            \n",
    "            # Función objetivo con costos de transacción incluidos\n",
    "            def objective(weights):\n",
    "                weights_series = pd.Series(weights, index=assets)\n",
    "                \n",
    "                # Retorno esperado\n",
    "                portfolio_return = np.sum(weights_series * adjusted_returns)\n",
    "                \n",
    "                # Riesgo\n",
    "                portfolio_risk = np.sqrt(weights_series.T @ asset_cov @ weights_series)\n",
    "                \n",
    "                # Costos de transacción\n",
    "                turnover = np.sum(np.abs(weights_series - previous_weights))\n",
    "                transaction_costs = turnover * self.transaction_cost\n",
    "                \n",
    "                # Impacto de mercado estimado\n",
    "                impact_costs = np.sum(np.abs(weights_series - previous_weights) * market_impact_cost)\n",
    "                \n",
    "                # Costos de préstamo para posiciones cortas\n",
    "                short_costs = np.sum(np.maximum(-weights_series, 0) * borrow_costs)\n",
    "                \n",
    "                # Utilidad final: retorno - riesgo - costos\n",
    "                utility = portfolio_return - adjusted_risk_aversion * portfolio_risk - transaction_costs - impact_costs - short_costs\n",
    "                \n",
    "                return -utility  # Negativo porque minimizamos\n",
    "            \n",
    "            # Restricciones\n",
    "            constraints = [\n",
    "                {'type': 'eq', 'fun': lambda x: np.sum(x) - 1.0}  # Suma de pesos = 1\n",
    "            ]\n",
    "            \n",
    "            # Límites en las posiciones\n",
    "            bounds = []\n",
    "            for asset in assets:\n",
    "                if asset not in tradable_assets:\n",
    "                    # Activo no negociable\n",
    "                    bounds.append((0, 0))\n",
    "                elif asset not in shortable_assets:\n",
    "                    # Activo negociable pero no shortable\n",
    "                    bounds.append((0, min(1.0, self.max_position_size)))\n",
    "                else:\n",
    "                    # Activo completamente negociable\n",
    "                    # Ajustar límites de posiciones cortas según régimen\n",
    "                    short_limit = -0.2 if current_regime == 0 else -0.1 if current_regime == 1 else 0.0\n",
    "                    bounds.append((max(short_limit, -self.max_position_size), min(1.0, self.max_position_size)))\n",
    "            \n",
    "            # Solución inicial: pesos previos o iguales si no hay previos\n",
    "            if previous_weights is not None and not previous_weights.isna().any():\n",
    "                initial_weights = previous_weights.values\n",
    "            else:\n",
    "                # Sólo asignar a activos negociables\n",
    "                initial_weights = np.zeros(n_assets)\n",
    "                tradable_indices = [i for i, asset in enumerate(assets) if asset in tradable_assets]\n",
    "                if tradable_indices:\n",
    "                    initial_weights[tradable_indices] = 1.0 / len(tradable_indices)\n",
    "            \n",
    "            # Optimizar\n",
    "            result = minimize(\n",
    "                objective,\n",
    "                initial_weights,\n",
    "                method='SLSQP',\n",
    "                bounds=bounds,\n",
    "                constraints=constraints,\n",
    "                options={'maxiter': 1000, 'ftol': 1e-8}\n",
    "            )\n",
    "            \n",
    "            if not result.success:\n",
    "                logging.warning(f\"Optimización no convergió: {result.message}\")\n",
    "                # Usar pesos iniciales como fallback\n",
    "                optimal_weights = pd.Series(initial_weights, index=assets)\n",
    "            else:\n",
    "                optimal_weights = pd.Series(result.x, index=assets)\n",
    "            \n",
    "            # Eliminar posiciones muy pequeñas (menor a 0.1%)\n",
    "            # Esto reduce costos de transacción innecesarios en la práctica\n",
    "            small_positions = np.abs(optimal_weights) < 0.001\n",
    "            optimal_weights[small_positions] = 0.0\n",
    "            \n",
    "            # Renormalizar para asegurar suma = 1.0\n",
    "            if optimal_weights.sum() != 0:\n",
    "                optimal_weights = optimal_weights / optimal_weights.sum()\n",
    "            \n",
    "            # Aplicar control de volatilidad objetivo\n",
    "            portfolio_vol = np.sqrt(optimal_weights.T @ asset_cov @ optimal_weights) * np.sqrt(252)\n",
    "            if portfolio_vol > 0:\n",
    "                vol_scalar = self.vol_target / portfolio_vol\n",
    "            else:\n",
    "                vol_scalar = 1.0\n",
    "            \n",
    "            # Limitar apalancamiento\n",
    "            leverage = min(vol_scalar, self.max_leverage)\n",
    "            \n",
    "            # Ajustar pesos finales\n",
    "            final_weights = optimal_weights * leverage\n",
    "            \n",
    "            return final_weights\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en optimize_portfolio: {str(e)}\", exc_info=True)\n",
    "            # Valor por defecto en caso de error: pesos iguales en activos negociables\n",
    "            if current_date in self.tradable_universe.index:\n",
    "                tradable_mask = self.tradable_universe.loc[current_date, assets]\n",
    "            else:\n",
    "                tradable_mask = pd.Series(True, index=assets)\n",
    "            \n",
    "            default_weights = pd.Series(0.0, index=assets)\n",
    "            tradable_assets = assets[tradable_mask]\n",
    "            if len(tradable_assets) > 0:\n",
    "                default_weights[tradable_assets] = 1.0 / len(tradable_assets)\n",
    "            return default_weights\n",
    "    \n",
    "    def backtest(self, start_date=None, end_date=None):\n",
    "        \"\"\"\n",
    "        Ejecuta un backtest de la estrategia con consideraciones realistas.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        start_date : str, opcional\n",
    "            Fecha de inicio del backtest (formato 'YYYY-MM-DD')\n",
    "        end_date : str, opcional\n",
    "            Fecha de fin del backtest (formato 'YYYY-MM-DD')\n",
    "            \n",
    "        Retorna:\n",
    "        --------\n",
    "        performance : DataFrame\n",
    "            Resultados del backtest incluyendo retornos, drawdowns, etc.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Configurar fechas\n",
    "            if start_date is None:\n",
    "                start_date = self.returns.index[self.lookback_window]\n",
    "            else:\n",
    "                start_date = pd.to_datetime(start_date)\n",
    "            \n",
    "            if end_date is None:\n",
    "                end_date = self.returns.index[-1]\n",
    "            else:\n",
    "                end_date = pd.to_datetime(end_date)\n",
    "            \n",
    "            # Filtrar datos por fechas\n",
    "            mask = (self.returns.index >= start_date) & (self.returns.index <= end_date)\n",
    "            backtest_dates = self.returns.index[mask]\n",
    "            \n",
    "            # Inicializar resultados\n",
    "            portfolio_values = [1.0]\n",
    "            portfolio_returns = []\n",
    "            weights_history = []\n",
    "            regime_history = []\n",
    "            pending_orders = {}  # Para simular retrasos en la ejecución\n",
    "            \n",
    "            # CORRECCIÓN: Inicializar pesos (comenzar con efectivo)\n",
    "            current_weights = pd.Series(0, index=self.returns.columns)\n",
    "            \n",
    "            # Ejecutar backtest\n",
    "            for i, date in enumerate(tqdm(backtest_dates)):\n",
    "                # CORRECCIÓN: Manejar órdenes pendientes (simular retraso en ejecución)\n",
    "                if date in pending_orders:\n",
    "                    order_date, target_weights = pending_orders[date]\n",
    "                    current_weights = target_weights.copy()\n",
    "                    del pending_orders[date]\n",
    "                \n",
    "                # Rebalancear en la primera fecha y luego según frecuencia\n",
    "                if i == 0 or i % self.rebalance_freq == 0:\n",
    "                    # Obtener datos hasta la fecha actual (evitar look-ahead bias)\n",
    "                    current_idx = self.returns.index.get_loc(date)\n",
    "                    history_end_idx = current_idx\n",
    "                    history_start_idx = max(0, history_end_idx - self.lookback_window)\n",
    "                    \n",
    "                    returns_window = self.returns.iloc[history_start_idx:history_end_idx]\n",
    "                    \n",
    "                    # Extraer factores latentes\n",
    "                    factor_loadings, factor_returns, n_components = self.extract_latent_factors(returns_window)\n",
    "                    \n",
    "                    # Detectar regímenes\n",
    "                    regimes, regime_probs = self.detect_regimes(factor_returns)\n",
    "                    \n",
    "                    # CORRECCIÓN: Usar método mejorado de predicción de retornos\n",
    "                    expected_returns, prediction_confidence = self.predict_expected_returns(\n",
    "                        returns_window, regimes, regime_probs[-1], horizon=5\n",
    "                    )\n",
    "                    \n",
    "                    # Optimizar portafolio con restricciones realistas\n",
    "                    current_regime = regimes[-1]\n",
    "                    regime_certainty = np.max(regime_probs[-1])\n",
    "                    \n",
    "                    # Ajustar aversión al riesgo según régimen\n",
    "                    risk_aversion = 1.0 + current_regime * 0.5\n",
    "                    \n",
    "                    # Calcular nuevos pesos objetivo\n",
    "                    target_weights = self.optimize_portfolio(\n",
    "                        expected_returns,\n",
    "                        factor_loadings,\n",
    "                        prediction_confidence,\n",
    "                        current_regime,\n",
    "                        regime_certainty,\n",
    "                        date,\n",
    "                        current_weights,  # Pasar pesos actuales para considerar costos de transacción\n",
    "                        risk_aversion\n",
    "                    )\n",
    "                    \n",
    "                    # CORRECCIÓN: Simular retraso en la ejecución\n",
    "                    if self.execution_delay > 0 and i + self.execution_delay < len(backtest_dates):\n",
    "                        execution_date = backtest_dates[i + self.execution_delay]\n",
    "                        pending_orders[execution_date] = (date, target_weights)\n",
    "                        # No actualizar pesos ahora, se hará cuando llegue la fecha de ejecución\n",
    "                    else:\n",
    "                        # Sin retraso o cerca del final del backtest, ejecutar inmediatamente\n",
    "                        current_weights = target_weights.copy()\n",
    "                    \n",
    "                    # Guardar régimen actual\n",
    "                    regime_history.append(current_regime)\n",
    "                \n",
    "                # CORRECCIÓN: Calcular costos de posiciones cortas\n",
    "                short_positions = current_weights[current_weights < 0]\n",
    "                short_cost = 0\n",
    "                if not short_positions.empty:\n",
    "                    daily_borrow_cost = self.borrow_cost / 252  # Anualizado a diario\n",
    "                    short_cost = (short_positions.abs() * daily_borrow_cost).sum()\n",
    "                \n",
    "                # Calcular retorno del portafolio para el día siguiente (evitar look-ahead bias)\n",
    "                if i + 1 < len(backtest_dates):\n",
    "                    next_date = backtest_dates[i + 1]\n",
    "                    next_returns = self.returns.loc[next_date]\n",
    "                    \n",
    "                    # CORRECCIÓN: Incluir costos de transacción si hubo rebalanceo\n",
    "                    transaction_cost = 0\n",
    "                    if i == 0 or i % self.rebalance_freq == 0:\n",
    "                        # Estimación de turnover: suma de cambios absolutos en los pesos\n",
    "                        weights_before = weights_history[-1] if weights_history else pd.Series(0, index=current_weights.index)\n",
    "                        turnover = np.sum(np.abs(current_weights - weights_before))\n",
    "                        transaction_cost = turnover * self.transaction_cost\n",
    "                    \n",
    "                    # Calcular retorno del portafolio con costos\n",
    "                    portfolio_return = (current_weights * next_returns).sum() - short_cost - transaction_cost\n",
    "                    portfolio_returns.append(portfolio_return)\n",
    "                    \n",
    "                    # Actualizar valor del portafolio\n",
    "                    portfolio_values.append(portfolio_values[-1] * (1 + portfolio_return))\n",
    "                \n",
    "                # Guardar pesos\n",
    "                weights_history.append(current_weights.copy())\n",
    "            \n",
    "            # Crear DataFrame de resultados\n",
    "            performance = pd.DataFrame({\n",
    "                'Portfolio_Value': portfolio_values[:-1],  # Ajustar longitud\n",
    "                'Returns': portfolio_returns\n",
    "            }, index=backtest_dates[:-1])  # Ajustar fechas\n",
    "            \n",
    "            # Calcular métricas\n",
    "            performance['Cumulative_Returns'] = (1 + performance['Returns']).cumprod()\n",
    "            performance['Drawdown'] = 1 - performance['Cumulative_Returns'] / performance['Cumulative_Returns'].cummax()\n",
    "            \n",
    "            # Guardar resultados adicionales\n",
    "            self.weights_history = pd.DataFrame(weights_history, index=backtest_dates)\n",
    "            self.regime_history = pd.Series(regime_history, index=backtest_dates[:len(regime_history)])\n",
    "            self.performance = performance\n",
    "            \n",
    "            return performance\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en backtest: {str(e)}\", exc_info=True)\n",
    "            raise\n",
    "    \n",
    "    def calculate_metrics(self, performance=None):\n",
    "        \"\"\"\n",
    "        Calcula métricas de rendimiento de la estrategia.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        performance : DataFrame, opcional\n",
    "            Resultados del backtest. Si es None, se usa self.performance.\n",
    "            \n",
    "        Retorna:\n",
    "        --------\n",
    "        metrics : dict\n",
    "            Diccionario con métricas de rendimiento\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if performance is None:\n",
    "                performance = self.performance\n",
    "            \n",
    "            if performance is None or len(performance) == 0:\n",
    "                raise ValueError(\"No hay datos de rendimiento disponibles\")\n",
    "            \n",
    "            # Calcular métricas anualizadas\n",
    "            returns = performance['Returns']\n",
    "            ann_factor = 252  # Factor de anualización para datos diarios\n",
    "            \n",
    "            total_return = performance['Cumulative_Returns'].iloc[-1] - 1\n",
    "            ann_return = (1 + total_return) ** (ann_factor / len(returns)) - 1\n",
    "            ann_volatility = returns.std() * np.sqrt(ann_factor)\n",
    "            sharpe_ratio = ann_return / ann_volatility if ann_volatility > 0 else 0\n",
    "            max_drawdown = performance['Drawdown'].max()\n",
    "            \n",
    "            # Calcular ratio de Sortino (solo considera volatilidad negativa)\n",
    "            negative_returns = returns[returns < 0]\n",
    "            downside_deviation = negative_returns.std() * np.sqrt(ann_factor) if len(negative_returns) > 0 else 0\n",
    "            sortino_ratio = ann_return / downside_deviation if downside_deviation > 0 else 0\n",
    "            \n",
    "            # Calcular ratio de Calmar (retorno anualizado / máximo drawdown)\n",
    "            calmar_ratio = ann_return / max_drawdown if max_drawdown > 0 else 0\n",
    "            \n",
    "            # Estimar turnover anualizado (rotación de cartera)\n",
    "            if hasattr(self, 'weights_history') and len(self.weights_history) > 1:\n",
    "                turnovers = []\n",
    "                for i in range(1, len(self.weights_history)):\n",
    "                    turnover = np.sum(np.abs(self.weights_history.iloc[i] - self.weights_history.iloc[i-1]))\n",
    "                    turnovers.append(turnover)\n",
    "                avg_turnover = np.mean(turnovers) if turnovers else 0\n",
    "                ann_turnover = avg_turnover * (252 / self.rebalance_freq)\n",
    "            else:\n",
    "                ann_turnover = 0\n",
    "            \n",
    "            # Calcular % de meses positivos\n",
    "            monthly_returns = returns.resample('M').apply(lambda x: (1 + x).prod() - 1)\n",
    "            pct_positive_months = (monthly_returns > 0).mean() if len(monthly_returns) > 0 else 0\n",
    "            \n",
    "            # Calcular métricas realistas\n",
    "            gross_return = ann_return\n",
    "            \n",
    "            # Estimar costos anuales\n",
    "            estimated_transaction_costs = ann_turnover * self.transaction_cost\n",
    "            \n",
    "            # Estimar costos de préstamo para posiciones cortas\n",
    "            if hasattr(self, 'weights_history'):\n",
    "                short_exposure = self.weights_history.apply(lambda x: np.sum(np.maximum(-x, 0)), axis=1).mean()\n",
    "                short_costs = short_exposure * self.borrow_cost\n",
    "            else:\n",
    "                short_costs = 0\n",
    "            \n",
    "            # Retorno neto\n",
    "            net_return = gross_return - estimated_transaction_costs - short_costs\n",
    "            net_sharpe = net_return / ann_volatility if ann_volatility > 0 else 0\n",
    "            \n",
    "            # Recopilar métricas\n",
    "            metrics = {\n",
    "                'Gross Total Return': total_return,\n",
    "                'Gross Annualized Return': gross_return,\n",
    "                'Net Annualized Return': net_return,\n",
    "                'Annualized Volatility': ann_volatility,\n",
    "                'Gross Sharpe Ratio': sharpe_ratio,\n",
    "                'Net Sharpe Ratio': net_sharpe,\n",
    "                'Sortino Ratio': sortino_ratio,\n",
    "                'Calmar Ratio': calmar_ratio,\n",
    "                'Maximum Drawdown': max_drawdown,\n",
    "                'Annualized Turnover': ann_turnover,\n",
    "                'Estimated Transaction Costs': estimated_transaction_costs,\n",
    "                'Estimated Short Costs': short_costs,\n",
    "                'Positive Months (%)': pct_positive_months,\n",
    "                'Number of Trades': len(self.weights_history) // self.rebalance_freq\n",
    "            }\n",
    "            \n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en calculate_metrics: {str(e)}\", exc_info=True)\n",
    "            return {}\n",
    "    \n",
    "    def plot_results(self, save_path='./artifacts/results/figures/'):\n",
    "        \"\"\"\n",
    "        Genera y guarda visualizaciones de los resultados.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        save_path : str\n",
    "            Ruta donde guardar las figuras\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.performance is None or len(self.performance) == 0:\n",
    "                raise ValueError(\"No hay datos de rendimiento disponibles\")\n",
    "            \n",
    "            # Crear directorio si no existe\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            \n",
    "            # 1. Gráfico de rendimiento acumulado\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            self.performance['Cumulative_Returns'].plot()\n",
    "            plt.title('Rendimiento Acumulado')\n",
    "            plt.xlabel('Fecha')\n",
    "            plt.ylabel('Retorno Acumulado')\n",
    "            plt.grid(True)\n",
    "            plt.savefig(f'{save_path}cumulative_returns.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # 2. Gráfico de drawdowns\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            self.performance['Drawdown'].plot(color='red')\n",
    "            plt.title('Drawdowns')\n",
    "            plt.xlabel('Fecha')\n",
    "            plt.ylabel('Drawdown')\n",
    "            plt.grid(True)\n",
    "            plt.savefig(f'{save_path}drawdowns.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # 3. Gráfico de regímenes de mercado\n",
    "            if hasattr(self, 'regime_history') and len(self.regime_history) > 0:\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                self.regime_history.plot(drawstyle='steps')\n",
    "                plt.title('Regímenes de Mercado Detectados')\n",
    "                plt.xlabel('Fecha')\n",
    "                plt.ylabel('Régimen')\n",
    "                plt.yticks(range(self.n_regimes))\n",
    "                plt.grid(True)\n",
    "                plt.savefig(f'{save_path}market_regimes.png', dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "                # 3b. Superponer regímenes con retornos acumulados\n",
    "                plt.figure(figsize=(14, 8))\n",
    "                ax1 = plt.gca()\n",
    "                self.performance['Cumulative_Returns'].plot(ax=ax1, color='blue')\n",
    "                ax1.set_xlabel('Fecha')\n",
    "                ax1.set_ylabel('Retorno Acumulado', color='blue')\n",
    "                ax1.tick_params(axis='y', labelcolor='blue')\n",
    "                \n",
    "                ax2 = ax1.twinx()\n",
    "                self.regime_history.plot(ax=ax2, color='red', drawstyle='steps', alpha=0.7)\n",
    "                ax2.set_ylabel('Régimen', color='red')\n",
    "                ax2.tick_params(axis='y', labelcolor='red')\n",
    "                ax2.set_yticks(range(self.n_regimes))\n",
    "                \n",
    "                plt.title('Rendimiento vs. Regímenes de Mercado')\n",
    "                plt.grid(True)\n",
    "                plt.savefig(f'{save_path}returns_vs_regimes.png', dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "            \n",
    "            # 4. Gráfico de exposición a activos a lo largo del tiempo\n",
    "            if hasattr(self, 'weights_history') and len(self.weights_history) > 0:\n",
    "                # Seleccionar los 10 activos con mayor peso promedio absoluto\n",
    "                top_assets = self.weights_history.abs().mean().nlargest(10).index\n",
    "                \n",
    "                plt.figure(figsize=(12, 8))\n",
    "                self.weights_history[top_assets].plot(colormap='viridis')\n",
    "                plt.title('Exposición a los 10 Activos Principales')\n",
    "                plt.xlabel('Fecha')\n",
    "                plt.ylabel('Peso en el Portafolio')\n",
    "                plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "                plt.grid(True)\n",
    "                plt.savefig(f'{save_path}asset_exposure.png', dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "                # 5. Heatmap de pesos a lo largo del tiempo\n",
    "                plt.figure(figsize=(14, 10))\n",
    "                sns.heatmap(\n",
    "                    self.weights_history[top_assets].T,\n",
    "                    cmap='RdBu_r',\n",
    "                    center=0,\n",
    "                    robust=True,\n",
    "                    cbar_kws={'label': 'Peso'}\n",
    "                )\n",
    "                plt.title('Evolución de Pesos del Portafolio (Top 10 Activos)')\n",
    "                plt.xlabel('Tiempo')\n",
    "                plt.ylabel('Activo')\n",
    "                plt.savefig(f'{save_path}weights_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "                # 6. Gráfico de apalancamiento a lo largo del tiempo\n",
    "                leverage = self.weights_history.abs().sum(axis=1)\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                leverage.plot()\n",
    "                plt.axhline(y=1.0, color='r', linestyle='--')\n",
    "                plt.title('Apalancamiento del Portafolio')\n",
    "                plt.xlabel('Fecha')\n",
    "                plt.ylabel('Apalancamiento')\n",
    "                plt.grid(True)\n",
    "                plt.savefig(f'{save_path}leverage.png', dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "                # 7. Gráfico de exposición neta y bruta\n",
    "                gross_exposure = self.weights_history.abs().sum(axis=1)\n",
    "                net_exposure = self.weights_history.sum(axis=1)\n",
    "                \n",
    "                plt.figure(figsize=(12, 6))\n",
    "                gross_exposure.plot(label='Exposición Bruta')\n",
    "                net_exposure.plot(label='Exposición Neta')\n",
    "                plt.axhline(y=1.0, color='r', linestyle='--')\n",
    "                plt.title('Exposición Neta y Bruta del Portafolio')\n",
    "                plt.xlabel('Fecha')\n",
    "                plt.ylabel('Exposición')\n",
    "                plt.legend()\n",
    "                plt.grid(True)\n",
    "                plt.savefig(f'{save_path}net_gross_exposure.png', dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "            \n",
    "            # 8. Distribución de retornos\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.histplot(self.performance['Returns'], kde=True)\n",
    "            plt.title('Distribución de Retornos Diarios')\n",
    "            plt.xlabel('Retorno')\n",
    "            plt.ylabel('Frecuencia')\n",
    "            plt.grid(True)\n",
    "            plt.savefig(f'{save_path}returns_distribution.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # 9. QQ-Plot de retornos vs distribución normal\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            from scipy import stats\n",
    "            stats.probplot(self.performance['Returns'].dropna(), dist=\"norm\", plot=plt)\n",
    "            plt.title('QQ-Plot de Retornos vs Distribución Normal')\n",
    "            plt.grid(True)\n",
    "            plt.savefig(f'{save_path}qqplot.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"Gráficos guardados en {save_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en plot_results: {str(e)}\", exc_info=True)\n",
    "    \n",
    "    def run_walk_forward_analysis(self, train_size=0.6, step_size=126, train_lookback=504):\n",
    "        \"\"\"\n",
    "        CORRECCIÓN: Ejecuta análisis walk-forward mejorado para evaluar la robustez de la estrategia.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        train_size : float\n",
    "            Proporción de datos a usar para entrenamiento en cada ventana\n",
    "        step_size : int\n",
    "            Tamaño del paso para avanzar la ventana de prueba (en días)\n",
    "        train_lookback : int\n",
    "            Cantidad de días máximos a utilizar en cada ventana de entrenamiento\n",
    "            \n",
    "        Retorna:\n",
    "        --------\n",
    "        wfa_results : DataFrame\n",
    "            Resultados del análisis walk-forward\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Asegurar que tenemos suficientes datos\n",
    "            if len(self.returns) < self.lookback_window + 2 * step_size:\n",
    "                raise ValueError(\"No hay suficientes datos para análisis walk-forward\")\n",
    "            \n",
    "            # Inicializar resultados\n",
    "            wfa_results = []\n",
    "            dates = self.returns.index\n",
    "            \n",
    "            # Definir ventanas\n",
    "            start_idx = self.lookback_window\n",
    "            while start_idx + step_size < len(dates):\n",
    "                # CORRECCIÓN: Limitar la ventana de entrenamiento para evitar datos muy antiguos\n",
    "                # Esto es más realista ya que modelos muy antiguos pueden perder relevancia\n",
    "                train_end_idx = start_idx + int((len(dates) - start_idx) * train_size)\n",
    "                train_start_idx = max(0, train_end_idx - train_lookback)\n",
    "                test_end_idx = min(train_end_idx + step_size, len(dates))\n",
    "                \n",
    "                train_start_date = dates[train_start_idx]\n",
    "                train_end_date = dates[train_end_idx - 1]\n",
    "                test_start_date = dates[train_end_idx]\n",
    "                test_end_date = dates[test_end_idx - 1]\n",
    "                \n",
    "                print(f\"\\nVentana WFA: {test_start_date.strftime('%Y-%m-%d')} a {test_end_date.strftime('%Y-%m-%d')}\")\n",
    "                \n",
    "                # Ejecutar backtest en datos de entrenamiento\n",
    "                train_performance = self.backtest(\n",
    "                    start_date=train_start_date,\n",
    "                    end_date=train_end_date\n",
    "                )\n",
    "                \n",
    "                # Guardar pesos óptimos del último rebalanceo\n",
    "                last_weights = self.weights_history.iloc[-1]\n",
    "                \n",
    "                # CORRECCIÓN: Considerar costos de transacción al aplicar pesos\n",
    "                # Calcular costos de ir desde cero a los pesos iniciales\n",
    "                initial_turnover = np.sum(np.abs(last_weights))\n",
    "                initial_cost = initial_turnover * self.transaction_cost\n",
    "                \n",
    "                # Inicializar tracking de posiciones cortas para costos de préstamo\n",
    "                short_positions = last_weights[last_weights < 0]\n",
    "                daily_borrow_cost = self.borrow_cost / 252\n",
    "                \n",
    "                # Ejecutar backtest en datos de prueba con pesos fijos\n",
    "                test_returns = self.returns.loc[test_start_date:test_end_date]\n",
    "                test_portfolio_values = [1.0 - initial_cost]  # Descontar costo inicial\n",
    "                \n",
    "                for date, returns in test_returns.iterrows():\n",
    "                    # Calcular costo de posiciones cortas para este día\n",
    "                    if not short_positions.empty:\n",
    "                        short_cost = (short_positions.abs() * daily_borrow_cost).sum()\n",
    "                    else:\n",
    "                        short_cost = 0\n",
    "                    \n",
    "                    # Calcular retorno del portafolio con costos\n",
    "                    portfolio_return = (last_weights * returns).sum() - short_cost\n",
    "                    test_portfolio_values.append(test_portfolio_values[-1] * (1 + portfolio_return))\n",
    "                \n",
    "                # Calcular métricas para esta ventana\n",
    "                test_returns_series = pd.Series(\n",
    "                    [test_portfolio_values[i+1]/test_portfolio_values[i] - 1 for i in range(len(test_portfolio_values)-1)],\n",
    "                    index=test_returns.index\n",
    "                )\n",
    "                \n",
    "                test_performance = pd.DataFrame({\n",
    "                    'Returns': test_returns_series,\n",
    "                    'Cumulative_Returns': (1 + test_returns_series).cumprod()\n",
    "                })\n",
    "                \n",
    "                test_performance['Drawdown'] = 1 - test_performance['Cumulative_Returns'] / test_performance['Cumulative_Returns'].cummax()\n",
    "                \n",
    "                # Calcular métricas\n",
    "                total_return = test_performance['Cumulative_Returns'].iloc[-1] - 1\n",
    "                ann_factor = 252\n",
    "                ann_return = (1 + total_return) ** (ann_factor / len(test_returns_series)) - 1\n",
    "                ann_volatility = test_returns_series.std() * np.sqrt(ann_factor)\n",
    "                sharpe_ratio = ann_return / ann_volatility if ann_volatility > 0 else 0\n",
    "                max_drawdown = test_performance['Drawdown'].max()\n",
    "                \n",
    "                # Guardar resultados\n",
    "                wfa_results.append({\n",
    "                    'Test_Start_Date': test_start_date,\n",
    "                    'Test_End_Date': test_end_date,\n",
    "                    'Total_Return': total_return,\n",
    "                    'Annualized_Return': ann_return,\n",
    "                    'Annualized_Volatility': ann_volatility,\n",
    "                    'Sharpe_Ratio': sharpe_ratio,\n",
    "                    'Max_Drawdown': max_drawdown,\n",
    "                    'Initial_Cost': initial_cost,\n",
    "                    'Short_Exposure': short_positions.abs().sum() if not short_positions.empty else 0\n",
    "                })\n",
    "                \n",
    "                # Avanzar ventana\n",
    "                start_idx = train_end_idx\n",
    "            \n",
    "            # Convertir resultados a DataFrame\n",
    "            wfa_df = pd.DataFrame(wfa_results)\n",
    "            \n",
    "            # Guardar resultados\n",
    "            wfa_df.to_csv('./artifacts/results/data/walk_forward_analysis.csv', index=False)\n",
    "            \n",
    "            # Calcular métricas agregadas\n",
    "            wfa_metrics = {\n",
    "                'Mean_Sharpe': wfa_df['Sharpe_Ratio'].mean(),\n",
    "                'Median_Sharpe': wfa_df['Sharpe_Ratio'].median(),\n",
    "                'Min_Sharpe': wfa_df['Sharpe_Ratio'].min(),\n",
    "                'Max_Sharpe': wfa_df['Sharpe_Ratio'].max(),\n",
    "                'Mean_Return': wfa_df['Annualized_Return'].mean(),\n",
    "                'Mean_Volatility': wfa_df['Annualized_Volatility'].mean(),\n",
    "                'Mean_Drawdown': wfa_df['Max_Drawdown'].mean(),\n",
    "                'Consistency': (wfa_df['Sharpe_Ratio'] > 0).mean(),\n",
    "                'Mean_Initial_Cost': wfa_df['Initial_Cost'].mean(),\n",
    "                'Mean_Short_Exposure': wfa_df['Short_Exposure'].mean()\n",
    "            }\n",
    "            \n",
    "            # Guardar métricas agregadas\n",
    "            pd.Series(wfa_metrics).to_csv('./artifacts/results/data/walk_forward_metrics.csv')\n",
    "            \n",
    "            # Visualizar resultados\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.subplot(2, 1, 1)\n",
    "            plt.bar(range(len(wfa_df)), wfa_df['Sharpe_Ratio'], color='skyblue')\n",
    "            plt.axhline(y=0, color='r', linestyle='-')\n",
    "            plt.title('Sharpe Ratio por Ventana de Prueba')\n",
    "            plt.xticks(range(len(wfa_df)), [d.strftime('%Y-%m') for d in wfa_df['Test_Start_Date']], rotation=45)\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.subplot(2, 1, 2)\n",
    "            plt.bar(range(len(wfa_df)), wfa_df['Total_Return'], color='lightgreen')\n",
    "            plt.axhline(y=0, color='r', linestyle='-')\n",
    "            plt.title('Retorno Total por Ventana de Prueba')\n",
    "            plt.xticks(range(len(wfa_df)), [d.strftime('%Y-%m') for d in wfa_df['Test_Start_Date']], rotation=45)\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig('./artifacts/results/figures/walk_forward_results.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            return wfa_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en run_walk_forward_analysis: {str(e)}\", exc_info=True)\n",
    "            return pd.DataFrame()\n",
    "\n",
    "# Ejecutar la estrategia\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Inicializar estrategia con parámetros realistas\n",
    "        strategy = AdaptiveMultifactorStrategy(\n",
    "            start_date='2015-01-01',\n",
    "            end_date='2025-01-01',\n",
    "            lookback_window=252,\n",
    "            regime_window=126,\n",
    "            n_regimes=3,\n",
    "            rebalance_freq=21,\n",
    "            vol_target=0.10,\n",
    "            max_leverage=1.5,\n",
    "            transaction_cost=0.0005,  # 5 puntos básicos\n",
    "            market_impact=0.1,        # Como factor de volatilidad diaria\n",
    "            borrow_cost=0.0002,       # 20 puntos básicos anualizados\n",
    "            execution_delay=1,        # 1 día de retraso\n",
    "            use_point_in_time=False    # Intentar evitar sesgo de supervivencia\n",
    "        )\n",
    "        \n",
    "        # Ejecutar backtest\n",
    "        performance = strategy.backtest()\n",
    "        \n",
    "        # Calcular métricas\n",
    "        metrics = strategy.calculate_metrics()\n",
    "        print(\"\\nMétricas de Rendimiento:\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "        \n",
    "        # Generar visualizaciones\n",
    "        strategy.plot_results()\n",
    "        \n",
    "        # Ejecutar análisis walk-forward\n",
    "        wfa_results = strategy.run_walk_forward_analysis(train_size=0.6, step_size=126, train_lookback=504)\n",
    "        \n",
    "        print(\"\\nAnálisis completado. Todos los resultados guardados en ./artifacts/results/\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en la ejecución principal: {str(e)}\", exc_info=True)\n",
    "        print(f\"Error: {str(e)}. Ver ./artifacts/errors.txt para más detalles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40459837-6f4f-4a0c-abdd-8a67ce36ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df361ee-d3bd-4472-802d-ec5df8a23376",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
