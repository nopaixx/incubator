{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be4bc02d-ade3-4b54-ac36-a81fdbc599ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "def plot_performance(strategy_returns, benchmark_returns=None, title='Strategy Performance'):\n",
    "    \"\"\"\n",
    "    Genera gráfico de rendimiento.\n",
    "    \n",
    "    Args:\n",
    "        strategy_returns (Series): Serie con retornos de la estrategia\n",
    "        benchmark_returns (Series, optional): Serie con retornos del benchmark\n",
    "        title (str): Título del gráfico\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Calcular retorno acumulado\n",
    "        strategy_cumulative = (1 + strategy_returns).cumprod() - 1\n",
    "        \n",
    "        # Graficar retorno acumulado de la estrategia\n",
    "        plt.plot(strategy_cumulative.index, strategy_cumulative.values, label='Strategy')\n",
    "        \n",
    "        # Si hay benchmark, graficar también\n",
    "        if benchmark_returns is not None:\n",
    "            # Alinear fechas\n",
    "            aligned_benchmark = benchmark_returns.reindex(strategy_returns.index)\n",
    "            \n",
    "            # Calcular retorno acumulado del benchmark\n",
    "            benchmark_cumulative = (1 + aligned_benchmark).cumprod() - 1\n",
    "            \n",
    "            # Graficar retorno acumulado del benchmark\n",
    "            plt.plot(benchmark_cumulative.index, benchmark_cumulative.values, label='Benchmark')\n",
    "        \n",
    "        # Añadir título y etiquetas\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Cumulative Return')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Guardar gráfico\n",
    "        plt.savefig(f'./artifacts/results/figures/{title.replace(\" \", \"_\")}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error generando gráfico de rendimiento: {str(e)}\")\n",
    "\n",
    "def plot_drawdown(returns, title='Drawdown Analysis'):\n",
    "    \"\"\"\n",
    "    Genera gráfico de drawdown.\n",
    "    \n",
    "    Args:\n",
    "        returns (Series): Serie con retornos\n",
    "        title (str): Título del gráfico\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calcular retorno acumulado\n",
    "        cumulative_return = (1 + returns).cumprod() - 1\n",
    "        \n",
    "        # Calcular drawdown\n",
    "        peak = cumulative_return.cummax()\n",
    "        drawdown = (cumulative_return - peak) / (1 + peak)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Graficar drawdown\n",
    "        plt.fill_between(drawdown.index, drawdown.values, 0, color='red', alpha=0.3)\n",
    "        plt.plot(drawdown.index, drawdown.values, color='red', alpha=0.5)\n",
    "        \n",
    "        # Añadir título y etiquetas\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Drawdown')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Guardar gráfico\n",
    "        plt.savefig(f'./artifacts/results/figures/{title.replace(\" \", \"_\")}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error generando gráfico de drawdown: {str(e)}\")\n",
    "\n",
    "def plot_regime_performance(returns, regimes, title='Performance by Regime'):\n",
    "    \"\"\"\n",
    "    Genera gráfico de rendimiento por régimen.\n",
    "    \n",
    "    Args:\n",
    "        returns (Series): Serie con retornos\n",
    "        regimes (Series): Serie con regímenes\n",
    "        title (str): Título del gráfico\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Alinear fechas\n",
    "        aligned_regimes = regimes.reindex(returns.index)\n",
    "        \n",
    "        # Crear DataFrame con retornos y regímenes\n",
    "        df = pd.DataFrame({\n",
    "            'returns': returns,\n",
    "            'regime': aligned_regimes\n",
    "        })\n",
    "        \n",
    "        # Calcular retorno promedio por régimen\n",
    "        regime_returns = df.groupby('regime')['returns'].mean() * 252  # Anualizado\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Graficar retorno por régimen\n",
    "        bars = plt.bar(regime_returns.index, regime_returns.values)\n",
    "        \n",
    "        # Colorear barras según régimen\n",
    "        colors = ['green', 'yellow', 'red']\n",
    "        for i, bar in enumerate(bars):\n",
    "            if i < len(colors):\n",
    "                bar.set_color(colors[i])\n",
    "        \n",
    "        # Añadir título y etiquetas\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Regime')\n",
    "        plt.ylabel('Annualized Return')\n",
    "        plt.xticks(regime_returns.index)\n",
    "        plt.grid(True, axis='y')\n",
    "        \n",
    "        # Guardar gráfico\n",
    "        plt.savefig(f'./artifacts/results/figures/{title.replace(\" \", \"_\")}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error generando gráfico de rendimiento por régimen: {str(e)}\")\n",
    "\n",
    "def plot_sector_exposure(portfolio_weights, sectors, date, title='Sector Exposure'):\n",
    "    \"\"\"\n",
    "    Genera gráfico de exposición por sector.\n",
    "    \n",
    "    Args:\n",
    "        portfolio_weights (dict): Diccionario con pesos para una fecha\n",
    "        sectors (dict): Diccionario con sectores para cada ticker\n",
    "        date (datetime): Fecha para la cual mostrar exposición\n",
    "        title (str): Título del gráfico\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calcular exposición por sector\n",
    "        sector_exposure = {}\n",
    "        \n",
    "        for ticker, weight in portfolio_weights.items():\n",
    "            sector = sectors.get(ticker, 'Unknown')\n",
    "            sector_exposure[sector] = sector_exposure.get(sector, 0) + weight\n",
    "        \n",
    "        # Ordenar sectores por exposición\n",
    "        sorted_sectors = sorted(sector_exposure.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Extraer sectores y exposiciones\n",
    "        sector_names = [s[0] for s in sorted_sectors]\n",
    "        exposures = [s[1] for s in sorted_sectors]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Graficar exposición por sector\n",
    "        bars = plt.barh(sector_names, exposures)\n",
    "        \n",
    "        # Añadir título y etiquetas\n",
    "        plt.title(f'{title} - {date.strftime(\"%Y-%m-%d\")}')\n",
    "        plt.xlabel('Exposure')\n",
    "        plt.ylabel('Sector')\n",
    "        plt.grid(True, axis='x')\n",
    "        \n",
    "        # Guardar gráfico\n",
    "        plt.savefig(f'./artifacts/results/figures/{title.replace(\" \", \"_\")}_{date.strftime(\"%Y%m%d\")}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error generando gráfico de exposición por sector: {str(e)}\")\n",
    "\n",
    "def plot_volatility_targeting(market_volatility, exposure_multipliers, title='Volatility Targeting'):\n",
    "    \"\"\"\n",
    "    Genera gráfico de volatility targeting.\n",
    "    \n",
    "    Args:\n",
    "        market_volatility (Series): Serie con volatilidad del mercado\n",
    "        exposure_multipliers (Series): Serie con multiplicadores de exposición\n",
    "        title (str): Título del gráfico\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Crear subplot para volatilidad\n",
    "        ax1 = plt.subplot(211)\n",
    "        ax1.plot(market_volatility.index, market_volatility.values, color='red')\n",
    "        ax1.set_ylabel('Market Volatility (Annualized)')\n",
    "        ax1.set_title('Market Volatility Over Time')\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Crear subplot para multiplicadores\n",
    "        ax2 = plt.subplot(212, sharex=ax1)\n",
    "        ax2.plot(exposure_multipliers.index, exposure_multipliers.values, color='blue')\n",
    "        ax2.set_ylabel('Exposure Multiplier')\n",
    "        ax2.set_title('Volatility Targeting Exposure')\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        # Ajustar diseño\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Guardar gráfico\n",
    "        plt.savefig(f'./artifacts/results/figures/{title.replace(\" \", \"_\")}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error generando gráfico de volatility targeting: {str(e)}\")\n",
    "\n",
    "        \n",
    "def make_plots(strategy_returns, benchmark_returns, market_regimes, market_volatility, exposure_multipliers):        \n",
    "        plot_performance(strategy_returns, benchmark_returns, title='Strategy vs Benchmark')\n",
    "        plot_drawdown(strategy_returns, title='Strategy Drawdown')\n",
    "        plot_regime_performance(strategy_returns, market_regimes, title='Performance by Regime')\n",
    "        \n",
    "        # NUEVO: Generar gráfico de volatility targeting\n",
    "        plot_volatility_targeting(market_volatility, exposure_multipliers, title='Volatility Targeting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91abe057-8d2b-4bde-97e4-08f0b52c2c58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f026d35c-711e-483f-8d83-fbfe022eed51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "900edfca-550b-48e9-8e7a-a419bc0449de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando estrategia de momentum multi-horizonte mejorada...\n",
      "\n",
      "=== Ejecutando Backtest === 2022-04-21 2025-04-20\n",
      "Iniciando backtest...\n",
      "Calculando características...\n",
      "Detectando regímenes de mercado...\n",
      "Generando señales...\n",
      "Ajustando señales por autocorrelación...\n",
      "Combinando señales...\n",
      "Construyendo portafolios...\n",
      "Calculando retornos...\n",
      "Calculando métricas de rendimiento...\n",
      "Pesos del último portafolio guardados (fecha: 2025-03-31)\n",
      "Backtest completado.\n",
      "\n",
      "Métricas de rendimiento del backtest:\n",
      "annualized_return: 0.0000\n",
      "annualized_volatility: 0.0000\n",
      "sharpe_ratio: 0.0000\n",
      "max_drawdown: 0.0000\n",
      "win_rate: 0.0000\n",
      "information_ratio: -0.7109\n",
      "sortino_ratio: 0.0000\n",
      "calmar_ratio: 0.0000\n",
      "avg_monthly_return: 0.0000\n",
      "worst_month: 0.0000\n",
      "\n",
      "Plots:\n",
      "\n",
      "Estrategia completada. Resultados guardados en ./artifacts/results/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# Crear directorios para resultados\n",
    "os.makedirs('./artifacts/results', exist_ok=True)\n",
    "os.makedirs('./artifacts/results/figures', exist_ok=True)\n",
    "os.makedirs('./artifacts/results/data', exist_ok=True)\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    filename='./artifacts/errors.txt',\n",
    "    level=logging.ERROR,\n",
    "    format='[%(asctime)s] %(levelname)s: %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# Ignorar advertencias\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_sp500_tickers():\n",
    "    \"\"\"\n",
    "    Obtiene la lista de tickers del S&P 500 desde Wikipedia.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Diccionario con tickers como claves y sectores como valores\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "        tables = pd.read_html(url)\n",
    "        df = tables[0]\n",
    "        \n",
    "        # Crear diccionario de ticker -> sector\n",
    "        ticker_sector_dict = dict(zip(df['Symbol'], df['GICS Sector']))\n",
    "        \n",
    "        # Limpiar tickers (algunos tienen puntos que yfinance no maneja bien)\n",
    "        ticker_sector_dict = {ticker.replace('.', '-'): sector \n",
    "                             for ticker, sector in ticker_sector_dict.items()}\n",
    "        \n",
    "        return ticker_sector_dict\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error obteniendo tickers del S&P 500: {str(e)}\")\n",
    "        # Devolver un diccionario vacío en caso de error\n",
    "        return {}\n",
    "\n",
    "def get_defensive_tickers():\n",
    "    \"\"\"\n",
    "    Crea una lista de tickers defensivos (ETFs de bonos, oro, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        list: Lista de tickers defensivos\n",
    "    \"\"\"\n",
    "    # ETFs de renta fija, oro, utilities y consumo defensivo\n",
    "    defensive_tickers = [\n",
    "        'TLT',  # iShares 20+ Year Treasury Bond ETF\n",
    "        'IEF',  # iShares 7-10 Year Treasury Bond ETF\n",
    "        'LQD',  # iShares iBoxx $ Investment Grade Corporate Bond ETF\n",
    "        'GLD',  # SPDR Gold Shares\n",
    "        'XLU',  # Utilities Select Sector SPDR Fund\n",
    "        'XLP',  # Consumer Staples Select Sector SPDR Fund\n",
    "        'USMV', # iShares MSCI USA Min Vol Factor ETF\n",
    "        'SPLV', # Invesco S&P 500 Low Volatility ETF\n",
    "        'VNQ',  # Vanguard Real Estate ETF\n",
    "    ]\n",
    "    \n",
    "    # Crear un diccionario con sectores \"defensivos\" para estos tickers\n",
    "    defensive_sectors = {ticker: 'Defensive' for ticker in defensive_tickers}\n",
    "    \n",
    "    return defensive_sectors\n",
    "\n",
    "def download_data(tickers, start_date, end_date, include_defensive=True):\n",
    "    \"\"\"\n",
    "    Descarga datos históricos para los tickers especificados.\n",
    "    \n",
    "    Args:\n",
    "        tickers (list): Lista de tickers a descargar\n",
    "        start_date (str): Fecha de inicio en formato 'YYYY-MM-DD'\n",
    "        end_date (str): Fecha de fin en formato 'YYYY-MM-DD'\n",
    "        include_defensive (bool): Si True, incluye activos defensivos\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (precios, volumen, sectores actualizados)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Obtener tickers defensivos si se solicita\n",
    "        defensive_sectors = {}\n",
    "        if include_defensive:\n",
    "            defensive_sectors = get_defensive_tickers()\n",
    "            tickers = list(tickers) + list(defensive_sectors.keys())\n",
    "        \n",
    "        # Añadir un margen de tiempo para calcular características que requieren datos históricos\n",
    "        extended_start = (pd.to_datetime(start_date) - pd.Timedelta(days=365)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Descargar datos\n",
    "        data = yf.download(tickers, start=extended_start, end=end_date, progress=False)\n",
    "        \n",
    "        # Extraer precios de cierre y volumen\n",
    "        prices = data['Close']\n",
    "        volume = data['Volume']\n",
    "        \n",
    "        # Verificar si hay datos\n",
    "        if prices.empty or volume.empty:\n",
    "            raise ValueError(\"No se pudieron obtener datos para los tickers especificados\")\n",
    "        \n",
    "        # Eliminar columnas con más del 30% de valores NaN\n",
    "        valid_columns = prices.columns[prices.isna().mean() < 0.3]\n",
    "        prices = prices[valid_columns]\n",
    "        volume = volume[valid_columns]\n",
    "        \n",
    "        # Llenar valores NaN con el último valor disponible\n",
    "        prices = prices.fillna(method='ffill')\n",
    "        volume = volume.fillna(method='ffill')\n",
    "        \n",
    "        # Filtrar para el período solicitado\n",
    "        prices = prices.loc[start_date:end_date]\n",
    "        volume = volume.loc[start_date:end_date]\n",
    "        \n",
    "        return prices, volume, defensive_sectors\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error descargando datos: {str(e)}\")\n",
    "        # Devolver DataFrames vacíos en caso de error\n",
    "        return pd.DataFrame(), pd.DataFrame(), {}\n",
    "\n",
    "def calculate_returns(prices, periods):\n",
    "    \"\"\"\n",
    "    Calcula los retornos para diferentes períodos de tiempo.\n",
    "    \n",
    "    Args:\n",
    "        prices (DataFrame): DataFrame con precios de cierre\n",
    "        periods (dict): Diccionario con nombres de períodos y número de días\n",
    "        \n",
    "    Returns:\n",
    "        dict: Diccionario con retornos para cada período\n",
    "    \"\"\"\n",
    "    try:\n",
    "        returns = {}\n",
    "        \n",
    "        for period_name, days in periods.items():\n",
    "            # Calcular retornos para el período especificado\n",
    "            period_returns = prices.pct_change(periods=days).shift(1)\n",
    "            returns[period_name] = period_returns\n",
    "        \n",
    "        return returns\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error calculando retornos: {str(e)}\")\n",
    "        # Devolver un diccionario vacío en caso de error\n",
    "        return {}\n",
    "\n",
    "def calculate_volatility_target_multiplier(market_volatility, target_vol=0.20, max_leverage=1.5, min_leverage=0.25):\n",
    "    \"\"\"\n",
    "    Calcula el multiplicador de exposición basado en la volatilidad del mercado.\n",
    "    \n",
    "    Args:\n",
    "        market_volatility (float): Volatilidad anualizada actual del mercado\n",
    "        target_vol (float): Volatilidad objetivo anualizada\n",
    "        max_leverage (float): Apalancamiento máximo permitido\n",
    "        min_leverage (float): Apalancamiento mínimo permitido\n",
    "        \n",
    "    Returns:\n",
    "        float: Multiplicador para el tamaño de posición\n",
    "    \"\"\"\n",
    "    # Evitar división por cero\n",
    "    if market_volatility <= 0:\n",
    "        return max_leverage\n",
    "    \n",
    "    # Calcular multiplicador basado en volatilidad objetivo\n",
    "    vol_multiplier = target_vol / market_volatility\n",
    "    \n",
    "    # Limitar el multiplicador dentro del rango permitido\n",
    "    vol_multiplier = max(min(vol_multiplier, max_leverage), min_leverage)\n",
    "    \n",
    "    return vol_multiplier\n",
    "\n",
    "def calculate_features(prices, volume, returns):\n",
    "    \"\"\"\n",
    "    Calcula características para cada ticker en cada fecha.\n",
    "    \n",
    "    Args:\n",
    "        prices (DataFrame): DataFrame con precios de cierre\n",
    "        volume (DataFrame): DataFrame con volumen\n",
    "        returns (dict): Diccionario con retornos para diferentes períodos\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame con características\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calcular retornos diarios para volatilidad\n",
    "        daily_returns = prices.pct_change()\n",
    "        \n",
    "        # Calcular volatilidad (ventana de 21 días)\n",
    "        volatility = daily_returns.rolling(window=21).std() * np.sqrt(252)\n",
    "        \n",
    "        # Calcular cambio de volumen (21 días)\n",
    "        volume_change = volume.pct_change(periods=21)\n",
    "        \n",
    "        # Calcular volumen promedio (21 días)\n",
    "        avg_volume = volume.rolling(window=21).mean()\n",
    "        \n",
    "        # Calcular autocorrelación (21 días)\n",
    "        autocorr = pd.DataFrame(index=prices.index, columns=prices.columns)\n",
    "        \n",
    "        # Necesitamos al menos 22 días de datos para calcular autocorrelación\n",
    "        min_required_days = 22\n",
    "        if len(daily_returns) >= min_required_days:\n",
    "            for ticker in prices.columns:\n",
    "                for i in range(min_required_days, len(daily_returns)):\n",
    "                    window = daily_returns.iloc[i-21:i][ticker].dropna()\n",
    "                    if len(window) > 5:  # Necesitamos al menos algunos puntos para la autocorrelación\n",
    "                        try:\n",
    "                            autocorr.iloc[i][ticker] = window.autocorr(lag=1)\n",
    "                        except:\n",
    "                            autocorr.iloc[i][ticker] = 0\n",
    "        \n",
    "        # Calcular beta (ventana de 63 días) - NUEVO\n",
    "        spy_returns = daily_returns.mean(axis=1)  # Proxy para el mercado\n",
    "        beta = pd.DataFrame(index=prices.index, columns=prices.columns)\n",
    "        \n",
    "        for ticker in prices.columns:\n",
    "            # Calcular beta usando regresión con ventana móvil\n",
    "            for i in range(63, len(daily_returns)):\n",
    "                x = spy_returns.iloc[i-63:i].values.reshape(-1, 1)\n",
    "                y = daily_returns.iloc[i-63:i][ticker].values\n",
    "                \n",
    "                # Eliminar valores NaN\n",
    "                mask = ~np.isnan(y)\n",
    "                if sum(mask) > 30:  # Al menos 30 puntos válidos\n",
    "                    try:\n",
    "                        model = LinearRegression().fit(x[mask], y[mask])\n",
    "                        beta.iloc[i][ticker] = model.coef_[0]\n",
    "                    except:\n",
    "                        beta.iloc[i][ticker] = 1.0  # Valor por defecto\n",
    "        \n",
    "        # Calcular indicador RSI (14 días) - NUEVO\n",
    "        rsi = pd.DataFrame(index=prices.index, columns=prices.columns)\n",
    "        \n",
    "        for ticker in prices.columns:\n",
    "            # Calcular cambios diarios\n",
    "            delta = daily_returns[ticker]\n",
    "            \n",
    "            # Separar ganancias y pérdidas\n",
    "            gains = delta.copy()\n",
    "            losses = delta.copy()\n",
    "            gains[gains < 0] = 0\n",
    "            losses[losses > 0] = 0\n",
    "            losses = abs(losses)\n",
    "            \n",
    "            # Calcular promedio de ganancias y pérdidas\n",
    "            avg_gain = gains.rolling(window=14).mean()\n",
    "            avg_loss = losses.rolling(window=14).mean()\n",
    "            \n",
    "            # Calcular RS y RSI\n",
    "            rs = avg_gain / avg_loss\n",
    "            rsi[ticker] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        # Asegurarse de que tenemos suficientes datos\n",
    "        valid_dates = prices.index[63:]  # Cambiar de 21 a 63 debido al cálculo de beta\n",
    "        \n",
    "        # Crear lista para almacenar características\n",
    "        features_list = []\n",
    "        \n",
    "        # Para cada fecha después de tener suficientes datos\n",
    "        for date in valid_dates:\n",
    "            # Para cada ticker\n",
    "            for ticker in prices.columns:\n",
    "                # Verificar si tenemos datos para este ticker en esta fecha\n",
    "                if pd.isna(prices.loc[date, ticker]):\n",
    "                    continue\n",
    "                \n",
    "                # Extraer características para este ticker en esta fecha\n",
    "                features_dict = {\n",
    "                    'date': date,\n",
    "                    'ticker': ticker,\n",
    "                    'momentum_1m': returns['1M'].loc[date, ticker] if not pd.isna(returns['1M'].loc[date, ticker]) else 0,\n",
    "                    'momentum_3m': returns['3M'].loc[date, ticker] if not pd.isna(returns['3M'].loc[date, ticker]) else 0,\n",
    "                    'momentum_6m': returns['6M'].loc[date, ticker] if not pd.isna(returns['6M'].loc[date, ticker]) else 0,\n",
    "                    'momentum_12m': returns['12M'].loc[date, ticker] if not pd.isna(returns['12M'].loc[date, ticker]) else 0,\n",
    "                    'volatility': volatility.loc[date, ticker] if not pd.isna(volatility.loc[date, ticker]) else np.nan,\n",
    "                    'avg_volume': avg_volume.loc[date, ticker] if not pd.isna(avg_volume.loc[date, ticker]) else np.nan,\n",
    "                    'volume_change': volume_change.loc[date, ticker] if not pd.isna(volume_change.loc[date, ticker]) else 0,\n",
    "                    'autocorr': autocorr.loc[date, ticker] if not pd.isna(autocorr.loc[date, ticker]) else 0,\n",
    "                    'beta': beta.loc[date, ticker] if not pd.isna(beta.loc[date, ticker]) else 1.0,\n",
    "                    'rsi': rsi.loc[date, ticker] if not pd.isna(rsi.loc[date, ticker]) else 50\n",
    "                }\n",
    "                \n",
    "                features_list.append(features_dict)\n",
    "        \n",
    "        # Crear DataFrame con características\n",
    "        features_df = pd.DataFrame(features_list)\n",
    "        \n",
    "        # Manejar valores NaN\n",
    "        features_df = features_df.fillna(0)\n",
    "        \n",
    "        return features_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error calculando características: {str(e)}\")\n",
    "        # Devolver un DataFrame vacío en caso de error\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def detect_market_regimes(prices, n_regimes=3):\n",
    "    \"\"\"\n",
    "    Detecta regímenes de mercado utilizando clustering.\n",
    "    \n",
    "    Args:\n",
    "        prices (DataFrame): DataFrame con precios de cierre\n",
    "        n_regimes (int): Número de regímenes a detectar\n",
    "        \n",
    "    Returns:\n",
    "        Series: Serie con regímenes para cada fecha\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calcular retornos del mercado (promedio de todos los activos)\n",
    "        market_returns = prices.pct_change().mean(axis=1).dropna()\n",
    "        \n",
    "        # Verificar si hay suficientes datos\n",
    "        if len(market_returns) < 42:  # Necesitamos al menos 42 días para calcular volatilidad\n",
    "            logging.warning(\"Datos insuficientes para detectar regímenes. Usando régimen por defecto.\")\n",
    "            return pd.Series(0, index=prices.index)\n",
    "        \n",
    "        # Calcular volatilidad rodante (21 días)\n",
    "        rolling_vol = market_returns.rolling(window=21).std().dropna()\n",
    "        \n",
    "        # Calcular volatilidad rodante de alta frecuencia (5 días) - NUEVO\n",
    "        short_vol = market_returns.rolling(window=5).std() * np.sqrt(252)\n",
    "        \n",
    "        # Calcular ratio de volatilidad (volatilidad reciente / volatilidad de largo plazo) - NUEVO\n",
    "        vol_ratio = short_vol / rolling_vol.rolling(window=252).mean()\n",
    "        vol_ratio = vol_ratio[rolling_vol.index]\n",
    "        \n",
    "        # Crear características para el modelo\n",
    "        features = pd.DataFrame({\n",
    "            'returns': market_returns[rolling_vol.index],\n",
    "            'volatility': rolling_vol,\n",
    "            'vol_ratio': vol_ratio[rolling_vol.index].fillna(1)  # NUEVO\n",
    "        })\n",
    "        \n",
    "        # Normalizar características\n",
    "        scaler = StandardScaler()\n",
    "        features_scaled = scaler.fit_transform(features)\n",
    "        \n",
    "        # Aplicar K-means clustering\n",
    "        kmeans = KMeans(n_clusters=n_regimes, random_state=42)\n",
    "        regimes = kmeans.fit_predict(features_scaled)\n",
    "        \n",
    "        # Crear Serie con regímenes\n",
    "        regime_series = pd.Series(regimes, index=features.index)\n",
    "        \n",
    "        # Calcular y ordenar los centroides para asignar regímenes de manera consistente\n",
    "        # Régimen 0: baja volatilidad, Régimen 1: media volatilidad, Régimen 2: alta volatilidad\n",
    "        centroids = kmeans.cluster_centers_\n",
    "        vol_levels = centroids[:, 1]  # Índice 1 corresponde a la volatilidad\n",
    "        sorted_indices = np.argsort(vol_levels)\n",
    "        \n",
    "        # Crear un mapeo de etiquetas originales a etiquetas ordenadas\n",
    "        label_map = {sorted_indices[i]: i for i in range(n_regimes)}\n",
    "        \n",
    "        # Aplicar el mapeo a las etiquetas de régimen\n",
    "        mapped_regimes = np.array([label_map[r] for r in regimes])\n",
    "        regime_series = pd.Series(mapped_regimes, index=features.index)\n",
    "        \n",
    "        # Propagar regímenes a todas las fechas\n",
    "        full_regime_series = pd.Series(index=prices.index)\n",
    "        \n",
    "        # Para cada fecha en el índice de precios\n",
    "        for date in prices.index:\n",
    "            # Si la fecha está en el índice de regímenes, usar ese régimen\n",
    "            if date in regime_series.index:\n",
    "                full_regime_series[date] = regime_series[date]\n",
    "            # Si no, usar el último régimen disponible\n",
    "            elif date > regime_series.index[0]:\n",
    "                # Encontrar la fecha más reciente en el índice de regímenes\n",
    "                last_date = regime_series.index[regime_series.index < date][-1]\n",
    "                full_regime_series[date] = regime_series[last_date]\n",
    "            # Si la fecha es anterior al primer régimen, usar el primer régimen\n",
    "            else:\n",
    "                full_regime_series[date] = regime_series.iloc[0]\n",
    "        \n",
    "        return full_regime_series\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error detectando regímenes de mercado: {str(e)}\")\n",
    "        # Devolver una serie con régimen por defecto en caso de error\n",
    "        return pd.Series(0, index=prices.index)\n",
    "\n",
    "def generate_signals(features, market_regimes):\n",
    "    \"\"\"\n",
    "    Genera señales de trading basadas en características y regímenes de mercado.\n",
    "    \n",
    "    Args:\n",
    "        features (DataFrame): DataFrame con características\n",
    "        market_regimes (Series): Serie con regímenes de mercado\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame con señales para cada ticker en cada fecha\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Crear DataFrame para almacenar señales\n",
    "        signals = pd.DataFrame()\n",
    "        \n",
    "        # Obtener fechas únicas\n",
    "        dates = features['date'].unique()\n",
    "        \n",
    "        # Para cada fecha\n",
    "        for date in dates:\n",
    "            # Obtener características para esta fecha\n",
    "            date_features = features[features['date'] == date]\n",
    "            \n",
    "            # Obtener régimen de mercado para esta fecha\n",
    "            regime = market_regimes.get(date, 0)  # Usar régimen 0 por defecto si no hay dato\n",
    "            \n",
    "            # Calcular señales según el régimen\n",
    "            if regime == 0:  # Régimen de baja volatilidad\n",
    "                # En régimen de baja volatilidad, dar más peso a momentum de largo plazo\n",
    "                momentum_signal = (\n",
    "                    0.1 * date_features['momentum_1m'] +\n",
    "                    0.2 * date_features['momentum_3m'] +\n",
    "                    0.3 * date_features['momentum_6m'] +\n",
    "                    0.4 * date_features['momentum_12m']\n",
    "                )\n",
    "            elif regime == 1:  # Régimen de volatilidad media\n",
    "                # En régimen de volatilidad media, equilibrar pesos\n",
    "                momentum_signal = (\n",
    "                    0.25 * date_features['momentum_1m'] +\n",
    "                    0.25 * date_features['momentum_3m'] +\n",
    "                    0.25 * date_features['momentum_6m'] +\n",
    "                    0.25 * date_features['momentum_12m']\n",
    "                )\n",
    "            else:  # Régimen de alta volatilidad\n",
    "                # En régimen de alta volatilidad, dar más peso a momentum de corto plazo y menor peso total a momentum\n",
    "                # MODIFICADO: Incorporar señales de mean-reversion en régimen de alta volatilidad\n",
    "                momentum_component = (\n",
    "                    0.4 * date_features['momentum_1m'] +\n",
    "                    0.3 * date_features['momentum_3m'] +\n",
    "                    0.2 * date_features['momentum_6m'] +\n",
    "                    0.1 * date_features['momentum_12m']\n",
    "                )\n",
    "                \n",
    "                # Añadir componente de mean-reversion basado en RSI (sobrevendido = señal positiva)\n",
    "                rsi_signal = (50 - date_features['rsi']) / 50  # Valores negativos para RSI > 50, positivos para RSI < 50\n",
    "                \n",
    "                # Combinar momentum y mean-reversion con más peso a mean-reversion\n",
    "                momentum_signal = 0.5 * momentum_component + 0.5 * rsi_signal\n",
    "            \n",
    "            # Ajustar señales por volatilidad (penalizar alta volatilidad) - MODIFICADO: exponente más agresivo\n",
    "            vol_adjustment = 1 / (1 + date_features['volatility'] ** 1.5)\n",
    "            \n",
    "            # Ajustar señales por volumen (favorecer alto volumen y cambios positivos)\n",
    "            volume_adjustment = (\n",
    "                date_features['avg_volume'] / date_features['avg_volume'].mean() *\n",
    "                (1 + date_features['volume_change'])\n",
    "            )\n",
    "            volume_adjustment = volume_adjustment / volume_adjustment.mean()  # Normalizar\n",
    "            \n",
    "            # NUEVO: Ajustar por beta (penalizar beta alta en regímenes de alta volatilidad)\n",
    "            if regime == 2:  # Alta volatilidad\n",
    "                beta_adjustment = 1 / (1 + date_features['beta'])\n",
    "            elif regime == 1:  # Media volatilidad\n",
    "                beta_adjustment = 1 / (1 + 0.5 * date_features['beta'])\n",
    "            else:  # Baja volatilidad\n",
    "                beta_adjustment = 1  # Sin ajuste en baja volatilidad\n",
    "            \n",
    "            # Combinar señales\n",
    "            combined_signal = momentum_signal * vol_adjustment * volume_adjustment * beta_adjustment\n",
    "            \n",
    "            # Crear DataFrame con señales para esta fecha\n",
    "            date_signals = pd.DataFrame({\n",
    "                'date': date,\n",
    "                'ticker': date_features['ticker'],\n",
    "                'signal': combined_signal,\n",
    "                'autocorr': date_features['autocorr'],\n",
    "                'regime': regime,\n",
    "                'volatility': date_features['volatility'],\n",
    "                'beta': date_features['beta'],  # NUEVO\n",
    "                'rsi': date_features['rsi']     # NUEVO\n",
    "            })\n",
    "            \n",
    "            # Añadir a DataFrame de señales\n",
    "            signals = pd.concat([signals, date_signals], ignore_index=True)\n",
    "        \n",
    "        return signals\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error generando señales: {str(e)}\")\n",
    "        # Devolver un DataFrame vacío en caso de error\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def adjust_for_autocorrelation(signals):\n",
    "    \"\"\"\n",
    "    Ajusta las señales basándose en la autocorrelación de los retornos.\n",
    "    \n",
    "    Args:\n",
    "        signals (DataFrame): DataFrame con señales\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame con señales ajustadas\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Crear copia para no modificar el original\n",
    "        adjusted_signals = signals.copy()\n",
    "        \n",
    "        # Para cada fecha\n",
    "        for date in adjusted_signals['date'].unique():\n",
    "            # Obtener señales para esta fecha\n",
    "            date_signals = adjusted_signals[adjusted_signals['date'] == date]\n",
    "            \n",
    "            # Para cada ticker\n",
    "            for idx, row in date_signals.iterrows():\n",
    "                # Obtener autocorrelación\n",
    "                autocorr = row['autocorr']\n",
    "                \n",
    "                # Ajustar señal según autocorrelación\n",
    "                if not pd.isna(autocorr):\n",
    "                    # Si autocorrelación es positiva, reducir la señal\n",
    "                    if autocorr > 0:\n",
    "                        adjustment_factor = 1 / (1 + 2 * autocorr)\n",
    "                    # Si autocorrelación es negativa, aumentar la señal\n",
    "                    else:\n",
    "                        adjustment_factor = 1 - 2 * autocorr\n",
    "                    \n",
    "                    # Aplicar ajuste\n",
    "                    adjusted_signals.loc[idx, 'signal'] *= adjustment_factor\n",
    "        \n",
    "        return adjusted_signals\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error ajustando por autocorrelación: {str(e)}\")\n",
    "        # Devolver señales sin ajustar en caso de error\n",
    "        return signals\n",
    "\n",
    "def combine_signals(signals, lookback_window=63):\n",
    "    \"\"\"\n",
    "    Combina señales utilizando pesos adaptativos basados en rendimiento reciente.\n",
    "    Versión optimizada para mejorar rendimiento.\n",
    "    \n",
    "    Args:\n",
    "        signals (DataFrame): DataFrame con señales\n",
    "        lookback_window (int): Ventana para evaluar rendimiento de señales\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame con señales combinadas\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Si el DataFrame está vacío, devolverlo sin cambios\n",
    "        if signals.empty:\n",
    "            return signals\n",
    "        \n",
    "        # Crear copia para no modificar el original\n",
    "        combined_signals = signals.copy()\n",
    "        \n",
    "        # Obtener fechas únicas ordenadas\n",
    "        dates = np.sort(combined_signals['date'].unique())\n",
    "        \n",
    "        # Si no hay suficientes fechas, devolver señales sin combinar\n",
    "        if len(dates) <= lookback_window:\n",
    "            return combined_signals\n",
    "        \n",
    "        # Precomputar un diccionario de mapeo de régimen a factor de exposición\n",
    "        regime_exposure_map = {0: 1.0, 1: 0.8, 2: 0.4}\n",
    "        \n",
    "        # Crear un índice para acceso rápido\n",
    "        combined_signals.set_index(['date', 'ticker'], inplace=True, drop=False)\n",
    "        \n",
    "        # Preprocesar todas las señales para evitar filtrados repetidos\n",
    "        date_dict = {}\n",
    "        for date in dates:\n",
    "            try:\n",
    "                date_dict[date] = combined_signals.xs(date, level='date')\n",
    "            except:\n",
    "                date_dict[date] = pd.DataFrame()  # Fecha sin datos\n",
    "        \n",
    "        # Para cada fecha después de la ventana de lookback\n",
    "        for i in range(lookback_window, len(dates)):\n",
    "            current_date = dates[i]\n",
    "            \n",
    "            # Obtener señales para la fecha actual (ya preprocesado)\n",
    "            current_signals = date_dict[current_date]\n",
    "            \n",
    "            # Si no hay señales para esta fecha, continuar\n",
    "            if current_signals.empty:\n",
    "                continue\n",
    "            \n",
    "            # Obtener régimen actual (una sola vez)\n",
    "            current_regime = current_signals['regime'].iloc[0]\n",
    "            \n",
    "            # Obtener factor global según régimen (una sola vez)\n",
    "            regime_exposure_factor = regime_exposure_map.get(current_regime, 1.0)\n",
    "            \n",
    "            # Crear un array para todas las fechas de lookback\n",
    "            lookback_dates = dates[i-lookback_window:i]\n",
    "            \n",
    "            # Para cada ticker, calcular de forma vectorizada\n",
    "            processed_tickers = set()\n",
    "            \n",
    "            # Procesar en batch por ticker\n",
    "            for ticker in current_signals['ticker'].unique():\n",
    "                processed_tickers.add(ticker)\n",
    "                \n",
    "                # Recopilar historial para este ticker (de forma eficiente)\n",
    "                ticker_history_list = []\n",
    "                for ld in lookback_dates:\n",
    "                    if ld in date_dict and not date_dict[ld].empty:\n",
    "                        try:\n",
    "                            ticker_data = date_dict[ld].xs(ticker, level='ticker', drop=False)\n",
    "                            if not ticker_data.empty:\n",
    "                                ticker_history_list.append(ticker_data)\n",
    "                        except:\n",
    "                            continue\n",
    "                \n",
    "                if not ticker_history_list:\n",
    "                    continue\n",
    "                    \n",
    "                ticker_history = pd.concat(ticker_history_list)\n",
    "                \n",
    "                # Verificar suficientes datos\n",
    "                if len(ticker_history) < lookback_window / 2:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Calcular correlación de forma más eficiente\n",
    "                    signal_regime_corr = np.corrcoef(\n",
    "                        ticker_history['signal'].values,\n",
    "                        ticker_history['regime'].values\n",
    "                    )[0, 1]\n",
    "                    \n",
    "                    # Verificar validez\n",
    "                    if np.isnan(signal_regime_corr):\n",
    "                        continue\n",
    "                    \n",
    "                    # Obtener régimen actual para este ticker\n",
    "                    ticker_regime = current_signals.loc[\n",
    "                        current_signals['ticker'] == ticker, 'regime'\n",
    "                    ].iloc[0]\n",
    "                    \n",
    "                    # Calcular factor según correlación\n",
    "                    if signal_regime_corr > 0:\n",
    "                        regime_factor = 1 + 0.2 * ticker_regime\n",
    "                    else:\n",
    "                        regime_factor = 1 - 0.2 * ticker_regime\n",
    "                    \n",
    "                    # Calcular factor combinado (una sola multiplicación)\n",
    "                    combined_factor = regime_factor * regime_exposure_factor\n",
    "                    \n",
    "                    # Aplicar factor de forma más eficiente (una sola operación)\n",
    "                    idx = (current_date, ticker)\n",
    "                    if idx in combined_signals.index:\n",
    "                        combined_signals.at[idx, 'signal'] *= combined_factor\n",
    "                except Exception as inner_e:\n",
    "                    # Manejar errores silenciosamente para continuar el procesamiento\n",
    "                    continue\n",
    "        \n",
    "        # Restaurar el DataFrame a su forma original\n",
    "        combined_signals.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        return combined_signals\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error combinando señales: {str(e)}\")\n",
    "        # Devolver señales sin combinar en caso de error\n",
    "        return signals\n",
    "\n",
    "def construct_portfolio(signals, sectors, date, top_pct=0.1, max_sector_exposure=0.25, max_position_size=0.05):\n",
    "    \"\"\"\n",
    "    Construye un portafolio basado en señales para una fecha específica.\n",
    "    \n",
    "    Args:\n",
    "        signals (DataFrame): DataFrame con señales\n",
    "        sectors (dict): Diccionario con sectores para cada ticker\n",
    "        date (datetime): Fecha para la cual construir el portafolio\n",
    "        top_pct (float): Porcentaje de tickers con mejores señales a incluir\n",
    "        max_sector_exposure (float): Exposición máxima por sector\n",
    "        max_position_size (float): Tamaño máximo por posición\n",
    "        \n",
    "    Returns:\n",
    "        dict: Diccionario con pesos para cada ticker\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Obtener señales para la fecha especificada\n",
    "        date_signals = signals[signals['date'] == date].copy()\n",
    "        \n",
    "        # Si no hay señales para esta fecha, devolver diccionario vacío\n",
    "        if date_signals.empty:\n",
    "            return {}\n",
    "        \n",
    "        # Añadir sector a cada ticker\n",
    "        date_signals['sector'] = date_signals['ticker'].map(lambda x: sectors.get(x, 'Unknown'))\n",
    "        \n",
    "        # Obtener el régimen actual\n",
    "        current_regime = date_signals['regime'].iloc[0]\n",
    "        \n",
    "        # NUEVO: Separar tickers defensivos y no defensivos\n",
    "        defensive_tickers = date_signals[date_signals['sector'] == 'Defensive']\n",
    "        non_defensive_tickers = date_signals[date_signals['sector'] != 'Defensive']\n",
    "        \n",
    "        # NUEVO: Ajustar porcentaje de exposición a activos defensivos según régimen\n",
    "        defensive_allocation = 0.0  # Por defecto, sin activos defensivos\n",
    "        \n",
    "        if current_regime == 2:  # Alta volatilidad\n",
    "            defensive_allocation = 0.40  # 40% a defensivos en alta volatilidad\n",
    "        elif current_regime == 1:  # Media volatilidad\n",
    "            defensive_allocation = 0.20  # 20% a defensivos en media volatilidad\n",
    "        elif current_regime == 0:  # Baja volatilidad\n",
    "            defensive_allocation = 0.05  # 5% a defensivos en baja volatilidad\n",
    "        \n",
    "        # Asignar pesos iniciales\n",
    "        weights = {}\n",
    "        \n",
    "        # Procesar activos no defensivos\n",
    "        non_defensive_tickers = non_defensive_tickers.sort_values('signal', ascending=False)\n",
    "        n_non_defensive = int(len(non_defensive_tickers) * top_pct)\n",
    "        top_non_defensive = non_defensive_tickers.head(n_non_defensive)\n",
    "        \n",
    "        # Calcular exposición por sector para activos no defensivos\n",
    "        sector_exposure = top_non_defensive.groupby('sector').size() / n_non_defensive\n",
    "        \n",
    "        # NUEVO: Incorporar risk parity para activos no defensivos\n",
    "        # Inversamente proporcional a la volatilidad\n",
    "        top_non_defensive['risk_weight'] = 1 / top_non_defensive['volatility'].replace(0, 0.01)\n",
    "        top_non_defensive['risk_weight'] = top_non_defensive['risk_weight'] / top_non_defensive['risk_weight'].sum()\n",
    "        \n",
    "        # Combinar señal y riesgo (70% señal, 30% risk parity)\n",
    "        top_non_defensive['combined_weight'] = 0.7 * top_non_defensive['signal'] + 0.3 * top_non_defensive['risk_weight']\n",
    "        \n",
    "        # Para cada ticker no defensivo\n",
    "        for _, row in top_non_defensive.iterrows():\n",
    "            ticker = row['ticker']\n",
    "            sector = row['sector']\n",
    "            \n",
    "            # Si la exposición del sector excede el máximo, reducir peso\n",
    "            if sector in sector_exposure and sector_exposure[sector] > max_sector_exposure:\n",
    "                weight = row['combined_weight'] * (max_sector_exposure / sector_exposure[sector])\n",
    "            else:\n",
    "                weight = row['combined_weight']\n",
    "            \n",
    "            weights[ticker] = weight * (1 - defensive_allocation)\n",
    "        \n",
    "        # Procesar activos defensivos\n",
    "        if not defensive_tickers.empty and defensive_allocation > 0:\n",
    "            defensive_tickers = defensive_tickers.sort_values('signal', ascending=False)\n",
    "            \n",
    "            # Tomar los mejores activos defensivos\n",
    "            n_defensive = min(5, len(defensive_tickers))\n",
    "            top_defensive = defensive_tickers.head(n_defensive)\n",
    "            \n",
    "            # Asignar pesos equitativos a los activos defensivos\n",
    "            defensive_weight_per_asset = defensive_allocation / n_defensive\n",
    "            \n",
    "            for ticker in top_defensive['ticker']:\n",
    "                weights[ticker] = defensive_weight_per_asset\n",
    "        \n",
    "        # NUEVO: Limitar tamaño máximo por posición\n",
    "        for ticker in list(weights.keys()):\n",
    "            if weights[ticker] > max_position_size:\n",
    "                excess = weights[ticker] - max_position_size\n",
    "                weights[ticker] = max_position_size\n",
    "                \n",
    "                # Redistribuir exceso proporcionalmente entre otros activos\n",
    "                total_other_weights = sum(w for t, w in weights.items() if t != ticker)\n",
    "                if total_other_weights > 0:\n",
    "                    for t in weights:\n",
    "                        if t != ticker:\n",
    "                            weights[t] += excess * (weights[t] / total_other_weights)\n",
    "        \n",
    "        # Normalizar pesos para que sumen 1\n",
    "        total_weight = sum(weights.values())\n",
    "        \n",
    "        if total_weight > 0:\n",
    "            weights = {ticker: weight / total_weight for ticker, weight in weights.items()}\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error construyendo portafolio para {date}: {str(e)}\")\n",
    "        # Devolver diccionario vacío en caso de error\n",
    "        return {}\n",
    "\n",
    "def implement_stop_loss(prices, portfolio_weights, current_date, max_drawdown=-0.10):\n",
    "    \"\"\"\n",
    "    Implementa stop loss para reducir drawdowns.\n",
    "    \n",
    "    Args:\n",
    "        prices (DataFrame): DataFrame con precios de cierre\n",
    "        portfolio_weights (dict): Diccionario con pesos para cada fecha\n",
    "        current_date (datetime): Fecha actual\n",
    "        max_drawdown (float): Drawdown máximo permitido\n",
    "        \n",
    "    Returns:\n",
    "        dict: Diccionario con pesos ajustados\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Si no hay pesos para la fecha actual, devolver diccionario vacío\n",
    "        if current_date not in portfolio_weights:\n",
    "            return {}\n",
    "        \n",
    "        # Obtener pesos actuales\n",
    "        current_weights = portfolio_weights[current_date].copy()\n",
    "        \n",
    "        # Si no hay pesos, devolver diccionario vacío\n",
    "        if not current_weights:\n",
    "            return {}\n",
    "        \n",
    "        # Obtener fechas anteriores (último mes)\n",
    "        one_month_ago = current_date - pd.Timedelta(days=30)\n",
    "        past_dates = prices.index[(prices.index >= one_month_ago) & (prices.index < current_date)]\n",
    "        \n",
    "        if len(past_dates) < 5:  # Necesitamos al menos algunos días de historia\n",
    "            return current_weights\n",
    "        \n",
    "        # Calcular drawdowns para cada activo\n",
    "        for ticker in list(current_weights.keys()):\n",
    "            if ticker in prices.columns:\n",
    "                # Obtener precios para este ticker\n",
    "                ticker_prices = prices.loc[past_dates, ticker]\n",
    "                \n",
    "                if len(ticker_prices) < 5:\n",
    "                    continue\n",
    "                \n",
    "                # Calcular drawdown\n",
    "                peak = ticker_prices.max()\n",
    "                current_price = prices.loc[current_date, ticker]\n",
    "                drawdown = (current_price / peak) - 1\n",
    "                \n",
    "                # Si drawdown excede el máximo, eliminar posición\n",
    "                if drawdown < max_drawdown:\n",
    "                    del current_weights[ticker]\n",
    "        \n",
    "        # Si se eliminaron todas las posiciones, devolver diccionario vacío\n",
    "        if not current_weights:\n",
    "            return {}\n",
    "        \n",
    "        # Normalizar pesos restantes\n",
    "        total_weight = sum(current_weights.values())\n",
    "        if total_weight > 0:\n",
    "            current_weights = {ticker: weight / total_weight for ticker, weight in current_weights.items()}\n",
    "        \n",
    "        return current_weights\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error implementando stop loss para {current_date}: {str(e)}\")\n",
    "        # Devolver los pesos originales en caso de error\n",
    "        return portfolio_weights.get(current_date, {})\n",
    "\n",
    "def calculate_portfolio_returns(prices, portfolio_weights, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Calcula los retornos del portafolio.\n",
    "    \n",
    "    Args:\n",
    "        prices (DataFrame): DataFrame con precios de cierre\n",
    "        portfolio_weights (dict): Diccionario con pesos para cada fecha\n",
    "        start_date (datetime): Fecha de inicio\n",
    "        end_date (datetime): Fecha de fin\n",
    "        \n",
    "    Returns:\n",
    "        Series: Serie con retornos del portafolio\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Crear serie para almacenar retornos\n",
    "        strategy_returns = pd.Series(index=pd.date_range(start=start_date, end=end_date, freq='B'))\n",
    "        \n",
    "        # Filtrar fechas de trading disponibles\n",
    "        trading_dates = prices.index\n",
    "        trading_dates = trading_dates[(trading_dates >= start_date) & (trading_dates <= end_date)]\n",
    "        \n",
    "        if len(trading_dates) < 2:\n",
    "            logging.warning(\"Insuficientes fechas de trading para calcular retornos\")\n",
    "            return pd.Series(index=pd.date_range(start=start_date, end=end_date, freq='B'))\n",
    "        \n",
    "        # Verificar si hay pesos para alguna fecha\n",
    "        valid_dates = [d for d in trading_dates if d in portfolio_weights]\n",
    "        if not valid_dates:\n",
    "            logging.warning(\"No hay pesos de portafolio para ninguna fecha en el período\")\n",
    "            return pd.Series(index=pd.date_range(start=start_date, end=end_date, freq='B'))\n",
    "        \n",
    "        # Calcular retornos diarios\n",
    "        daily_returns = prices.pct_change()\n",
    "        \n",
    "        # Inicializar pesos actuales\n",
    "        current_weights = None\n",
    "        last_rebalance_date = None\n",
    "        \n",
    "        # Para cada fecha de trading\n",
    "        for i in range(1, len(trading_dates)):\n",
    "            current_date = trading_dates[i]\n",
    "            previous_date = trading_dates[i-1]\n",
    "            \n",
    "            # Si es fecha de rebalanceo o primera fecha, actualizar pesos\n",
    "            if current_date in portfolio_weights:\n",
    "                current_weights = portfolio_weights[current_date]\n",
    "                last_rebalance_date = current_date\n",
    "            \n",
    "            # Si no hay pesos actuales, continuar\n",
    "            if current_weights is None:\n",
    "                continue\n",
    "            \n",
    "            # Calcular retorno del portafolio para esta fecha\n",
    "            portfolio_return = 0\n",
    "            \n",
    "            for ticker, weight in current_weights.items():\n",
    "                # Verificar si el ticker está en los datos\n",
    "                if ticker in daily_returns.columns:\n",
    "                    # Obtener retorno para este ticker\n",
    "                    ticker_return = daily_returns.loc[current_date, ticker]\n",
    "                    \n",
    "                    # Si no es NaN, añadir al retorno del portafolio\n",
    "                    if not pd.isna(ticker_return):\n",
    "                        portfolio_return += weight * ticker_return\n",
    "            \n",
    "            # Guardar retorno del portafolio\n",
    "            strategy_returns[current_date] = portfolio_return\n",
    "        \n",
    "        # Eliminar NaN\n",
    "        strategy_returns = strategy_returns.dropna()\n",
    "        \n",
    "        return strategy_returns\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error calculando retornos del portafolio: {str(e)}\")\n",
    "        # Devolver serie vacía en caso de error\n",
    "        return pd.Series()\n",
    "\n",
    "def calculate_performance_metrics(returns, benchmark_returns=None):\n",
    "    \"\"\"\n",
    "    Calcula métricas de rendimiento para una serie de retornos.\n",
    "    \n",
    "    Args:\n",
    "        returns (Series): Serie con retornos\n",
    "        benchmark_returns (Series, optional): Serie con retornos del benchmark\n",
    "        \n",
    "    Returns:\n",
    "        dict: Diccionario con métricas de rendimiento\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Verificar si hay retornos\n",
    "        if returns is None or len(returns) == 0:\n",
    "            return {\n",
    "                'annualized_return': 0,\n",
    "                'annualized_volatility': 0,\n",
    "                'sharpe_ratio': 0,\n",
    "                'max_drawdown': 0,\n",
    "                'win_rate': 0,\n",
    "                'information_ratio': 0,\n",
    "                'sortino_ratio': 0,  # NUEVO\n",
    "                'calmar_ratio': 0,   # NUEVO\n",
    "                'avg_monthly_return': 0,  # NUEVO\n",
    "                'worst_month': 0     # NUEVO\n",
    "            }\n",
    "        \n",
    "        # Asegurarnos de que trabajamos con una Serie unidimensional\n",
    "        if isinstance(returns, pd.DataFrame):\n",
    "            if returns.shape[1] == 1:\n",
    "                returns = returns.iloc[:, 0]\n",
    "            else:\n",
    "                returns = returns.mean(axis=1)  # Promedio si hay múltiples columnas\n",
    "        \n",
    "        # Eliminar NaNs\n",
    "        returns_clean = returns.dropna()\n",
    "        \n",
    "        # Si después de eliminar NaNs no quedan datos, devolver ceros\n",
    "        if len(returns_clean) == 0:\n",
    "            return {\n",
    "                'annualized_return': 0,\n",
    "                'annualized_volatility': 0,\n",
    "                'sharpe_ratio': 0,\n",
    "                'max_drawdown': 0,\n",
    "                'win_rate': 0,\n",
    "                'information_ratio': 0,\n",
    "                'sortino_ratio': 0,\n",
    "                'calmar_ratio': 0,\n",
    "                'avg_monthly_return': 0,\n",
    "                'worst_month': 0\n",
    "            }\n",
    "        \n",
    "        # Calcular retorno acumulado\n",
    "        cumulative_return = (1 + returns_clean).cumprod() - 1\n",
    "        \n",
    "        # Calcular retorno anualizado\n",
    "        n_years = len(returns_clean) / 252.0\n",
    "        annualized_return = (1 + cumulative_return.iloc[-1]) ** (1 / n_years) - 1\n",
    "        \n",
    "        # Calcular volatilidad anualizada\n",
    "        annualized_volatility = returns_clean.std() * np.sqrt(252)\n",
    "        \n",
    "        # Calcular Sharpe ratio\n",
    "        risk_free_rate = 0.02  # Tasa libre de riesgo (2%)\n",
    "        sharpe_ratio = (annualized_return - risk_free_rate) / annualized_volatility if annualized_volatility > 0 else 0\n",
    "        \n",
    "        # Calcular máximo drawdown\n",
    "        peak = cumulative_return.cummax()\n",
    "        drawdown = (cumulative_return - peak) / (1 + peak)\n",
    "        max_drawdown = drawdown.min()\n",
    "        \n",
    "        # Calcular win rate\n",
    "        win_rate = (returns_clean > 0).mean()\n",
    "        \n",
    "        # NUEVO: Calcular Sortino ratio (solo considera downside volatility)\n",
    "        downside_returns = returns_clean[returns_clean < 0]\n",
    "        downside_volatility = downside_returns.std() * np.sqrt(252) if len(downside_returns) > 0 else 0\n",
    "        sortino_ratio = (annualized_return - risk_free_rate) / downside_volatility if downside_volatility > 0 else 0\n",
    "        \n",
    "        # NUEVO: Calcular Calmar ratio (retorno anualizado / máximo drawdown absoluto)\n",
    "        calmar_ratio = annualized_return / abs(max_drawdown) if max_drawdown < 0 else 0\n",
    "        \n",
    "        # NUEVO: Calcular retorno mensual promedio y peor mes\n",
    "        monthly_returns = returns_clean.resample('M').apply(lambda x: (1 + x).prod() - 1)\n",
    "        avg_monthly_return = monthly_returns.mean()\n",
    "        worst_month = monthly_returns.min()\n",
    "        \n",
    "        # Calcular Information Ratio si hay benchmark\n",
    "        information_ratio = 0\n",
    "        if benchmark_returns is not None and hasattr(benchmark_returns, '__len__') and len(benchmark_returns) > 0:\n",
    "            # Convertir a Serie unidimensional si es necesario\n",
    "            if isinstance(benchmark_returns, pd.DataFrame):\n",
    "                if benchmark_returns.shape[1] == 1:\n",
    "                    benchmark_returns = benchmark_returns.iloc[:, 0]\n",
    "                else:\n",
    "                    benchmark_returns = benchmark_returns.mean(axis=1)\n",
    "            elif isinstance(benchmark_returns, np.ndarray):\n",
    "                if benchmark_returns.ndim > 1:\n",
    "                    # Si es un array de más de 1 dimensión, aplanarlo\n",
    "                    benchmark_returns = benchmark_returns.flatten()\n",
    "                benchmark_returns = pd.Series(benchmark_returns, index=returns.index[:len(benchmark_returns)])\n",
    "            \n",
    "            # Limpiar benchmark\n",
    "            benchmark_clean = pd.Series(benchmark_returns).dropna()\n",
    "            \n",
    "            if len(benchmark_clean) > 0:\n",
    "                # Intentar alinear índices\n",
    "                try:\n",
    "                    common_dates = returns_clean.index.intersection(benchmark_clean.index)\n",
    "                    if len(common_dates) > 0:\n",
    "                        aligned_returns = returns_clean.loc[common_dates]\n",
    "                        aligned_benchmark = benchmark_clean.loc[common_dates]\n",
    "                        \n",
    "                        # Calcular excess returns\n",
    "                        excess_returns = aligned_returns - aligned_benchmark\n",
    "                        \n",
    "                        # Calcular tracking error\n",
    "                        tracking_error = excess_returns.std() * np.sqrt(252)\n",
    "                        \n",
    "                        # Calcular Information Ratio\n",
    "                        if tracking_error > 0:\n",
    "                            information_ratio = excess_returns.mean() * 252 / tracking_error\n",
    "                except Exception as inner_e:\n",
    "                    logging.error(f\"Error alineando benchmark: {str(inner_e)}\")\n",
    "        \n",
    "        return {\n",
    "            'annualized_return': float(annualized_return),\n",
    "            'annualized_volatility': float(annualized_volatility),\n",
    "            'sharpe_ratio': float(sharpe_ratio),\n",
    "            'max_drawdown': float(max_drawdown),\n",
    "            'win_rate': float(win_rate),\n",
    "            'information_ratio': float(information_ratio),\n",
    "            'sortino_ratio': float(sortino_ratio),\n",
    "            'calmar_ratio': float(calmar_ratio),\n",
    "            'avg_monthly_return': float(avg_monthly_return),\n",
    "            'worst_month': float(worst_month)\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error calculando métricas de rendimiento: {str(e)}\")\n",
    "        import traceback\n",
    "        logging.error(traceback.format_exc())\n",
    "        # Devolver métricas vacías en caso de error\n",
    "        return {\n",
    "            'annualized_return': 0,\n",
    "            'annualized_volatility': 0,\n",
    "            'sharpe_ratio': 0,\n",
    "            'max_drawdown': 0,\n",
    "            'win_rate': 0,\n",
    "            'information_ratio': 0,\n",
    "            'sortino_ratio': 0,\n",
    "            'calmar_ratio': 0,\n",
    "            'avg_monthly_return': 0,\n",
    "            'worst_month': 0\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "def backtest_strategy(tickers, sectors, start_date, end_date, rebalance_freq='M'):\n",
    "    \"\"\"\n",
    "    Realiza un backtest de la estrategia.\n",
    "    \n",
    "    Args:\n",
    "        tickers (list): Lista de tickers\n",
    "        sectors (dict): Diccionario con sectores para cada ticker\n",
    "        start_date (str): Fecha de inicio en formato 'YYYY-MM-DD'\n",
    "        end_date (str): Fecha de fin en formato 'YYYY-MM-DD'\n",
    "        rebalance_freq (str): Frecuencia de rebalanceo ('D', 'W', 'M', etc.)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (retornos de la estrategia, retornos del benchmark, métricas, regímenes de mercado, \n",
    "                volatilidad del mercado, multiplicadores de exposición, pesos del último portafolio)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Iniciando backtest...\")\n",
    "        \n",
    "        # Descargar datos incluyendo activos defensivos\n",
    "        prices, volume, defensive_sectors = download_data(tickers, start_date, end_date, include_defensive=True)\n",
    "        \n",
    "        # Actualizar diccionario de sectores con activos defensivos\n",
    "        sectors.update(defensive_sectors)\n",
    "        \n",
    "        # Verificar si hay datos\n",
    "        if prices.empty or volume.empty:\n",
    "            raise ValueError(\"No se pudieron obtener datos para los tickers especificados\")\n",
    "        \n",
    "        # Descargar datos del benchmark (S&P 500)\n",
    "        benchmark_data = yf.download('^GSPC', start=start_date, end=end_date, progress=False)\n",
    "        benchmark_returns = benchmark_data['Close'].pct_change().dropna()\n",
    "        \n",
    "        # Calcular retornos para diferentes períodos\n",
    "        periods = {\n",
    "            '1M': 21,\n",
    "            '3M': 63,\n",
    "            '6M': 126,\n",
    "            '12M': 252\n",
    "        }\n",
    "        returns = calculate_returns(prices, periods)\n",
    "        \n",
    "        # Calcular características\n",
    "        print(\"Calculando características...\")\n",
    "        features = calculate_features(prices, volume, returns)\n",
    "        \n",
    "        # Detectar regímenes de mercado\n",
    "        print(\"Detectando regímenes de mercado...\")\n",
    "        market_regimes = detect_market_regimes(prices)\n",
    "        \n",
    "        # Generar señales\n",
    "        print(\"Generando señales...\")\n",
    "        signals = combine_signals(features, market_regimes)\n",
    "        \n",
    "        # Ajustar señales por autocorrelación\n",
    "        print(\"Ajustando señales por autocorrelación...\")\n",
    "        adjusted_signals = adjust_for_autocorrelation(signals)\n",
    "        \n",
    "        # Combinar señales\n",
    "        print(\"Combinando señales...\")\n",
    "        combined_signals = combine_signals(adjusted_signals)\n",
    "        \n",
    "        # Determinar fechas de rebalanceo\n",
    "        rebalance_dates = pd.date_range(start=start_date, end=end_date, freq=rebalance_freq)\n",
    "        rebalance_dates = rebalance_dates[rebalance_dates.isin(prices.index)]\n",
    "        \n",
    "        # NUEVO: Calcular volatilidad del mercado para volatility targeting\n",
    "        market_returns = prices.pct_change().mean(axis=1)\n",
    "        market_volatility = market_returns.rolling(window=21).std() * np.sqrt(252)\n",
    "        \n",
    "        # NUEVO: Calcular multiplicadores de exposición para volatility targeting\n",
    "        exposure_multipliers = pd.Series(index=prices.index)\n",
    "        for date in prices.index:\n",
    "            if date in market_volatility.index and not pd.isna(market_volatility[date]):\n",
    "                exposure_multipliers[date] = calculate_volatility_target_multiplier(market_volatility[date])\n",
    "            else:\n",
    "                exposure_multipliers[date] = 1.0\n",
    "        \n",
    "        # Construir portafolios para cada fecha de rebalanceo\n",
    "        print(\"Construyendo portafolios...\")\n",
    "        portfolio_weights = {}\n",
    "        \n",
    "        for date in rebalance_dates:\n",
    "            # Verificar si hay señales para esta fecha\n",
    "            date_signals = combined_signals[combined_signals['date'] == date]\n",
    "            \n",
    "            if not date_signals.empty:\n",
    "                # Construir portafolio\n",
    "                weights = construct_portfolio(combined_signals, sectors, date, max_position_size=0.05)\n",
    "                \n",
    "                # NUEVO: Implementar stop loss\n",
    "                # weights = implement_stop_loss(prices, {date: weights}, date, max_drawdown=-0.15)\n",
    "                \n",
    "                # NUEVO: Aplicar volatility targeting\n",
    "                if date in exposure_multipliers.index:\n",
    "                    vol_multiplier = exposure_multipliers[date]\n",
    "                    \n",
    "                    # Si el multiplicador es menor que 1, reducir exposición total\n",
    "                    if vol_multiplier < 1:\n",
    "                        # Crear posición en efectivo (no invertida)\n",
    "                        weights['CASH'] = 1 - vol_multiplier\n",
    "                        \n",
    "                        # Reducir todas las demás posiciones proporcionalmente\n",
    "                        for ticker in list(weights.keys()):\n",
    "                            if ticker != 'CASH':\n",
    "                                weights[ticker] *= vol_multiplier\n",
    "                \n",
    "                # Guardar pesos\n",
    "                portfolio_weights[date] = weights\n",
    "        \n",
    "        # Calcular retornos del portafolio\n",
    "        print(\"Calculando retornos...\")\n",
    "        strategy_returns = calculate_portfolio_returns(\n",
    "            prices,\n",
    "            portfolio_weights,\n",
    "            pd.to_datetime(start_date),\n",
    "            pd.to_datetime(end_date)\n",
    "        )\n",
    "        \n",
    "        # Calcular métricas de rendimiento\n",
    "        print(\"Calculando métricas de rendimiento...\")\n",
    "        metrics = calculate_performance_metrics(strategy_returns, benchmark_returns)\n",
    "        \n",
    "        \n",
    "        # Guardar métricas en CSV\n",
    "        metrics_df = pd.DataFrame([metrics])\n",
    "        metrics_df.to_csv('./artifacts/results/data/performance_metrics.csv', index=False)\n",
    "        \n",
    "        # Guardar retornos en CSV\n",
    "        strategy_returns.to_csv('./artifacts/results/data/strategy_returns.csv')\n",
    "        benchmark_returns.to_csv('./artifacts/results/data/benchmark_returns.csv')\n",
    "        \n",
    "        # Variable para almacenar los pesos del último portafolio\n",
    "        last_portfolio_weights = {}\n",
    "        \n",
    "        # Guardar exposición sectorial para la última fecha de rebalanceo\n",
    "        if rebalance_dates.size > 0:\n",
    "            last_rebalance = rebalance_dates[-1]\n",
    "            if last_rebalance in portfolio_weights:\n",
    "                plot_sector_exposure(\n",
    "                    portfolio_weights[last_rebalance],\n",
    "                    sectors,\n",
    "                    last_rebalance,\n",
    "                    title='Last Rebalance Sector Exposure'\n",
    "                )\n",
    "                \n",
    "                # Guardar los pesos del último portafolio\n",
    "                last_portfolio_weights = portfolio_weights[last_rebalance]\n",
    "                \n",
    "                # Convertir el diccionario de pesos a DataFrame y guardar como CSV\n",
    "                last_weights_df = pd.DataFrame(list(last_portfolio_weights.items()), \n",
    "                                             columns=['ticker', 'weight'])\n",
    "                last_weights_df['date'] = last_rebalance\n",
    "                last_weights_df = last_weights_df[['date', 'ticker', 'weight']]  # Reordenar columnas\n",
    "                last_weights_df.to_csv(f'./artifacts/results/data/last_portfolio_weights{end_date}.csv', index=False)\n",
    "                \n",
    "                print(f\"Pesos del último portafolio guardados (fecha: {last_rebalance.strftime('%Y-%m-%d')})\")\n",
    "        \n",
    "        print(\"Backtest completado.\")\n",
    "        \n",
    "        return strategy_returns, benchmark_returns, metrics, market_regimes, market_volatility, exposure_multipliers, last_portfolio_weights\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en backtest: {str(e)}\")\n",
    "        import traceback\n",
    "        logging.error(traceback.format_exc())\n",
    "        # Devolver valores vacíos en caso de error\n",
    "        return pd.Series(), pd.Series(), {}, pd.Series(), pd.Series(), pd.Series(), {}\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal que ejecuta la estrategia.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Iniciando estrategia de momentum multi-horizonte mejorada...\")\n",
    "        \n",
    "        # Obtener tickers y sectores del S&P 500\n",
    "        sectors = get_sp500_tickers()\n",
    "        tickers = list(sectors.keys())\n",
    "        \n",
    "        # Si hay demasiados tickers, limitar para evitar errores de API\n",
    "        if len(tickers) > 10:\n",
    "            tickers = tickers[:10]\n",
    "            sectors = {ticker: sectors[ticker] for ticker in tickers}\n",
    "        \n",
    "        # Definir fechas\n",
    "        end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        start_date = (datetime.now() - timedelta(days=3*365)).strftime('%Y-%m-%d')  # 3 años\n",
    "        # start_date = \"2022-01-01\"\n",
    "        # end_date = \"2024-01-01\"\n",
    "\n",
    "        # Realizar backtest\n",
    "        print(\"\\n=== Ejecutando Backtest ===\", start_date, end_date)\n",
    "    \n",
    "        strategy_returns, benchmark_returns, metrics, market_regimes, market_volatility, exposure_multipliers, last_portfolio_weights = backtest_strategy(\n",
    "            tickers,\n",
    "            sectors,\n",
    "            start_date,\n",
    "            end_date,\n",
    "            rebalance_freq='M'  # Rebalanceo mensual\n",
    "        )\n",
    "        \n",
    "        # Mostrar métricas\n",
    "        print(\"\\nMétricas de rendimiento del backtest:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "        # Make plots\n",
    "        print(\"\\nPlots:\")\n",
    "        make_plots(strategy_returns, benchmark_returns, market_regimes, market_volatility, exposure_multipliers)\n",
    "        \n",
    "        # Mostrar los pesos del último portafoliou\n",
    "        if last_portfolio_weights:\n",
    "            print(\"\\nPesos del último portafolio:\")\n",
    "            for ticker, weight in sorted(last_portfolio_weights.items(), key=lambda x: x[1], reverse=True):\n",
    "                print(f\"{ticker}: {weight:.4f}\")\n",
    "            print(f\"\\nEstos pesos han sido guardados en './artifacts/results/data/last_portfolio_weights{end_date}.csv'\")\n",
    "                   \n",
    "        print(\"\\nEstrategia completada. Resultados guardados en ./artifacts/results/\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en función principal: {str(e)}\")\n",
    "        import traceback\n",
    "        logging.error(traceback.format_exc())\n",
    "        print(f\"Error: {str(e)} {traceback.format_exc()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63f181f-9688-49d4-9499-2bb69c74802b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54628685-7286-4000-843e-6353ff7e45dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c966826-aa89-4f25-b41b-dde97704f0b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
