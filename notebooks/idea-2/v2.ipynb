{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e75a7e-9389-48a0-8fb2-a186515829e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando implementación de estrategia de arbitraje estadístico...\n",
      "Obteniendo tickers del S&P 500 para período 2017-01-04 - 2025-04-15\n",
      "Descargando datos históricos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Descargando datos:   0%|                                | 0/503 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Descargando datos: 100%|██████████████████████| 503/503 [02:34<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos obtenidos para 503 tickers. Fallaron 0 tickers.\n",
      "Procesando datos...\n",
      "Inicializando estrategia...\n",
      "Ejecutando backtest...\n"
     ]
    }
   ],
   "source": [
    "# Asegurar importaciones necesarias\n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import coint as statsmodels_coint\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from hmmlearn import hmm\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "from scipy.stats import linregress\n",
    "import json\n",
    "# Configuraciones para visualización\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "warnings.filterwarnings(\"ignore\")   \n",
    "\n",
    "# Configurar logging más detallado\n",
    "logging.basicConfig(\n",
    "    filename='./artifacts/logs/strategy.log',\n",
    "    level=logging.INFO,\n",
    "    format='[%(asctime)s] %(levelname)s: %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# Crear directorios para logs y resultados si no existen\n",
    "os.makedirs('./artifacts/logs', exist_ok=True)\n",
    "os.makedirs('./artifacts/results', exist_ok=True)\n",
    "os.makedirs('./artifacts/results/figures', exist_ok=True)\n",
    "os.makedirs('./artifacts/results/data', exist_ok=True)\n",
    "\n",
    "\n",
    "# Función para obtener tickers del S&P 500\n",
    "def get_sp500_tickers():\n",
    "    \"\"\"Obtiene los tickers del S&P 500 y sus sectores GICS\"\"\"\n",
    "    try:\n",
    "        sp500_table = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "        tickers = sp500_table['Symbol'].str.replace('.', '-').tolist()\n",
    "        sector_map = {k.replace('.', '-'): v for k, v in \n",
    "                     sp500_table.set_index('Symbol')['GICS Sector'].to_dict().items()}\n",
    "        subsector_map = {k.replace('.', '-'): v for k, v in \n",
    "                        sp500_table.set_index('Symbol')['GICS Sub-Industry'].to_dict().items()}\n",
    "        return tickers, sector_map, subsector_map\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error obteniendo tickers del S&P 500: {str(e)}\")\n",
    "        return [], {}, {}\n",
    "\n",
    "# Función para descargar datos históricos\n",
    "def get_historical_data(tickers, start_date, end_date, sleep_time=0.1):\n",
    "    \"\"\"Descarga datos históricos para los tickers especificados\"\"\"\n",
    "    data = {}\n",
    "    failed_tickers = []\n",
    "    \n",
    "    for ticker in tqdm(tickers, desc=\"Descargando datos\"):\n",
    "        try:\n",
    "            ticker_data = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "            if len(ticker_data) > 60:  # Asegurar datos suficientes\n",
    "                data[ticker] = ticker_data\n",
    "            else:\n",
    "                failed_tickers.append(ticker)\n",
    "            time.sleep(sleep_time)  # Evitar sobrecargar la API\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error descargando datos para {ticker}: {str(e)}\")\n",
    "            failed_tickers.append(ticker)\n",
    "    \n",
    "    print(f\"Datos obtenidos para {len(data)} tickers. Fallaron {len(failed_tickers)} tickers.\")\n",
    "    return data\n",
    "\n",
    "# Preprocesamiento de datos\n",
    "def preprocess_data(data):\n",
    "    \"\"\"Preprocesa los datos históricos para la estrategia\"\"\"\n",
    "    tickers = list(data.keys())\n",
    "    \n",
    "    # Obtener fechas comunes\n",
    "    all_dates = set()\n",
    "    for ticker_data in data.values():\n",
    "        all_dates.update(ticker_data.index)\n",
    "    all_dates = sorted(list(all_dates))\n",
    "    \n",
    "    # Crear DataFrames alineados\n",
    "    prices = pd.DataFrame(index=all_dates, columns=tickers)\n",
    "    volumes = pd.DataFrame(index=all_dates, columns=tickers)\n",
    "    \n",
    "    # Usar método más seguro para llenar los DataFrames\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Obtener datos para el ticker actual\n",
    "            ticker_data = data[ticker]\n",
    "            \n",
    "            # Verificar si existen las columnas necesarias\n",
    "            if 'Close' in ticker_data.columns and 'Volume' in ticker_data.columns:\n",
    "                # Crear Series temporales con el índice completo\n",
    "                close_series = pd.Series(index=all_dates, dtype='float64')\n",
    "                volume_series = pd.Series(index=all_dates, dtype='float64')\n",
    "                \n",
    "                # Llenar solo en los índices donde tenemos datos\n",
    "                for date in ticker_data.index:\n",
    "                    if date in all_dates:  # Verificación adicional de seguridad\n",
    "                        close_series[date] = ticker_data.loc[date, 'Close']\n",
    "                        volume_series[date] = ticker_data.loc[date, 'Volume']\n",
    "                \n",
    "                # Asignar las series a los DataFrames\n",
    "                prices[ticker] = close_series\n",
    "                volumes[ticker] = volume_series\n",
    "        except Exception as e:\n",
    "            # Ignorar tickers con errores y continuar con los demás\n",
    "            print(f\"Error procesando ticker {ticker}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Calcular métricas derivadas (asegurando que no haya errores)\n",
    "    returns = prices.pct_change().replace([np.inf, -np.inf], np.nan)\n",
    "    realized_vol = returns.rolling(window=22).std() * np.sqrt(252)\n",
    "    \n",
    "    # Manejar caso donde volumes tiene todos NaN para algunos tickers\n",
    "    relative_volume = pd.DataFrame(index=volumes.index, columns=volumes.columns)\n",
    "    for col in volumes.columns:\n",
    "        if not volumes[col].isna().all():  # Solo procesar columnas con datos\n",
    "            rolling_mean = volumes[col].rolling(window=20).mean()\n",
    "            relative_volume[col] = volumes[col] / rolling_mean\n",
    "    \n",
    "    # Manejar outliers en retornos de manera segura\n",
    "    mad = lambda x: np.abs(x - x.median()).median() if not x.isna().all() else np.nan\n",
    "    returns_mad = returns.apply(mad)\n",
    "    outlier_threshold = 3.5\n",
    "    \n",
    "    returns_cleaned = returns.copy()\n",
    "    for col in returns.columns:\n",
    "        if pd.isna(returns_mad[col]):\n",
    "            continue\n",
    "            \n",
    "        threshold = outlier_threshold * returns_mad[col]\n",
    "        outliers = np.abs(returns[col]) > threshold\n",
    "        \n",
    "        # Solo modificar valores donde hay outliers\n",
    "        if outliers.any():\n",
    "            for idx in returns.index[outliers]:\n",
    "                # Verificar cada valor antes de modificarlo\n",
    "                if not pd.isna(returns.loc[idx, col]):\n",
    "                    sign = 1 if returns.loc[idx, col] > 0 else -1\n",
    "                    returns_cleaned.loc[idx, col] = sign * threshold\n",
    "    \n",
    "    # Imputar datos faltantes (gaps < 3 días)\n",
    "    prices_filled = prices.interpolate(method='linear', limit=3, axis=0)\n",
    "    \n",
    "    # Calcular ADV en dólares (de forma segura)\n",
    "    adv = pd.DataFrame(index=volumes.index, columns=volumes.columns)\n",
    "    for col in volumes.columns:\n",
    "        if not volumes[col].isna().all() and not prices[col].isna().all():\n",
    "            adv[col] = (volumes[col] * prices[col]).rolling(window=20).mean()\n",
    "    \n",
    "    # Recalcular retornos con precios imputados\n",
    "    returns_filled = prices_filled.pct_change().replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    processed_data = {\n",
    "        'prices': prices_filled,\n",
    "        'returns': returns_filled,\n",
    "        'volumes': volumes,\n",
    "        'realized_vol': realized_vol,\n",
    "        'relative_volume': relative_volume,\n",
    "        'adv': adv\n",
    "    }\n",
    "    \n",
    "    return processed_data\n",
    "    \n",
    "# Sistema de Detección de Regímenes\n",
    "class RegimeDetector:\n",
    "    \"\"\"Implementa el sistema de detección de regímenes usando HMM con correcciones críticas\"\"\"\n",
    "    \n",
    "    def __init__(self, min_train_samples=1260, persistence_days=3, prob_threshold=0.85):\n",
    "        # Cambio a min_train_samples en lugar de min_train_years\n",
    "        self.min_train_samples = min_train_samples  # Aprox. 5 años de datos diarios\n",
    "        self.persistence_days = persistence_days\n",
    "        self.prob_threshold = prob_threshold\n",
    "        self.hmm_models = {\n",
    "            'hmm1': None,  # 2 estados, vol + correlaciones\n",
    "            'hmm2': None,  # 3 estados, retornos + vol + volumen\n",
    "            'hmm3': None   # 2 estados, dispersión + breadth\n",
    "        }\n",
    "        self.model_weights = {'hmm1': 1/3, 'hmm2': 1/3, 'hmm3': 1/3}\n",
    "        self.last_calibration = None\n",
    "        self.regime_history = None\n",
    "        self.current_regime = None\n",
    "        self.sector_map = {}\n",
    "        self.is_model_valid = {'hmm1': False, 'hmm2': False, 'hmm3': False}\n",
    "    \n",
    "    def create_features(self, data, market_index='^GSPC', lookback=None):\n",
    "        \"\"\"Crea features para los modelos HMM\"\"\"\n",
    "        try:\n",
    "            # Usar lookback adaptativo si se proporciona\n",
    "            if lookback is None:\n",
    "                lookback = 252  # Por defecto\n",
    "            \n",
    "            prices = data['prices'].iloc[-lookback:] if len(data['prices']) > lookback else data['prices']\n",
    "            returns = data['returns'].iloc[-lookback:] if len(data['returns']) > lookback else data['returns']\n",
    "            realized_vol = data['realized_vol'].iloc[-lookback:] if len(data['realized_vol']) > lookback else data['realized_vol']\n",
    "            relative_volume = data['relative_volume'].iloc[-lookback:] if len(data['relative_volume']) > lookback else data['relative_volume']\n",
    "            \n",
    "            # Verificación de datos suficientes\n",
    "            if len(prices) < 60:\n",
    "                logging.warning(f\"Datos insuficientes para crear features: {len(prices)} < 60\")\n",
    "                return {'hmm1': pd.DataFrame(), 'hmm2': pd.DataFrame(), 'hmm3': pd.DataFrame()}\n",
    "            \n",
    "            # Obtener datos de mercado si no están en los datos\n",
    "            if market_index not in prices.columns:\n",
    "                try:\n",
    "                    market_data = yf.download(market_index, \n",
    "                                          start=prices.index[0].strftime('%Y-%m-%d'),\n",
    "                                          end=prices.index[-1].strftime('%Y-%m-%d'),\n",
    "                                          progress=False)\n",
    "                    \n",
    "                    # Verificar que se obtuvieron datos\n",
    "                    if len(market_data) < 30:\n",
    "                        logging.warning(f\"Datos insuficientes para {market_index}: {len(market_data)} < 30\")\n",
    "                        # Usar promedio como proxy\n",
    "                        market_returns = returns.mean(axis=1)\n",
    "                        market_vol = market_returns.rolling(window=min(22, len(market_returns))).std() * np.sqrt(252)\n",
    "                    else:\n",
    "                        market_returns = market_data['Close'].pct_change().dropna()\n",
    "                        market_vol = market_returns.rolling(window=min(22, len(market_returns))).std() * np.sqrt(252)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error obteniendo datos de mercado: {str(e)}\")\n",
    "                    logging.info(\"Usando datos promedio como proxy para el mercado\")\n",
    "                    # Usar promedio como proxy\n",
    "                    market_returns = returns.mean(axis=1)\n",
    "                    market_vol = market_returns.rolling(window=min(22, len(market_returns))).std() * np.sqrt(252)\n",
    "            else:\n",
    "                market_returns = returns[market_index]\n",
    "                market_vol = realized_vol[market_index]\n",
    "            \n",
    "            # Features para HMM-1: volatilidad y correlaciones sectoriales\n",
    "            vix_proxy = market_vol.dropna()\n",
    "            \n",
    "            # Calcular correlaciones entre sectores con manejo adaptativo\n",
    "            sector_returns = {}\n",
    "            for sector in set(self.sector_map.values()):\n",
    "                sector_tickers = [t for t in returns.columns if self.sector_map.get(t) == sector]\n",
    "                if sector_tickers:\n",
    "                    # Usar solo columnas existentes\n",
    "                    valid_tickers = [t for t in sector_tickers if t in returns.columns]\n",
    "                    if valid_tickers:\n",
    "                        sector_returns[sector] = returns[valid_tickers].mean(axis=1)\n",
    "            \n",
    "            sector_corr = pd.DataFrame(index=prices.index)\n",
    "            if len(sector_returns) > 1:\n",
    "                sectors = list(sector_returns.keys())\n",
    "                for i in range(len(sectors)):\n",
    "                    for j in range(i+1, len(sectors)):\n",
    "                        s1, s2 = sectors[i], sectors[j]\n",
    "                        # Verificar que tenemos datos para ambos sectores\n",
    "                        if s1 in sector_returns and s2 in sector_returns:\n",
    "                            pair_df = pd.DataFrame({s1: sector_returns[s1], s2: sector_returns[s2]})\n",
    "                            # Eliminar filas con NaN para evitar problemas en correlación\n",
    "                            pair_df = pair_df.dropna()\n",
    "                            \n",
    "                            # Usar ventana adaptativa\n",
    "                            roll_win = min(22, len(pair_df) // 2) if len(pair_df) > 10 else 5\n",
    "                            if len(pair_df) >= roll_win and roll_win > 1:\n",
    "                                rolling_corr = pair_df.rolling(window=roll_win).corr().iloc[1::2][s1]\n",
    "                                # Asegurarse de que rolling_corr no esté vacío\n",
    "                                if not rolling_corr.empty:\n",
    "                                    sector_corr[f'{s1}_{s2}'] = np.nan\n",
    "                                    # Comprobar índices coincidentes antes de asignar\n",
    "                                    valid_indices = rolling_corr.index.intersection(sector_corr.index)\n",
    "                                    if len(valid_indices) > 0:\n",
    "                                        for idx in valid_indices:\n",
    "                                            if not pd.isna(rolling_corr.loc[idx]):\n",
    "                                                sector_corr.loc[idx, f'{s1}_{s2}'] = rolling_corr.loc[idx]\n",
    "                \n",
    "                # Solo calcular mean si hay columnas\n",
    "                if len(sector_corr.columns) > 0:\n",
    "                    mean_sector_corr = sector_corr.mean(axis=1)\n",
    "                else:\n",
    "                    logging.info(\"No hay suficientes correlaciones sectoriales, usando valor por defecto\")\n",
    "                    mean_sector_corr = pd.Series(index=prices.index, data=0.5)\n",
    "            else:\n",
    "                logging.info(\"Menos de 2 sectores disponibles, usando valor por defecto para correlación\")\n",
    "                mean_sector_corr = pd.Series(index=prices.index, data=0.5)\n",
    "            \n",
    "            # Features para HMM-2: retornos, volatilidad y volumen relativo\n",
    "            market_rets = market_returns.dropna()\n",
    "            avg_rel_vol = relative_volume.mean(axis=1)\n",
    "            \n",
    "            # Features para HMM-3: dispersión sectorial y breadth de mercado\n",
    "            if len(sector_returns) > 1:\n",
    "                # Convertir a DataFrame y asegurar que no hay NaNs\n",
    "                sector_df = pd.DataFrame(sector_returns).dropna(how='all')\n",
    "                if not sector_df.empty and sector_df.shape[1] > 1:  # Necesitamos al menos 2 columnas para std\n",
    "                    sector_dispersion = sector_df.std(axis=1)\n",
    "                else:\n",
    "                    logging.info(\"Datos insuficientes para cálculo de dispersión sectorial\")\n",
    "                    sector_dispersion = pd.Series(index=prices.index, data=0.01)\n",
    "            else:\n",
    "                logging.info(\"Menos de 2 sectores disponibles, usando valor por defecto para dispersión\")\n",
    "                sector_dispersion = pd.Series(index=prices.index, data=0.01)\n",
    "            \n",
    "            # Calcular market breadth adaptativo (% de acciones sobre su MA)\n",
    "            ma_window = min(50, len(prices) // 3) if len(prices) > 60 else 20\n",
    "            above_ma = pd.DataFrame(index=prices.index, columns=prices.columns)\n",
    "            ma = prices.rolling(window=ma_window).mean()\n",
    "            for col in prices.columns:\n",
    "                above_ma[col] = prices[col] > ma[col]\n",
    "            \n",
    "            market_breadth = above_ma.mean(axis=1)\n",
    "            \n",
    "            # Alinear y eliminar NaNs con cuidado\n",
    "            features_hmm1 = pd.DataFrame()\n",
    "            if not vix_proxy.empty and not mean_sector_corr.empty:\n",
    "                # Encontrar índices comunes\n",
    "                common_indices = vix_proxy.index.intersection(mean_sector_corr.index)\n",
    "                if len(common_indices) > 0:\n",
    "                    features_hmm1 = pd.DataFrame({\n",
    "                        'volatility': vix_proxy.loc[common_indices].values,\n",
    "                        'sector_correlation': mean_sector_corr.loc[common_indices].values\n",
    "                    }, index=common_indices).dropna()\n",
    "                    logging.info(f\"Features HMM1 creadas: {len(features_hmm1)} observaciones\")\n",
    "                else:\n",
    "                    logging.warning(\"No hay índices comunes para features HMM1\")\n",
    "            else:\n",
    "                logging.warning(\"Datos insuficientes para features HMM1\")\n",
    "            \n",
    "            features_hmm2 = pd.DataFrame()\n",
    "            # Verificar que tenemos todos los datos necesarios\n",
    "            if not market_rets.empty and not market_vol.empty and not avg_rel_vol.empty:\n",
    "                # Alinear índices\n",
    "                common_indices = market_rets.index.intersection(market_vol.index).intersection(avg_rel_vol.index)\n",
    "                if len(common_indices) > 0:\n",
    "                    features_hmm2 = pd.DataFrame({\n",
    "                        'returns': market_rets.loc[common_indices].values,\n",
    "                        'volatility': market_vol.loc[common_indices].values,\n",
    "                        'relative_volume': avg_rel_vol.loc[common_indices].values\n",
    "                    }, index=common_indices).dropna()\n",
    "                    logging.info(f\"Features HMM2 creadas: {len(features_hmm2)} observaciones\")\n",
    "                else:\n",
    "                    logging.warning(\"No hay índices comunes para features HMM2\")\n",
    "            else:\n",
    "                logging.warning(\"Datos insuficientes para features HMM2\")\n",
    "            \n",
    "            features_hmm3 = pd.DataFrame()\n",
    "            if not sector_dispersion.empty and not market_breadth.empty:\n",
    "                common_indices = sector_dispersion.index.intersection(market_breadth.index)\n",
    "                if len(common_indices) > 0:\n",
    "                    features_hmm3 = pd.DataFrame({\n",
    "                        'sector_dispersion': sector_dispersion.loc[common_indices].values,\n",
    "                        'market_breadth': market_breadth.loc[common_indices].values\n",
    "                    }, index=common_indices).dropna()\n",
    "                    logging.info(f\"Features HMM3 creadas: {len(features_hmm3)} observaciones\")\n",
    "                else:\n",
    "                    logging.warning(\"No hay índices comunes para features HMM3\")\n",
    "            else:\n",
    "                logging.warning(\"Datos insuficientes para features HMM3\")\n",
    "            \n",
    "            result = {\n",
    "                'hmm1': features_hmm1,\n",
    "                'hmm2': features_hmm2,\n",
    "                'hmm3': features_hmm3\n",
    "            }\n",
    "            \n",
    "            # Verificar si tenemos suficientes datos para al menos un modelo\n",
    "            has_sufficient_data = any(not df.empty and len(df) >= 30 for df in result.values())\n",
    "            if not has_sufficient_data:\n",
    "                logging.warning(\"No hay suficientes datos para ningún modelo HMM\")\n",
    "            \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en create_features: {str(e)}\")\n",
    "            logging.error(f\"Stack trace: {traceback.format_exc()}\")\n",
    "            return {'hmm1': pd.DataFrame(), 'hmm2': pd.DataFrame(), 'hmm3': pd.DataFrame(), 'error': str(e)}\n",
    "    \n",
    "    def train_models(self, features, end_date=None):\n",
    "        \"\"\"Entrena los modelos HMM con las features proporcionadas\"\"\"\n",
    "        try:\n",
    "            # Reset estado de modelos\n",
    "            self.is_model_valid = {'hmm1': False, 'hmm2': False, 'hmm3': False}\n",
    "            \n",
    "            self.hmm_models = {\n",
    "                'hmm1': hmm.GaussianHMM(n_components=2, covariance_type=\"full\", \n",
    "                                      n_iter=1000, random_state=42),\n",
    "                'hmm2': hmm.GaussianHMM(n_components=3, covariance_type=\"full\", \n",
    "                                      n_iter=1000, random_state=42),\n",
    "                'hmm3': hmm.GaussianHMM(n_components=2, covariance_type=\"full\", \n",
    "                                      n_iter=1000, random_state=42)\n",
    "            }\n",
    "            \n",
    "            # Limitar datos a la fecha de fin si se proporciona\n",
    "            if end_date:\n",
    "                for key in features:\n",
    "                    if isinstance(features[key], pd.DataFrame) and not features[key].empty:\n",
    "                        features[key] = features[key][features[key].index <= end_date]\n",
    "            \n",
    "            # Entrenar modelos\n",
    "            for key, model in self.hmm_models.items():\n",
    "                if key in features and isinstance(features[key], pd.DataFrame) and not features[key].empty and features[key].shape[0] > 30:\n",
    "                    logging.info(f\"Entrenando modelo {key} con {features[key].shape[0]} observaciones\")\n",
    "                    \n",
    "                    # Asegurar que los datos están como array 2D\n",
    "                    X = features[key].values\n",
    "                    # StandardScaler espera datos 2D\n",
    "                    scaler = StandardScaler()\n",
    "                    X_scaled = scaler.fit_transform(X)\n",
    "                    \n",
    "                    try:\n",
    "                        self.hmm_models[key].fit(X_scaled)\n",
    "                        self.is_model_valid[key] = True\n",
    "                        logging.info(f\"Modelo {key} entrenado exitosamente\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error entrenando modelo {key}: {str(e)}\")\n",
    "                        self.hmm_models[key] = None\n",
    "                        self.is_model_valid[key] = False\n",
    "                else:\n",
    "                    logging.warning(f\"Datos insuficientes para entrenar modelo {key}\")\n",
    "                    self.hmm_models[key] = None\n",
    "                    self.is_model_valid[key] = False\n",
    "            \n",
    "            self.last_calibration = datetime.now()\n",
    "            \n",
    "            # Verificar si al menos un modelo se entrenó correctamente\n",
    "            if not any(self.is_model_valid.values()):\n",
    "                logging.error(\"Ningún modelo pudo ser entrenado correctamente\")\n",
    "            else:\n",
    "                valid_models = [k for k, v in self.is_model_valid.items() if v]\n",
    "                logging.info(f\"Modelos válidos después del entrenamiento: {valid_models}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error general en train_models: {str(e)}\")\n",
    "            logging.error(f\"Stack trace: {traceback.format_exc()}\")\n",
    "    \n",
    "    def predict_regimes(self, features, apply_persistence=True):\n",
    "        \"\"\"Predice regímenes con los modelos HMM\"\"\"\n",
    "        try:\n",
    "            predictions = {}\n",
    "            probs_log = {}\n",
    "            \n",
    "            # Predecir con cada modelo\n",
    "            for key, model in self.hmm_models.items():\n",
    "                if (self.is_model_valid[key] and model is not None and \n",
    "                    key in features and isinstance(features[key], pd.DataFrame) and \n",
    "                    not features[key].empty and features[key].shape[0] > 1):\n",
    "                    \n",
    "                    # Asegurar que los datos están como array 2D\n",
    "                    X = features[key].values\n",
    "                    # StandardScaler espera datos 2D\n",
    "                    scaler = StandardScaler()\n",
    "                    X_scaled = scaler.fit_transform(X)\n",
    "                    \n",
    "                    try:\n",
    "                        hidden_states = model.predict(X_scaled)\n",
    "                        probs = model.predict_proba(X_scaled)\n",
    "                        \n",
    "                        pred_df = pd.DataFrame(index=features[key].index)\n",
    "                        pred_df['state'] = hidden_states\n",
    "                        \n",
    "                        for i in range(model.n_components):\n",
    "                            pred_df[f'prob_state_{i}'] = probs[:, i]\n",
    "                        \n",
    "                        predictions[key] = pred_df\n",
    "                        \n",
    "                        # Guardar probabilidades para logs\n",
    "                        if len(probs) > 0:\n",
    "                            last_probs = {f'state_{i}': probs[-1, i] for i in range(model.n_components)}\n",
    "                            probs_log[key] = last_probs\n",
    "                            \n",
    "                        logging.info(f\"Predicción exitosa con modelo {key}\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error prediciendo con modelo {key}: {str(e)}\")\n",
    "                        logging.error(f\"Stack trace: {traceback.format_exc()}\")\n",
    "            \n",
    "            if not predictions:\n",
    "                logging.warning(\"No se pudieron hacer predicciones con ningún modelo\")\n",
    "                # Registrar información sobre los modelos y features\n",
    "                for key in self.hmm_models:\n",
    "                    logging.info(f\"Modelo {key} válido: {self.is_model_valid[key]}\")\n",
    "                    if key in features:\n",
    "                        if isinstance(features[key], pd.DataFrame):\n",
    "                            logging.info(f\"Features {key} disponibles: {not features[key].empty}\")\n",
    "                        else:\n",
    "                            logging.info(f\"Features {key} no es DataFrame: {type(features[key])}\")\n",
    "                \n",
    "                return pd.Series(2, index=[datetime.now()], name='regime')  # Régimen neutral por defecto\n",
    "            \n",
    "            # Alinear índices\n",
    "            all_indices = set()\n",
    "            for pred_df in predictions.values():\n",
    "                all_indices.update(pred_df.index)\n",
    "            all_indices = sorted(list(all_indices))\n",
    "            \n",
    "            for key in predictions:\n",
    "                predictions[key] = predictions[key].reindex(all_indices)\n",
    "            \n",
    "            # Combinar predicciones\n",
    "            combined_df = pd.DataFrame(index=all_indices)\n",
    "            \n",
    "            if 'hmm1' in predictions:\n",
    "                combined_df['hmm1_regime'] = predictions['hmm1']['state']\n",
    "            if 'hmm2' in predictions:\n",
    "                combined_df['hmm2_regime'] = predictions['hmm2']['state']\n",
    "            if 'hmm3' in predictions:\n",
    "                combined_df['hmm3_regime'] = predictions['hmm3']['state']\n",
    "            \n",
    "            # Mapear a régimen final (3 regímenes)\n",
    "            # Definir una función más segura que devuelva valores escalares\n",
    "            def map_regime(row):\n",
    "                try:\n",
    "                    hmm1 = row.get('hmm1_regime', 0)\n",
    "                    if pd.isna(hmm1):\n",
    "                        hmm1 = 0\n",
    "                    elif isinstance(hmm1, (list, np.ndarray)):\n",
    "                        hmm1 = hmm1[0] if len(hmm1) > 0 else 0\n",
    "                    hmm1 = int(hmm1)\n",
    "                    \n",
    "                    hmm2 = row.get('hmm2_regime', 1)\n",
    "                    if pd.isna(hmm2):\n",
    "                        hmm2 = 1\n",
    "                    elif isinstance(hmm2, (list, np.ndarray)):\n",
    "                        hmm2 = hmm2[0] if len(hmm2) > 0 else 1\n",
    "                    hmm2 = int(hmm2)\n",
    "                    \n",
    "                    hmm3 = row.get('hmm3_regime', 0)\n",
    "                    if pd.isna(hmm3):\n",
    "                        hmm3 = 0\n",
    "                    elif isinstance(hmm3, (list, np.ndarray)):\n",
    "                        hmm3 = hmm3[0] if len(hmm3) > 0 else 0\n",
    "                    hmm3 = int(hmm3)\n",
    "                    \n",
    "                    # Régimen 3 (Crisis): Alta volatilidad\n",
    "                    if hmm1 == 1 and hmm2 == 2:\n",
    "                        return 3\n",
    "                    # Régimen 1 (Favorable): Baja volatilidad y baja dispersión\n",
    "                    elif hmm1 == 0 and hmm3 == 1:\n",
    "                        return 1\n",
    "                    # Régimen 2 (Transición): Otros casos\n",
    "                    else:\n",
    "                        return 2\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error en map_regime: {str(e)}\")\n",
    "                    return 2  # Régimen neutral por defecto en caso de error\n",
    "            \n",
    "            # Aplicar la función de mapeo fila por fila para evitar problemas\n",
    "            final_regimes = []\n",
    "            for idx in combined_df.index:\n",
    "                row_dict = {}\n",
    "                for col in combined_df.columns:\n",
    "                    row_dict[col] = combined_df.loc[idx, col]\n",
    "                final_regimes.append(map_regime(row_dict))\n",
    "            \n",
    "            combined_df['final_regime'] = final_regimes\n",
    "            \n",
    "            # Aplicar filtro de persistencia\n",
    "            if apply_persistence and len(combined_df) > self.persistence_days:\n",
    "                filtered_regimes = combined_df['final_regime'].copy()\n",
    "                \n",
    "                for i in range(self.persistence_days, len(filtered_regimes)):\n",
    "                    window = combined_df['final_regime'].iloc[i-self.persistence_days:i]\n",
    "                    counts = window.value_counts()\n",
    "                    if len(counts) > 0:  # Asegurar que hay valores\n",
    "                        most_common = counts.idxmax()\n",
    "                        if (window == most_common).mean() >= self.prob_threshold:\n",
    "                            filtered_regimes.iloc[i] = most_common\n",
    "                        else:\n",
    "                            filtered_regimes.iloc[i] = filtered_regimes.iloc[i-1]\n",
    "                \n",
    "                combined_df['filtered_regime'] = filtered_regimes\n",
    "                regime_series = combined_df['filtered_regime']\n",
    "            else:\n",
    "                regime_series = combined_df['final_regime']\n",
    "            \n",
    "            # Retrasar 2 días para evitar look-ahead bias\n",
    "            if len(regime_series) > 2:\n",
    "                regime_series = regime_series.shift(2).fillna(method='bfill')\n",
    "            \n",
    "            self.regime_history = regime_series\n",
    "            if not regime_series.empty:\n",
    "                self.current_regime = int(regime_series.iloc[-1])\n",
    "                # Log detallado del régimen actual y sus causas\n",
    "                logging.info(f\"Régimen actual: {self.current_regime}\")\n",
    "                logging.info(f\"Probabilidades de los modelos: {probs_log}\")\n",
    "                \n",
    "                # Explicar factores que llevaron a este régimen\n",
    "                if self.current_regime == 1:\n",
    "                    logging.info(\"Régimen 1 (Favorable): Baja volatilidad, alta predictibilidad\")\n",
    "                elif self.current_regime == 2:\n",
    "                    logging.info(\"Régimen 2 (Transición): Volatilidad moderada, señales mixtas\")\n",
    "                else:\n",
    "                    logging.info(\"Régimen 3 (Crisis): Alta volatilidad, baja predictibilidad\")\n",
    "            else:\n",
    "                self.current_regime = 2  # Régimen neutral por defecto\n",
    "                logging.warning(\"No se pudo determinar régimen, usando valor por defecto (2)\")\n",
    "            \n",
    "            return regime_series\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error general en predict_regimes: {str(e)}\")\n",
    "            logging.error(f\"Stack trace: {traceback.format_exc()}\")\n",
    "            return pd.Series(2, index=[datetime.now()], name='regime')  # Régimen neutral por defecto\n",
    "    \n",
    "    def fit_predict(self, data, sector_map, current_date=None, force_calibration=False):\n",
    "        \"\"\"Entrena y predice el régimen actual\"\"\"\n",
    "        try:\n",
    "            self.sector_map = sector_map\n",
    "            \n",
    "            if current_date is None:\n",
    "                current_date = data['prices'].index[-1]\n",
    "            \n",
    "            # Verificar datos suficientes\n",
    "            if len(data['prices']) < self.min_train_samples:\n",
    "                logging.warning(f\"Datos insuficientes: {len(data['prices'])} < {self.min_train_samples}\")\n",
    "                return 2  # Régimen neutral por defecto con datos insuficientes\n",
    "            \n",
    "            # Crear features con ventanas adaptativas\n",
    "            lookback = min(252 * 5, len(data['prices']))\n",
    "            features = self.create_features(data, lookback=lookback)\n",
    "            \n",
    "            # Verificar si hay features para entrenar\n",
    "            if not features:\n",
    "                logging.warning(\"No hay features para entrenar.\")\n",
    "                return 2  # Régimen neutral por defecto\n",
    "                \n",
    "            # Corregir la verificación de features vacías\n",
    "            has_valid_features = False\n",
    "            for key, value in features.items():\n",
    "                if key != 'error' and isinstance(value, pd.DataFrame) and not value.empty:\n",
    "                    has_valid_features = True\n",
    "                    break\n",
    "                    \n",
    "            if not has_valid_features:\n",
    "                logging.warning(\"No hay suficientes datos para crear features.\")\n",
    "                return 2  # Régimen neutral por defecto\n",
    "            \n",
    "            # Entrenar modelos si es necesario\n",
    "            need_training = (force_calibration or \n",
    "                             all(model is None for model in self.hmm_models.values()) or \n",
    "                             not any(self.is_model_valid.values()) or\n",
    "                             (self.last_calibration and \n",
    "                              (current_date.date() - self.last_calibration.date()).days > 7))\n",
    "            \n",
    "            if need_training:\n",
    "                logging.info(\"Iniciando entrenamiento de modelos HMM\")\n",
    "                self.train_models(features)\n",
    "            \n",
    "            # Predecir regímenes\n",
    "            regimes = self.predict_regimes(features)\n",
    "            \n",
    "            # Devolver régimen actual\n",
    "            if not regimes.empty:\n",
    "                current_regime = regimes.iloc[-1]\n",
    "                if pd.isna(current_regime):  # Si es NaN, usar valor neutral\n",
    "                    logging.warning(\"Régimen actual es NaN, usando valor neutral (2)\")\n",
    "                    current_regime = 2\n",
    "                \n",
    "                logging.info(f\"Régimen detectado: {int(current_regime)}\")\n",
    "                return int(current_regime)\n",
    "            else:\n",
    "                logging.warning(\"No se pudo determinar régimen, usando valor neutral (2)\")\n",
    "                return 2  # Régimen neutral por defecto\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en fit_predict: {str(e)}\")\n",
    "            logging.error(f\"Stack trace: {traceback.format_exc()}\")\n",
    "            return 2  # Régimen neutral por defecto en caso de error\n",
    "            \n",
    "# Componente de Selección de Pares\n",
    "class PairSelector:\n",
    "    \"\"\"Sistema mejorado de identificación y selección de pares para arbitraje\"\"\"\n",
    "    \n",
    "    def __init__(self, min_liquidity=10e6, \n",
    "                 max_pairs_by_regime={1: 25, 2: 20, 3: 15},\n",
    "                 recalibration_days=5,\n",
    "                 similarity_threshold=0.7):\n",
    "        self.min_liquidity = min_liquidity\n",
    "        self.max_pairs_by_regime = max_pairs_by_regime\n",
    "        self.recalibration_days = recalibration_days\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.candidate_pairs = None\n",
    "        self.selected_pairs = None\n",
    "        self.last_calibration_date = None\n",
    "        self.coint_history = {}  # Historial de cointegración para seguimiento\n",
    "        self.market_volatility = None  # Para tracking de volatilidad del mercado\n",
    "    \n",
    "    def prefilter_by_liquidity(self, data, min_adv=None):\n",
    "        \"\"\"Filtro de tickers por liquidez mínima con verificación robusta\"\"\"\n",
    "        if min_adv is None:\n",
    "            min_adv = self.min_liquidity\n",
    "        \n",
    "        try:    \n",
    "            # Verificar que tenemos datos adecuados\n",
    "            if 'adv' not in data or data['adv'].empty:\n",
    "                logging.warning(\"Datos ADV no disponibles para filtro de liquidez\")\n",
    "                # Alternativa usando volumen y precio si están disponibles\n",
    "                if 'volumes' in data and 'prices' in data and not data['volumes'].empty and not data['prices'].empty:\n",
    "                    logging.info(\"Usando volumen y precio como proxy para ADV\")\n",
    "                    # Calcular ADV como volumen * precio\n",
    "                    vol = data['volumes'].iloc[-20:].mean()\n",
    "                    price = data['prices'].iloc[-1]\n",
    "                    adv = vol * price\n",
    "                else:\n",
    "                    logging.warning(\"No se pueden calcular métricas de liquidez, retornando todos los tickers\")\n",
    "                    return list(data['prices'].columns)\n",
    "            else:\n",
    "                # Usar ADV promedio de las últimas 20 sesiones si hay suficientes datos\n",
    "                window = min(20, max(5, len(data['adv'])))\n",
    "                adv = data['adv'].iloc[-window:].mean()\n",
    "            \n",
    "            # Aplicar filtro con verificación\n",
    "            liquid_tickers = [ticker for ticker in adv.index if adv.get(ticker, 0) > min_adv]\n",
    "            \n",
    "            logging.info(f\"Filtro de liquidez: {len(liquid_tickers)}/{len(adv)} tickers pasan el umbral de ${min_adv/1e6:.1f}M\")\n",
    "            \n",
    "            return liquid_tickers\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en filtro de liquidez: {str(e)}\")\n",
    "            logging.error(traceback.format_exc())\n",
    "            # Retornar una lista segura en caso de error\n",
    "            return list(data['prices'].columns if 'prices' in data and not data['prices'].empty else [])\n",
    "    \n",
    "    def filter_by_events(self, tickers, data, days_ahead=7):\n",
    "        \"\"\"Filtro de tickers con eventos próximos (mejorado para evitar aleatorios)\"\"\"\n",
    "        # En implementación real, usaríamos datos de calendarios de eventos\n",
    "        try:\n",
    "            # En lugar de valores aleatorios, usar criterios basados en datos\n",
    "            # Por ejemplo, filtrar tickers con volatilidad anormalmente alta\n",
    "            if 'realized_vol' in data and not data['realized_vol'].empty:\n",
    "                vol = data['realized_vol'].iloc[-5:].mean()  # Últimos 5 días\n",
    "                vol_percentile = vol.rank(pct=True)  # Percentil de volatilidad\n",
    "                \n",
    "                # Filtrar tickers en el 10% superior de volatilidad como proxy de eventos\n",
    "                high_vol_tickers = [t for t in vol_percentile.index if t in tickers and vol_percentile.get(t, 0) > 0.9]\n",
    "                \n",
    "                logging.info(f\"Filtro de eventos: {len(high_vol_tickers)} tickers filtrados por volatilidad alta\")\n",
    "                \n",
    "                return [t for t in tickers if t not in high_vol_tickers]\n",
    "            else:\n",
    "                logging.warning(\"Datos de volatilidad no disponibles para filtro de eventos\")\n",
    "                return tickers\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en filtro de eventos: {str(e)}\")\n",
    "            logging.error(traceback.format_exc())\n",
    "            return tickers  # Retornar la lista original en caso de error\n",
    "    \n",
    "    def calculate_similarity_score(self, data, tickers, lookback=252):\n",
    "        \"\"\"Calcula scores de similitud entre tickers usando métricas reales en lugar de aleatorios\"\"\"\n",
    "        try:\n",
    "            # Adaptar lookback a los datos disponibles\n",
    "            lookback = min(lookback, len(data['prices']) - 1)\n",
    "            if lookback < 60:\n",
    "                logging.warning(f\"Datos insuficientes para similitud: {lookback} < 60 días\")\n",
    "                lookback = max(lookback, 20)  # Mínimo 20 días para análisis\n",
    "            \n",
    "            prices = data['prices'].iloc[-lookback:][tickers]\n",
    "            returns = data['returns'].iloc[-lookback:][tickers]\n",
    "            vols = data['realized_vol'].iloc[-lookback:][tickers]\n",
    "            \n",
    "            similarity = pd.DataFrame(0, index=tickers, columns=tickers)\n",
    "            \n",
    "            # Betas a mercado (en lugar de correlación entre pares)\n",
    "            market_returns = returns.mean(axis=1)\n",
    "            betas = {}\n",
    "            \n",
    "            for ticker in tickers:\n",
    "                X = market_returns.values.reshape(-1, 1)\n",
    "                y = returns[ticker].values\n",
    "                mask = ~np.isnan(X.flatten()) & ~np.isnan(y)\n",
    "                if mask.sum() > 30:\n",
    "                    X, y = X[mask], y[mask]\n",
    "                    beta = np.cov(y, X.flatten())[0, 1] / np.var(X.flatten())\n",
    "                    betas[ticker] = beta\n",
    "                else:\n",
    "                    betas[ticker] = 1.0\n",
    "            \n",
    "            # Proxy de capitalización: precio * volumen\n",
    "            mcap_proxy = pd.DataFrame(index=tickers)\n",
    "            \n",
    "            if 'volumes' in data and not data['volumes'].empty:\n",
    "                vol_avg = data['volumes'].iloc[-20:][tickers].mean()\n",
    "                price_last = prices.iloc[-1]\n",
    "                mcap_proxy = price_last * vol_avg\n",
    "                # Normalizar\n",
    "                if not mcap_proxy.empty and mcap_proxy.max() > mcap_proxy.min():\n",
    "                    mcap_proxy = (mcap_proxy - mcap_proxy.min()) / (mcap_proxy.max() - mcap_proxy.min())\n",
    "                else:\n",
    "                    mcap_proxy = pd.Series(0.5, index=tickers)\n",
    "            else:\n",
    "                # Capitalización (proxy: precio)\n",
    "                mcap = prices.iloc[-1]\n",
    "                if mcap.max() > mcap.min():\n",
    "                    mcap_proxy = (mcap - mcap.min()) / (mcap.max() - mcap.min())\n",
    "                else:\n",
    "                    mcap_proxy = pd.Series(0.5, index=tickers)\n",
    "            \n",
    "            # Volatilidad realizada\n",
    "            vol_window = min(60, lookback//2)\n",
    "            vol = vols.iloc[-vol_window:].mean()\n",
    "            if vol.max() > vol.min():\n",
    "                vol_norm = (vol - vol.min()) / (vol.max() - vol.min())\n",
    "            else:\n",
    "                vol_norm = pd.Series(0.5, index=tickers)\n",
    "            \n",
    "            # Ratios financieros reales en lugar de aleatorios (usar datos disponibles)\n",
    "            # En este caso, usamos el ratio precio/volumen como proxy de valoración\n",
    "            valuation_proxy = pd.Series(index=tickers)\n",
    "            try:\n",
    "                if 'volumes' in data and not data['volumes'].empty:\n",
    "                    price_avg = prices.iloc[-20:].mean()\n",
    "                    vol_avg = data['volumes'].iloc[-20:][tickers].mean()\n",
    "                    vol_avg = vol_avg.replace(0, np.nan)  # Evitar división por cero\n",
    "                    \n",
    "                    # Price/Volume ratio como proxy\n",
    "                    pv_ratio = price_avg / vol_avg\n",
    "                    pv_ratio = pv_ratio.fillna(pv_ratio.median())  # Manejar NaN\n",
    "                    \n",
    "                    # Normalizar\n",
    "                    if pv_ratio.max() > pv_ratio.min():\n",
    "                        valuation_proxy = (pv_ratio - pv_ratio.min()) / (pv_ratio.max() - pv_ratio.min())\n",
    "                    else:\n",
    "                        valuation_proxy = pd.Series(0.5, index=tickers)\n",
    "                else:\n",
    "                    valuation_proxy = pd.Series(0.5, index=tickers)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error calculando proxies de valoración: {str(e)}\")\n",
    "                valuation_proxy = pd.Series(0.5, index=tickers)\n",
    "            \n",
    "            # Registrar volatilidad de mercado para posible recalibración\n",
    "            self.market_volatility = vols.mean().mean()\n",
    "            \n",
    "            # Calcular similitud\n",
    "            for i, ticker1 in enumerate(tickers):\n",
    "                for j, ticker2 in enumerate(tickers):\n",
    "                    if i < j:\n",
    "                        # Beta similarity (30%)\n",
    "                        beta_sim = 1 - min(abs(betas.get(ticker1, 1) - betas.get(ticker2, 1)) / 2, 1)\n",
    "                        \n",
    "                        # Market cap similarity (25%)\n",
    "                        mcap_sim = 1 - abs(mcap_proxy.get(ticker1, 0.5) - mcap_proxy.get(ticker2, 0.5))\n",
    "                        \n",
    "                        # Volatility similarity (25%)\n",
    "                        vol_sim = 1 - abs(vol_norm.get(ticker1, 0.5) - vol_norm.get(ticker2, 0.5))\n",
    "                        \n",
    "                        # Valuation similarity (20%)\n",
    "                        val_sim = 1 - abs(valuation_proxy.get(ticker1, 0.5) - valuation_proxy.get(ticker2, 0.5))\n",
    "                        \n",
    "                        # Weighted score\n",
    "                        sim_score = 0.3*beta_sim + 0.25*mcap_sim + 0.25*vol_sim + 0.2*val_sim\n",
    "                        similarity.loc[ticker1, ticker2] = sim_score\n",
    "                        similarity.loc[ticker2, ticker1] = sim_score\n",
    "            \n",
    "            return similarity\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en calculate_similarity_score: {str(e)}\")\n",
    "            logging.error(traceback.format_exc())\n",
    "            # Retornar matriz de similitud vacía en caso de error\n",
    "            return pd.DataFrame(0.5, index=tickers, columns=tickers)\n",
    "    \n",
    "    def generate_candidate_pairs(self, data, sector_map, subsector_map, similarity_threshold=None):\n",
    "        \"\"\"Genera pares candidatos basados en sector/subsector y similitud con verificaciones robustas\"\"\"\n",
    "        try:\n",
    "            if similarity_threshold is None:\n",
    "                similarity_threshold = self.similarity_threshold\n",
    "                \n",
    "            # Filtrar por liquidez y eventos\n",
    "            liquid_tickers = self.prefilter_by_liquidity(data)\n",
    "            filtered_tickers = self.filter_by_events(liquid_tickers, data)\n",
    "            \n",
    "            # Verificar tickers suficientes\n",
    "            if len(filtered_tickers) < 2:\n",
    "                logging.warning(f\"Tickers insuficientes después de filtro: {len(filtered_tickers)}\")\n",
    "                return []\n",
    "            \n",
    "            # Calcular scores de similitud\n",
    "            similarity = self.calculate_similarity_score(data, filtered_tickers)\n",
    "            \n",
    "            # Generar pares candidatos\n",
    "            candidate_pairs = []\n",
    "            \n",
    "            # Agrupar por sector y subsector\n",
    "            sector_groups = {}\n",
    "            subsector_groups = {}\n",
    "            \n",
    "            for ticker in filtered_tickers:\n",
    "                sector = sector_map.get(ticker)\n",
    "                subsector = subsector_map.get(ticker)\n",
    "                \n",
    "                if sector:\n",
    "                    if sector not in sector_groups:\n",
    "                        sector_groups[sector] = []\n",
    "                    sector_groups[sector].append(ticker)\n",
    "                \n",
    "                if subsector:\n",
    "                    if subsector not in subsector_groups:\n",
    "                        subsector_groups[subsector] = []\n",
    "                    subsector_groups[subsector].append(ticker)\n",
    "            \n",
    "            # Buscar pares en mismo subsector\n",
    "            subsector_pairs = []\n",
    "            for subsector, tickers in subsector_groups.items():\n",
    "                if len(tickers) < 2:\n",
    "                    continue\n",
    "                    \n",
    "                for i, ticker1 in enumerate(tickers):\n",
    "                    for ticker2 in tickers[i+1:]:\n",
    "                        if ticker1 in similarity.index and ticker2 in similarity.columns:\n",
    "                            if similarity.loc[ticker1, ticker2] >= similarity_threshold:\n",
    "                                subsector_pairs.append((ticker1, ticker2))\n",
    "            \n",
    "            candidate_pairs.extend(subsector_pairs)\n",
    "            logging.info(f\"Pares candidatos del mismo subsector: {len(subsector_pairs)}\")\n",
    "            \n",
    "            # Buscar en mismo sector si necesitamos más pares\n",
    "            if len(candidate_pairs) < 50:\n",
    "                sector_pairs = []\n",
    "                for sector, tickers in sector_groups.items():\n",
    "                    if len(tickers) < 2:\n",
    "                        continue\n",
    "                        \n",
    "                    for i, ticker1 in enumerate(tickers):\n",
    "                        for ticker2 in tickers[i+1:]:\n",
    "                            if (ticker1, ticker2) not in candidate_pairs and (ticker2, ticker1) not in candidate_pairs:\n",
    "                                if ticker1 in similarity.index and ticker2 in similarity.columns:\n",
    "                                    if similarity.loc[ticker1, ticker2] >= similarity_threshold * 0.9:\n",
    "                                        sector_pairs.append((ticker1, ticker2))\n",
    "                \n",
    "                candidate_pairs.extend(sector_pairs)\n",
    "                logging.info(f\"Pares candidatos adicionales del mismo sector: {len(sector_pairs)}\")\n",
    "            \n",
    "            self.candidate_pairs = candidate_pairs\n",
    "            logging.info(f\"Total de pares candidatos: {len(candidate_pairs)}\")\n",
    "            \n",
    "            # Actualizar fecha de calibración\n",
    "            self.last_calibration_date = data['prices'].index[-1] if not data['prices'].empty else datetime.now()\n",
    "            \n",
    "            return candidate_pairs\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en generate_candidate_pairs: {str(e)}\")\n",
    "            logging.error(traceback.format_exc())\n",
    "            self.candidate_pairs = []\n",
    "            return []\n",
    "    \n",
    "    def test_cointegration(self, data, pair, window, significance=0.05):\n",
    "        \"\"\"Prueba de cointegración de Johansen para un par con manejo robusto de errores\"\"\"\n",
    "        ticker1, ticker2 = pair\n",
    "        \n",
    "        try:\n",
    "            # Importar la función coint correctamente\n",
    "            from statsmodels.tsa.stattools import coint as statsmodels_coint\n",
    "            \n",
    "            # Verificar datos suficientes\n",
    "            if window > len(data['prices']):\n",
    "                logging.warning(f\"Ventana solicitada {window} mayor que datos disponibles {len(data['prices'])}\")\n",
    "                window = min(window, len(data['prices']))\n",
    "            \n",
    "            prices = data['prices'].iloc[-window:][list(pair)]\n",
    "            \n",
    "            # Verificar datos suficientes y sin valores faltantes\n",
    "            if len(prices) < 60 or prices[ticker1].isna().any() or prices[ticker2].isna().any():\n",
    "                logging.info(f\"Datos insuficientes para prueba de cointegración de {ticker1}-{ticker2}: {len(prices)} puntos\")\n",
    "                return {'coint': False, 'pvalue': 1.0, 'half_life': np.inf, 'hedge_ratio': 1.0}\n",
    "            \n",
    "            # Test de Johansen\n",
    "            result = coint_johansen(prices, det_order=0, k_ar_diff=1)\n",
    "            trace_stat = result.lr1[0]  # Estadístico de traza para r=0\n",
    "            crit_value = result.cvt[0, 1]  # Valor crítico al 5%\n",
    "            \n",
    "            is_cointegrated = trace_stat > crit_value\n",
    "            \n",
    "            # También probar con test Engle-Granger para validación\n",
    "            eg_result = statsmodels_coint(prices[ticker1], prices[ticker2], maxlag=10, autolag='AIC')\n",
    "            eg_coint = eg_result[1] < significance\n",
    "            \n",
    "            # Considerar cointegrado solo si ambos tests lo confirman\n",
    "            is_cointegrated = is_cointegrated and eg_coint\n",
    "            \n",
    "            if is_cointegrated:\n",
    "                # Vector de cointegración normalizado\n",
    "                coef = result.evec[:, 0]\n",
    "                hedge_ratio = -coef[1] / coef[0]\n",
    "                \n",
    "                # Verificar hedge ratio razonable\n",
    "                if abs(hedge_ratio) > 10:\n",
    "                    logging.warning(f\"Hedge ratio extremo: {hedge_ratio} para {ticker1}-{ticker2}\")\n",
    "                    hedge_ratio = np.sign(hedge_ratio) * min(abs(hedge_ratio), 10)\n",
    "                \n",
    "                # Calcular spread\n",
    "                spread = prices[ticker1] + hedge_ratio * prices[ticker2]\n",
    "                \n",
    "                # Estimar half-life\n",
    "                lagged_spread = spread.shift(1).dropna()\n",
    "                delta_spread = spread.diff().dropna()\n",
    "                \n",
    "                if len(lagged_spread) > 30:\n",
    "                    # Regresión para modelo AR(1)\n",
    "                    X = lagged_spread.values.reshape(-1, 1)\n",
    "                    y = delta_spread.values\n",
    "                    X = np.hstack([np.ones_like(X), X])\n",
    "                    \n",
    "                    try:\n",
    "                        beta = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "                        \n",
    "                        # Verificar coeficiente válido para half-life\n",
    "                        if beta[1] < 0:\n",
    "                            half_life = -np.log(2) / beta[1]\n",
    "                            \n",
    "                            # Limitar rango de half-life razonable\n",
    "                            if half_life <= 0 or half_life > 252:\n",
    "                                logging.info(f\"Half-life fuera de rango: {half_life}\")\n",
    "                                is_cointegrated = False\n",
    "                                half_life = np.inf\n",
    "                        else:\n",
    "                            logging.info(f\"Coeficiente no negativo: {beta[1]}\")\n",
    "                            is_cointegrated = False\n",
    "                            half_life = np.inf\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error calculando half-life: {str(e)}\")\n",
    "                        half_life = np.inf\n",
    "                        is_cointegrated = False\n",
    "                else:\n",
    "                    half_life = np.inf\n",
    "                    is_cointegrated = False\n",
    "            else:\n",
    "                hedge_ratio = 1.0\n",
    "                half_life = np.inf\n",
    "            \n",
    "            # P-valor aproximado\n",
    "            p_value = 0.01 if trace_stat > result.cvt[0, 0] else 0.05 if trace_stat > result.cvt[0, 1] else 0.1 if trace_stat > result.cvt[0, 2] else 1.0\n",
    "            \n",
    "            # Guardar en historial para seguimiento\n",
    "            pair_id = f\"{ticker1}_{ticker2}\"\n",
    "            if pair_id not in self.coint_history:\n",
    "                self.coint_history[pair_id] = []\n",
    "            \n",
    "            self.coint_history[pair_id].append({\n",
    "                'date': data['prices'].index[-1],\n",
    "                'coint': is_cointegrated,\n",
    "                'pvalue': p_value,\n",
    "                'half_life': half_life,\n",
    "                'hedge_ratio': hedge_ratio\n",
    "            })\n",
    "            \n",
    "            # Mantener solo los últimos 10 tests para cada par\n",
    "            if len(self.coint_history[pair_id]) > 10:\n",
    "                self.coint_history[pair_id] = self.coint_history[pair_id][-10:]\n",
    "            \n",
    "            return {\n",
    "                'coint': is_cointegrated,\n",
    "                'pvalue': p_value,\n",
    "                'half_life': half_life,\n",
    "                'hedge_ratio': hedge_ratio\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en test de cointegración para {pair}: {str(e)}\")\n",
    "            logging.error(traceback.format_exc())\n",
    "            return {'coint': False, 'pvalue': 1.0, 'half_life': np.inf, 'hedge_ratio': 1.0}\n",
    "    \n",
    "    def test_structural_stability(self, data, pair, hedge_ratio, lookback=252):\n",
    "        \"\"\"Evalúa la estabilidad estructural de la relación con diagnóstico mejorado\"\"\"\n",
    "        ticker1, ticker2 = pair\n",
    "        \n",
    "        try:\n",
    "            # Verificar datos suficientes\n",
    "            lookback = min(lookback, len(data['prices']))\n",
    "            prices = data['prices'].iloc[-lookback:][list(pair)]\n",
    "            \n",
    "            if len(prices) < 60 or prices[ticker1].isna().any() or prices[ticker2].isna().any():\n",
    "                logging.info(f\"Datos insuficientes para estabilidad de {ticker1}-{ticker2}: {len(prices)} puntos\")\n",
    "                return 0.5  # Estabilidad media por defecto\n",
    "            \n",
    "            # Calcular spread histórico\n",
    "            spread = prices[ticker1] + hedge_ratio * prices[ticker2]\n",
    "            \n",
    "            # Evaluar estabilidad con ventanas móviles adaptativas\n",
    "            stability_scores = []\n",
    "            \n",
    "            # Adaptar número de sub-periodos según datos disponibles\n",
    "            n_periods = 4 if len(spread) >= 240 else 3 if len(spread) >= 120 else 2\n",
    "            subperiod_length = len(spread) // n_periods\n",
    "            \n",
    "            if subperiod_length < 30:\n",
    "                logging.warning(f\"Subperiodos muy cortos: {subperiod_length} < 30 días\")\n",
    "                n_periods = max(1, len(spread) // 30)\n",
    "                subperiod_length = len(spread) // max(n_periods, 1)\n",
    "            \n",
    "            logging.info(f\"Evaluando estabilidad en {n_periods} subperiodos de {subperiod_length} días\")\n",
    "            \n",
    "            for i in range(n_periods):\n",
    "                start_idx = i * subperiod_length\n",
    "                end_idx = (i+1) * subperiod_length if i < n_periods-1 else len(spread)\n",
    "                subspread = spread.iloc[start_idx:end_idx]\n",
    "                    \n",
    "                if len(subspread) < 30:\n",
    "                    logging.info(f\"Subperiodo {i+1} insuficiente: {len(subspread)} < 30\")\n",
    "                    continue\n",
    "                    \n",
    "                # Test ADF con manejo de errores\n",
    "                try:\n",
    "                    adf_result = adfuller(subspread, maxlag=min(10, len(subspread)//5), autolag='AIC')\n",
    "                    is_stationary = adf_result[1] < 0.05\n",
    "                    stability_scores.append(1 if is_stationary else 0)\n",
    "                    logging.info(f\"Subperiodo {i+1}: ADF p-valor={adf_result[1]:.4f}, estacionario={is_stationary}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error en ADF para subperiodo {i+1}: {str(e)}\")\n",
    "                    stability_scores.append(0)\n",
    "            \n",
    "            # Verificar consistencia de estacionariedad\n",
    "            stationarity_consistency = sum(stability_scores) / len(stability_scores) if stability_scores else 0\n",
    "            logging.info(f\"Consistencia de estacionariedad: {stationarity_consistency:.2f}\")\n",
    "            \n",
    "            # Calcular volatilidad del spread en cada sub-periodo\n",
    "            subperiod_vols = []\n",
    "            \n",
    "            for i in range(n_periods):\n",
    "                start_idx = i * subperiod_length\n",
    "                end_idx = (i+1) * subperiod_length if i < n_periods-1 else len(spread)\n",
    "                subspread = spread.iloc[start_idx:end_idx]\n",
    "                    \n",
    "                if len(subspread) < 10:\n",
    "                    continue\n",
    "                    \n",
    "                subperiod_vols.append(subspread.std())\n",
    "            \n",
    "            # Consistencia de volatilidad\n",
    "            vol_consistency = 1.0\n",
    "            if len(subperiod_vols) > 1:\n",
    "                vol_ratio = max(subperiod_vols) / min(subperiod_vols)\n",
    "                vol_consistency = 1.0 / min(vol_ratio, 5.0)\n",
    "                logging.info(f\"Consistencia de volatilidad: {vol_consistency:.2f} (ratio={vol_ratio:.2f})\")\n",
    "            \n",
    "            # Puntuación final (0-10)\n",
    "            stability_score = (0.7 * stationarity_consistency + 0.3 * vol_consistency) * 10\n",
    "            \n",
    "            return stability_score\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en test_structural_stability para {pair}: {str(e)}\")\n",
    "            logging.error(traceback.format_exc())\n",
    "            return 5.0  # Valor medio en caso de error\n",
    "    \n",
    "    def check_recalibration_needed(self, current_date, regime, prev_regime, market_conditions=None):\n",
    "        \"\"\"Determina si se requiere recalibración basado en múltiples factores\"\"\"\n",
    "        try:\n",
    "            # Verificar si es la primera calibración\n",
    "            if self.last_calibration_date is None:\n",
    "                logging.info(\"Recalibración necesaria: primera calibración\")\n",
    "                return True\n",
    "                \n",
    "            # 1. Calibración por tiempo transcurrido\n",
    "            days_since_calib = (current_date - self.last_calibration_date).days\n",
    "            if days_since_calib >= self.recalibration_days:\n",
    "                logging.info(f\"Recalibración por tiempo: {days_since_calib} días desde última calibración\")\n",
    "                return True\n",
    "                \n",
    "            # 2. Calibración por cambio de régimen\n",
    "            if prev_regime != regime:\n",
    "                logging.info(f\"Recalibración por cambio de régimen: {prev_regime} → {regime}\")\n",
    "                return True\n",
    "                \n",
    "            # 3. Calibración por aumento de volatilidad\n",
    "            if market_conditions and 'volatility_increase' in market_conditions:\n",
    "                vol_increase = market_conditions['volatility_increase']\n",
    "                if vol_increase > 0.3:  # 30% de aumento en volatilidad\n",
    "                    logging.info(f\"Recalibración por aumento de volatilidad: +{vol_increase*100:.1f}%\")\n",
    "                    return True\n",
    "                    \n",
    "            # 4. Calibración por día de la semana (menos prioritario)\n",
    "            is_monday = current_date.weekday() == 0\n",
    "            if is_monday and days_since_calib >= 3:  # Solo recalibrar lunes si han pasado al menos 3 días\n",
    "                logging.info(\"Recalibración programada (lunes)\")\n",
    "                return True\n",
    "                \n",
    "            logging.info(\"No se requiere recalibración\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en check_recalibration_needed: {str(e)}\")\n",
    "            return True  # En caso de error, mejor recalibrar\n",
    "    \n",
    "    def select_pairs(self, data, regime, candidate_pairs=None, market_conditions=None):\n",
    "        \"\"\"Selecciona los mejores pares para operar según el régimen actual con filtros mejorados\"\"\"\n",
    "        try:\n",
    "            if candidate_pairs is None:\n",
    "                candidate_pairs = self.candidate_pairs\n",
    "                \n",
    "            if not candidate_pairs:\n",
    "                logging.warning(\"No hay pares candidatos para selección\")\n",
    "                return []\n",
    "            \n",
    "            # Verificar pares ya seleccionados para evaluar su continua validez\n",
    "            existing_pair_ids = set()\n",
    "            if self.selected_pairs:\n",
    "                existing_pair_ids = {f\"{p['ticker1']}_{p['ticker2']}\" for p in self.selected_pairs}\n",
    "            \n",
    "            # Parámetros según régimen\n",
    "            if regime == 1:  # Baja volatilidad / Alta predictibilidad\n",
    "                window = 252\n",
    "                half_life_range = (10, 25)\n",
    "                max_pvalue = 0.01\n",
    "                max_pairs = self.max_pairs_by_regime.get(1, 25)\n",
    "                min_stability = 6.0\n",
    "            elif regime == 2:  # Transición / Volatilidad moderada\n",
    "                window = 180\n",
    "                half_life_range = (7, 20)\n",
    "                max_pvalue = 0.03\n",
    "                max_pairs = self.max_pairs_by_regime.get(2, 20)\n",
    "                min_stability = 5.5\n",
    "            else:  # Crisis / Alta volatilidad\n",
    "                window = 126\n",
    "                half_life_range = (5, 15)\n",
    "                max_pvalue = 0.05\n",
    "                max_pairs = self.max_pairs_by_regime.get(3, 15)\n",
    "                min_stability = 5.0\n",
    "            \n",
    "            # Ajustar ventana a datos disponibles\n",
    "            window = min(window, len(data['prices']))\n",
    "            if window < 60:\n",
    "                logging.warning(f\"Ventana ajustada a {window} días por datos insuficientes\")\n",
    "            \n",
    "            # Evaluar pares candidatos\n",
    "            pair_results = []\n",
    "            \n",
    "            for pair in candidate_pairs:\n",
    "                ticker1, ticker2 = pair\n",
    "                pair_id = f\"{ticker1}_{ticker2}\"\n",
    "                \n",
    "                # Test de cointegración\n",
    "                coint_result = self.test_cointegration(data, pair, window, max_pvalue)\n",
    "                \n",
    "                if not coint_result['coint']:\n",
    "                    # Verificar si era un par seleccionado anteriormente\n",
    "                    if pair_id in existing_pair_ids:\n",
    "                        logging.info(f\"Par {pair_id} perdió cointegración\")\n",
    "                    continue\n",
    "                    \n",
    "                if not half_life_range[0] <= coint_result['half_life'] <= half_life_range[1]:\n",
    "                    logging.info(f\"Par {pair_id} con half-life fuera de rango: {coint_result['half_life']:.2f}\")\n",
    "                    continue\n",
    "                    \n",
    "                # Evaluar estabilidad estructural\n",
    "                stability = self.test_structural_stability(data, pair, coint_result['hedge_ratio'])\n",
    "                \n",
    "                if stability < min_stability:\n",
    "                    logging.info(f\"Par {pair_id} con estabilidad insuficiente: {stability:.2f} < {min_stability}\")\n",
    "                    continue\n",
    "                \n",
    "                # Calcular liquidez combinada\n",
    "                ticker1_adv = data['adv'].iloc[-20:][ticker1].mean() if ticker1 in data['adv'].columns else 0\n",
    "                ticker2_adv = data['adv'].iloc[-20:][ticker2].mean() if ticker2 in data['adv'].columns else 0\n",
    "                combined_liquidity = min(ticker1_adv, ticker2_adv)\n",
    "                \n",
    "                # Rendimiento histórico (si está disponible en el historial)\n",
    "                historical_performance = 1.0\n",
    "                if pair_id in self.coint_history and len(self.coint_history[pair_id]) > 1:\n",
    "                    # Calcular variabilidad del hedge ratio como indicador de estabilidad\n",
    "                    hedge_ratios = [item['hedge_ratio'] for item in self.coint_history[pair_id] \n",
    "                                   if not np.isinf(item['hedge_ratio'])]\n",
    "                    if hedge_ratios:\n",
    "                        hr_std = np.std(hedge_ratios)\n",
    "                        hr_mean = np.mean(hedge_ratios)\n",
    "                        hr_cv = hr_std / abs(hr_mean) if abs(hr_mean) > 0 else np.inf\n",
    "                        \n",
    "                        # Mayor estabilidad = mejor rendimiento histórico\n",
    "                        if hr_cv < 0.1:\n",
    "                            historical_performance = 1.5\n",
    "                        elif hr_cv < 0.2:\n",
    "                            historical_performance = 1.2\n",
    "                        elif hr_cv > 0.5:\n",
    "                            historical_performance = 0.7\n",
    "                \n",
    "                # Score compuesto según régimen\n",
    "                if regime == 1:\n",
    "                    composite_score = (0.35 * (1/coint_result['pvalue']) + \n",
    "                                     0.35 * stability/10 + \n",
    "                                     0.2 * (combined_liquidity/1e7) + \n",
    "                                     0.1 * historical_performance)\n",
    "                elif regime == 2:\n",
    "                    composite_score = (0.4 * (1/coint_result['pvalue']) + \n",
    "                                     0.3 * stability/10 + \n",
    "                                     0.2 * (combined_liquidity/1e7) + \n",
    "                                     0.1 * historical_performance)\n",
    "                else:\n",
    "                    composite_score = (0.4 * (1/coint_result['pvalue']) + \n",
    "                                     0.3 * stability/10 + \n",
    "                                     0.2 * (combined_liquidity/1e7) + \n",
    "                                     0.1 * historical_performance)\n",
    "                \n",
    "                # Guardar resultados\n",
    "                pair_results.append({\n",
    "                    'ticker1': ticker1,\n",
    "                    'ticker2': ticker2,\n",
    "                    'hedge_ratio': coint_result['hedge_ratio'],\n",
    "                    'half_life': coint_result['half_life'],\n",
    "                    'pvalue': coint_result['pvalue'],\n",
    "                    'stability': stability,\n",
    "                    'liquidity': combined_liquidity,\n",
    "                    'composite_score': composite_score\n",
    "                })\n",
    "            \n",
    "            # Ordenar por score compuesto\n",
    "            sorted_pairs = sorted(pair_results, key=lambda x: x['composite_score'], reverse=True)\n",
    "            \n",
    "            # Seleccionar mejores pares\n",
    "            selected_pairs = sorted_pairs[:max_pairs]\n",
    "            \n",
    "            # Log de resultados\n",
    "            logging.info(f\"Pares seleccionados: {len(selected_pairs)}/{len(pair_results)} candidatos evaluados\")\n",
    "            \n",
    "            self.selected_pairs = selected_pairs\n",
    "            return selected_pairs\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en select_pairs: {str(e)}\")\n",
    "            logging.error(traceback.format_exc())\n",
    "            # Mantener pares anteriores en caso de error\n",
    "            return self.selected_pairs if self.selected_pairs else []\n",
    "\n",
    "# Modelo Predictivo de Convergencia\n",
    "class ConvergencePredictor:\n",
    "    \"\"\"Modelo predictivo mejorado para la probabilidad de convergencia\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, max_depth=4, n_estimators=200):\n",
    "        self.models = {\n",
    "            1: GradientBoostingClassifier(learning_rate=learning_rate, max_depth=max_depth, \n",
    "                                          n_estimators=n_estimators, subsample=0.8,\n",
    "                                          random_state=42),\n",
    "            2: GradientBoostingClassifier(learning_rate=learning_rate, max_depth=max_depth, \n",
    "                                          n_estimators=n_estimators, subsample=0.8,\n",
    "                                          random_state=42),\n",
    "            3: GradientBoostingClassifier(learning_rate=learning_rate, max_depth=max_depth, \n",
    "                                          n_estimators=n_estimators, subsample=0.8,\n",
    "                                          random_state=42)\n",
    "        }\n",
    "        self.scalers = {1: StandardScaler(), 2: StandardScaler(), 3: StandardScaler()}\n",
    "        self.is_trained = {1: False, 2: False, 3: False}\n",
    "        self.feature_importance = {1: None, 2: None, 3: None}  # Para almacenar importancia de features\n",
    "        self.last_training_date = None\n",
    "        self.best_validation_scores = {1: 0, 2: 0, 3: 0}  # Para monitorear calidad del modelo\n",
    "    \n",
    "    def create_features(self, data, pair, hedge_ratio, regime, lookback=None):\n",
    "        \"\"\"Crea features para el modelo de predicción con ventanas adaptativas\"\"\"\n",
    "        ticker1, ticker2 = pair\n",
    "        \n",
    "        try:\n",
    "            # Verificar que los tickers existen en los datos\n",
    "            if ticker1 not in data['prices'].columns or ticker2 not in data['prices'].columns:\n",
    "                logging.error(f\"Tickers {ticker1} o {ticker2} no encontrados en los datos\")\n",
    "                return None\n",
    "            \n",
    "            # Definir lookback adaptativo basado en datos disponibles\n",
    "            if lookback is None:\n",
    "                min_required = 60  # Mínimo requerido para cálculos\n",
    "                default_lookback = 252  # Valor por defecto\n",
    "                lookback = min(default_lookback, max(min_required, len(data['prices'])))\n",
    "            \n",
    "            # Obtener precios de manera segura\n",
    "            prices = data['prices'].iloc[-lookback:].copy()\n",
    "            prices = prices[[ticker1, ticker2]]  # Seleccionar solo las columnas necesarias\n",
    "            \n",
    "            # Verificar datos suficientes\n",
    "            if len(prices) < min_required or prices[ticker1].isna().all() or prices[ticker2].isna().all():\n",
    "                logging.warning(f\"Datos insuficientes para crear features: {len(prices)} puntos\")\n",
    "                return None\n",
    "            \n",
    "            # Imputar valores faltantes si hay algunos (máx 20% de la serie)\n",
    "            missing_pct1 = prices[ticker1].isna().mean()\n",
    "            missing_pct2 = prices[ticker2].isna().mean()\n",
    "            \n",
    "            if missing_pct1 > 0 and missing_pct1 < 0.2:\n",
    "                prices[ticker1] = prices[ticker1].interpolate(method='time').ffill().bfill()\n",
    "                logging.info(f\"Imputados {missing_pct1*100:.1f}% valores faltantes en {ticker1}\")\n",
    "                \n",
    "            if missing_pct2 > 0 and missing_pct2 < 0.2:\n",
    "                prices[ticker2] = prices[ticker2].interpolate(method='time').ffill().bfill()\n",
    "                logging.info(f\"Imputados {missing_pct2*100:.1f}% valores faltantes en {ticker2}\")\n",
    "            \n",
    "            # Calcular spread y z-score\n",
    "            spread = prices[ticker1] + hedge_ratio * prices[ticker2]\n",
    "            \n",
    "            # Ventanas adaptativas para media y desviación\n",
    "            ma_window = min(60, max(20, len(spread) // 4))\n",
    "            spread_mean = spread.rolling(window=ma_window).mean()\n",
    "            spread_std = spread.rolling(window=ma_window).std()\n",
    "            \n",
    "            # Evitar división por cero con verificación robusta\n",
    "            valid_idx = (spread_std > 0) & (~pd.isna(spread_mean)) & (~pd.isna(spread_std))\n",
    "            z_score = pd.Series(index=spread.index, data=np.nan)\n",
    "            z_score[valid_idx] = (spread[valid_idx] - spread_mean[valid_idx]) / spread_std[valid_idx]\n",
    "            \n",
    "            # Crear DataFrame de features con manejo de NaN\n",
    "            features = pd.DataFrame(index=z_score.index)\n",
    "            \n",
    "            # Z-score y cambios con períodos adaptativos\n",
    "            features['z_score'] = z_score\n",
    "            \n",
    "            # Usar ventanas adaptativas para diferencias\n",
    "            lag3 = min(3, max(1, len(z_score) // 84))\n",
    "            lag5 = min(5, max(2, len(z_score) // 50))\n",
    "            lag10 = min(10, max(3, len(z_score) // 25))\n",
    "            \n",
    "            features['z_score_change_short'] = z_score - z_score.shift(lag3)\n",
    "            features['z_score_change_medium'] = z_score - z_score.shift(lag5)\n",
    "            features['z_score_change_long'] = z_score - z_score.shift(lag10)\n",
    "            \n",
    "            # Velocidad de cambio (momentum)\n",
    "            features['z_momentum'] = features['z_score_change_short'] / lag3\n",
    "            \n",
    "            # Volatilidad relativa con manejo seguro de división por cero\n",
    "            spread_vol = pd.Series(index=spread_mean.index, data=np.nan)\n",
    "            nonzero_idx = (spread_mean.abs() > 1e-10) & (~pd.isna(spread_mean))\n",
    "            if nonzero_idx.any():\n",
    "                spread_vol[nonzero_idx] = spread_std[nonzero_idx] / spread_mean.abs()[nonzero_idx]\n",
    "            \n",
    "            # Ventana adaptativa para promedio de volatilidad\n",
    "            vol_window = min(60, max(20, len(spread) // 4))\n",
    "            spread_vol_avg = spread_vol.rolling(window=vol_window).mean()\n",
    "            \n",
    "            # Evitar otra división por cero\n",
    "            valid_vol_idx = (spread_vol_avg > 0) & (~pd.isna(spread_vol)) & (~pd.isna(spread_vol_avg))\n",
    "            rel_vol = pd.Series(index=spread_vol.index, data=np.nan)\n",
    "            rel_vol[valid_vol_idx] = spread_vol[valid_vol_idx] / spread_vol_avg[valid_vol_idx]\n",
    "            \n",
    "            features['rel_vol'] = rel_vol\n",
    "            \n",
    "            # Ratio de volumen anormal con verificación de datos disponibles\n",
    "            valid_vol_features = True\n",
    "            \n",
    "            if 'relative_volume' in data and not data['relative_volume'].empty:\n",
    "                rel_vol_data = data['relative_volume'].iloc[-lookback:]\n",
    "                \n",
    "                vol1 = pd.Series(1, index=prices.index)  # Valor por defecto\n",
    "                vol2 = pd.Series(1, index=prices.index)  # Valor por defecto\n",
    "                \n",
    "                if ticker1 in rel_vol_data.columns:\n",
    "                    common_idx1 = prices.index.intersection(rel_vol_data.index)\n",
    "                    vol1.loc[common_idx1] = rel_vol_data.loc[common_idx1, ticker1]\n",
    "                else:\n",
    "                    valid_vol_features = False\n",
    "                    \n",
    "                if ticker2 in rel_vol_data.columns:\n",
    "                    common_idx2 = prices.index.intersection(rel_vol_data.index)\n",
    "                    vol2.loc[common_idx2] = rel_vol_data.loc[common_idx2, ticker2]\n",
    "                else:\n",
    "                    valid_vol_features = False\n",
    "                \n",
    "                if valid_vol_features:\n",
    "                    features['abnormal_volume'] = (vol1 + vol2) / 2\n",
    "                    \n",
    "                    # Agregar features adicionales de volumen\n",
    "                    features['vol_imbalance'] = vol1 / vol2  # Desequilibrio de volumen entre pares\n",
    "                    features['vol_trend'] = vol1.rolling(window=5).mean() / vol1.rolling(window=20).mean()\n",
    "            \n",
    "            # Variables dummy de régimen\n",
    "            features['regime_1'] = 1 if regime == 1 else 0\n",
    "            features['regime_2'] = 1 if regime == 2 else 0\n",
    "            features['regime_3'] = 1 if regime == 3 else 0\n",
    "            \n",
    "            # Métricas de tendencia de mercado si están disponibles\n",
    "            if 'returns' in data and not data['returns'].empty:\n",
    "                mkt_returns = data['returns'].mean(axis=1).iloc[-lookback:]\n",
    "                if not mkt_returns.empty:\n",
    "                    mkt_trend = mkt_returns.rolling(window=20).mean()\n",
    "                    features['mkt_trend'] = 0\n",
    "                    common_idx = features.index.intersection(mkt_trend.index)\n",
    "                    if len(common_idx) > 0:\n",
    "                        features.loc[common_idx, 'mkt_trend'] = mkt_trend.loc[common_idx]\n",
    "            \n",
    "            # Características de autocorrelación del spread\n",
    "            if len(spread) > ma_window + 10:\n",
    "                # Autocorrelación del spread\n",
    "                spread_cleaned = spread.dropna()\n",
    "                if len(spread_cleaned) > ma_window:\n",
    "                    try:\n",
    "                        lag1_autocorr = pd.Series(\n",
    "                            [spread_cleaned.iloc[i-ma_window:i].autocorr(lag=1) \n",
    "                             for i in range(ma_window, len(spread_cleaned))],\n",
    "                            index=spread_cleaned.index[ma_window:]\n",
    "                        )\n",
    "                        features['autocorrelation'] = np.nan\n",
    "                        common_idx = features.index.intersection(lag1_autocorr.index)\n",
    "                        if len(common_idx) > 0:\n",
    "                            features.loc[common_idx, 'autocorrelation'] = lag1_autocorr.loc[common_idx]\n",
    "                    except Exception as e:\n",
    "                        logging.warning(f\"Error calculando autocorrelación: {str(e)}\")\n",
    "            \n",
    "            # Indicador de extremos (0-1)\n",
    "            features['extreme_indicator'] = features['z_score'].abs().apply(\n",
    "                lambda x: 1 if x > 2.5 else 0.75 if x > 2.0 else 0.5 if x > 1.5 else 0.25 if x > 1.0 else 0\n",
    "            )\n",
    "            \n",
    "            # Eliminar NaNs\n",
    "            features = features.dropna()\n",
    "            \n",
    "            # Verificar que tenemos suficientes datos\n",
    "            if len(features) < 10:\n",
    "                logging.warning(f\"Features insuficientes después de eliminar NaN: {len(features)} < 10\")\n",
    "                return None\n",
    "                \n",
    "            return features\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en create_features para {pair}: {str(e)}\")\n",
    "            logging.error(f\"Detalle: {traceback.format_exc()}\")\n",
    "            return None\n",
    "    \n",
    "    def create_target(self, data, pair, hedge_ratio, forward_period=None, threshold=0.5):\n",
    "        \"\"\"Crea variable objetivo para entrenamiento con período futuro adaptativo\"\"\"\n",
    "        try:\n",
    "            ticker1, ticker2 = pair\n",
    "            \n",
    "            # Verificar que los tickers existen en los datos\n",
    "            if ticker1 not in data['prices'].columns or ticker2 not in data['prices'].columns:\n",
    "                logging.error(f\"Tickers {ticker1} o {ticker2} no encontrados en create_target\")\n",
    "                return pd.Series(dtype='int')\n",
    "            \n",
    "            # Adaptar forward_period según datos disponibles\n",
    "            if forward_period is None:\n",
    "                # Adaptar basado en half-life típico de pares cointegrados (5-25 días)\n",
    "                # Usar un período más corto si tenemos pocos datos\n",
    "                if len(data['prices']) < 126:\n",
    "                    forward_period = min(5, max(2, len(data['prices']) // 20))\n",
    "                elif len(data['prices']) < 252:\n",
    "                    forward_period = min(10, max(3, len(data['prices']) // 20))\n",
    "                else:\n",
    "                    forward_period = 10\n",
    "                \n",
    "                logging.info(f\"Forward period adaptativo: {forward_period} días\")\n",
    "            \n",
    "            # Obtener precios de manera segura usando solo las columnas necesarias\n",
    "            prices = data['prices'][[ticker1, ticker2]].copy()\n",
    "            \n",
    "            # Imputar valores faltantes si hay algunos (máx 20% de la serie)\n",
    "            missing_pct1 = prices[ticker1].isna().mean()\n",
    "            missing_pct2 = prices[ticker2].isna().mean()\n",
    "            \n",
    "            if missing_pct1 > 0 and missing_pct1 < 0.2:\n",
    "                prices[ticker1] = prices[ticker1].interpolate(method='time').ffill().bfill()\n",
    "                \n",
    "            if missing_pct2 > 0 and missing_pct2 < 0.2:\n",
    "                prices[ticker2] = prices[ticker2].interpolate(method='time').ffill().bfill()\n",
    "            \n",
    "            # Verificar datos suficientes después de imputación\n",
    "            if len(prices) < 30 or prices[ticker1].isna().any() or prices[ticker2].isna().any():\n",
    "                logging.warning(f\"Datos insuficientes para crear target: {len(prices)} puntos\")\n",
    "                return pd.Series(dtype='int')\n",
    "            \n",
    "            # Calcular spread y z-score\n",
    "            spread = prices[ticker1] + hedge_ratio * prices[ticker2]\n",
    "            \n",
    "            # Ventanas adaptativas para media y desviación\n",
    "            ma_window = min(60, max(20, len(spread) // 4))\n",
    "            spread_mean = spread.rolling(window=ma_window).mean()\n",
    "            spread_std = spread.rolling(window=ma_window).std()\n",
    "            \n",
    "            # Manejar división por cero\n",
    "            valid_idx = (spread_std > 0) & (~pd.isna(spread_mean)) & (~pd.isna(spread_std))\n",
    "            z_score = pd.Series(index=spread.index, data=np.nan)\n",
    "            z_score[valid_idx] = (spread[valid_idx] - spread_mean[valid_idx]) / spread_std[valid_idx]\n",
    "            \n",
    "            # Crear variable objetivo\n",
    "            target = pd.Series(index=z_score.index, data=0)\n",
    "            \n",
    "            # Determinar convergencia futura\n",
    "            logging.info(f\"Calculando target con forward_period={forward_period}, threshold={threshold}\")\n",
    "            \n",
    "            for i in range(len(z_score) - forward_period):\n",
    "                current_z = z_score.iloc[i]\n",
    "                \n",
    "                if pd.isna(current_z):\n",
    "                    continue\n",
    "                    \n",
    "                # Solo considerar desviaciones significativas\n",
    "                if abs(current_z) > 1.0:\n",
    "                    future_idx_end = min(i + forward_period + 1, len(z_score))\n",
    "                    future_z = z_score.iloc[i+1:future_idx_end]\n",
    "                        \n",
    "                    if future_z.empty or future_z.isna().all():\n",
    "                        continue\n",
    "                        \n",
    "                    future_z_abs = future_z.abs()\n",
    "                    future_z_filtered = future_z_abs[~future_z_abs.isna()]\n",
    "                    \n",
    "                    if not future_z_filtered.empty:\n",
    "                        min_distance = future_z_filtered.min()\n",
    "                        \n",
    "                        # Verificar convergencia con umbral relativo\n",
    "                        if min_distance < threshold * abs(current_z):\n",
    "                            target.iloc[i] = 1\n",
    "                            \n",
    "                            # Registrar si la convergencia es rápida (útil para aprendizaje)\n",
    "                            min_idx = future_z_abs.idxmin()\n",
    "                            if min_idx is not None:\n",
    "                                days_to_converge = future_z.index.get_loc(min_idx) + 1\n",
    "                                if days_to_converge <= forward_period // 3:\n",
    "                                    # Convergencia muy rápida podría recibir más peso (2) en entrenamiento\n",
    "                                    target.iloc[i] = 2\n",
    "            \n",
    "            # Convertir a categorías 0-1 si hay valores 2\n",
    "            if (target == 2).any():\n",
    "                target = (target > 0).astype(int)\n",
    "                \n",
    "            return target\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en create_target para {pair}: {str(e)}\")\n",
    "            logging.error(f\"Detalle: {traceback.format_exc()}\")\n",
    "            return pd.Series(dtype='int')\n",
    "        \n",
    "    def train(self, data, selected_pairs, regime, max_history=None):\n",
    "        \"\"\"Entrena el modelo para un régimen específico con validación mejorada\"\"\"\n",
    "        try:\n",
    "            if not selected_pairs:\n",
    "                logging.warning(f\"No hay pares seleccionados para el régimen {regime}.\")\n",
    "                return False\n",
    "                \n",
    "            # Adaptar max_history a datos disponibles\n",
    "            if max_history is None:\n",
    "                max_history = min(1260, len(data['prices']))\n",
    "            else:\n",
    "                max_history = min(max_history, len(data['prices']))\n",
    "                \n",
    "            prices = data['prices'].iloc[-max_history:]\n",
    "            \n",
    "            all_features = []\n",
    "            all_targets = []\n",
    "            pair_identifiers = []  # Para tracking de resultados por par\n",
    "            \n",
    "            for pair_info in selected_pairs:\n",
    "                ticker1 = pair_info['ticker1']\n",
    "                ticker2 = pair_info['ticker2']\n",
    "                hedge_ratio = pair_info['hedge_ratio']\n",
    "                half_life = pair_info.get('half_life', 15)  # Valor por defecto si no está\n",
    "                \n",
    "                # Crear features con parámetros adaptativos\n",
    "                pair = (ticker1, ticker2)\n",
    "                features = self.create_features(data, pair, hedge_ratio, regime)\n",
    "                \n",
    "                if features is None or len(features) < 30:\n",
    "                    logging.info(f\"Features insuficientes para {ticker1}-{ticker2}: {0 if features is None else len(features)}\")\n",
    "                    continue\n",
    "                    \n",
    "                # Añadir half-life como feature\n",
    "                features['half_life'] = half_life\n",
    "                \n",
    "                # Crear target con forward_period adaptativo basado en half-life\n",
    "                forward_period = min(int(half_life * 1.5), 15)\n",
    "                forward_period = max(5, forward_period)  # Mínimo 5 días\n",
    "                \n",
    "                target = self.create_target(data, pair, hedge_ratio, forward_period=forward_period)\n",
    "                \n",
    "                # Alinear features y target\n",
    "                common_index = features.index.intersection(target.index)\n",
    "                if len(common_index) < 30:\n",
    "                    logging.info(f\"Datos alineados insuficientes para {ticker1}-{ticker2}: {len(common_index)} < 30\")\n",
    "                    continue\n",
    "                    \n",
    "                features = features.loc[common_index]\n",
    "                target = target.loc[common_index]\n",
    "                \n",
    "                all_features.append(features)\n",
    "                all_targets.append(target)\n",
    "                pair_identifiers.extend([(ticker1, ticker2)] * len(features))\n",
    "            \n",
    "            if not all_features:\n",
    "                logging.warning(f\"No se pudieron crear features para ningún par en régimen {regime}.\")\n",
    "                return False\n",
    "                \n",
    "            # Concatenar datos\n",
    "            X = pd.concat(all_features)\n",
    "            y = pd.concat(all_targets)\n",
    "            \n",
    "            # Verificar que tenemos suficientes muestras y clases\n",
    "            if len(X) < 50:\n",
    "                logging.warning(f\"Datos insuficientes para entrenar modelo de régimen {regime}: {len(X)} muestras\")\n",
    "                return False\n",
    "            \n",
    "            # Verificar que hay suficientes muestras de cada clase\n",
    "            class_counts = y.value_counts()\n",
    "            if len(class_counts) < 2:\n",
    "                logging.warning(f\"Solo hay una clase en los datos de entrenamiento para régimen {regime}: {class_counts}\")\n",
    "                \n",
    "                # Generar datos sintéticos balanceados si solo hay una clase\n",
    "                if 0 in class_counts and class_counts[0] > 0:\n",
    "                    # Si solo hay class 0, añadir algunas class 1 sintéticas\n",
    "                    synthetic_size = min(int(class_counts[0] * 0.2), 20)\n",
    "                    synthetic_indices = np.random.choice(X.index, size=synthetic_size, replace=False)\n",
    "                    synthetic_X = X.loc[synthetic_indices].copy()\n",
    "                    # Modificar features para que parezcan class 1 (convergentes)\n",
    "                    synthetic_X['z_score'] = synthetic_X['z_score'] * 1.5  # Aumentar magnitud\n",
    "                    synthetic_y = pd.Series(1, index=synthetic_X.index)\n",
    "                    \n",
    "                    X = pd.concat([X, synthetic_X])\n",
    "                    y = pd.concat([y, synthetic_y])\n",
    "                elif 1 in class_counts and class_counts[1] > 0:\n",
    "                    # Si solo hay class 1, añadir algunas class 0 sintéticas\n",
    "                    synthetic_size = min(int(class_counts[1] * 0.2), 20)\n",
    "                    synthetic_indices = np.random.choice(X.index, size=synthetic_size, replace=False)\n",
    "                    synthetic_X = X.loc[synthetic_indices].copy()\n",
    "                    # Modificar features para que parezcan class 0 (no convergentes)\n",
    "                    synthetic_X['z_score'] = synthetic_X['z_score'] * 0.5  # Reducir magnitud\n",
    "                    synthetic_y = pd.Series(0, index=synthetic_X.index)\n",
    "                    \n",
    "                    X = pd.concat([X, synthetic_X])\n",
    "                    y = pd.concat([y, synthetic_y])\n",
    "                else:\n",
    "                    # Si no hay datos en ninguna clase, no se puede entrenar\n",
    "                    logging.error(f\"No hay datos válidos para entrenar régimen {regime}\")\n",
    "                    return False\n",
    "            \n",
    "            # Re-verificar después de sintéticos\n",
    "            class_counts = y.value_counts()\n",
    "            if len(class_counts) < 2:\n",
    "                logging.error(f\"No se pudo crear un conjunto balanceado para régimen {regime}\")\n",
    "                return False\n",
    "                \n",
    "            # Asegurar balanceo mínimo para evitar sesgos\n",
    "            min_class = class_counts.min()\n",
    "            maj_class = class_counts.idxmax()\n",
    "            min_class_indices = y[y != maj_class].index\n",
    "            \n",
    "            # Sobremuestrear la clase minoritaria si es necesario\n",
    "            if len(min_class_indices) > 0 and min_class < 15 and class_counts[maj_class] > 30:\n",
    "                oversample_size = min(30, class_counts[maj_class])\n",
    "                oversample_indices = np.random.choice(min_class_indices, \n",
    "                                                   size=oversample_size, \n",
    "                                                   replace=True)\n",
    "                X_oversample = X.loc[oversample_indices]\n",
    "                y_oversample = y.loc[oversample_indices]\n",
    "                \n",
    "                X = pd.concat([X, X_oversample])\n",
    "                y = pd.concat([y, y_oversample])\n",
    "                \n",
    "                logging.info(f\"Clase minoritaria sobremuestreada: {min_class} -> {min_class + len(oversample_indices)}\")\n",
    "            \n",
    "            # Validación cruzada temporal con manejo de fechas\n",
    "            tscv = TimeSeriesSplit(n_splits=3)  # Validación temporal con 3 splits\n",
    "            best_score = 0\n",
    "            best_model = None\n",
    "            best_scaler = None\n",
    "            \n",
    "            # Convertir índice a números para split temporal\n",
    "            X_reset = X.reset_index(drop=True)\n",
    "            y_reset = y.reset_index(drop=True)\n",
    "            \n",
    "            for train_index, test_index in tscv.split(X_reset):\n",
    "                X_train, X_test = X_reset.iloc[train_index], X_reset.iloc[test_index]\n",
    "                y_train, y_test = y_reset.iloc[train_index], y_reset.iloc[test_index]\n",
    "                \n",
    "                # Verificar que ambos conjuntos tienen suficientes muestras\n",
    "                if len(X_train) < 30 or len(X_test) < 15:\n",
    "                    logging.info(f\"Split con datos insuficientes: train={len(X_train)}, test={len(X_test)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Verificar que ambos conjuntos tienen ambas clases\n",
    "                if len(pd.Series(y_train).unique()) < 2 or len(pd.Series(y_test).unique()) < 2:\n",
    "                    logging.info(\"Split sin ambas clases en train o test\")\n",
    "                    continue\n",
    "                \n",
    "                # Escalar features\n",
    "                scaler = StandardScaler()\n",
    "                X_train_scaled = scaler.fit_transform(X_train)\n",
    "                X_test_scaled = scaler.transform(X_test)\n",
    "                \n",
    "                # Ajustar complejidad del modelo según tamaño de datos\n",
    "                if len(X_train) < 100:\n",
    "                    model = GradientBoostingClassifier(\n",
    "                        learning_rate=0.05, \n",
    "                        max_depth=3, \n",
    "                        n_estimators=50,\n",
    "                        subsample=0.8,\n",
    "                        random_state=42\n",
    "                    )\n",
    "                elif len(X_train) < 200:\n",
    "                    model = GradientBoostingClassifier(\n",
    "                        learning_rate=0.03, \n",
    "                        max_depth=3, \n",
    "                        n_estimators=100,\n",
    "                        subsample=0.8,\n",
    "                        random_state=42\n",
    "                    )\n",
    "                else:\n",
    "                    model = self.models[regime]\n",
    "                \n",
    "                # Balancear pesos si hay desbalance significativo\n",
    "                class_weight = None\n",
    "                class_counts_train = pd.Series(y_train).value_counts()\n",
    "                if len(class_counts_train) >= 2:\n",
    "                    count_0 = class_counts_train.get(0, 0)\n",
    "                    count_1 = class_counts_train.get(1, 0)\n",
    "                    if count_0 > 0 and count_1 > 0:\n",
    "                        ratio = count_0 / count_1 if count_0 > count_1 else count_1 / count_0\n",
    "                        if ratio > 3:  # Desbalanceo significativo\n",
    "                            class_weight = 'balanced'\n",
    "                \n",
    "                try:\n",
    "                    # Entrenar modelo con balanceo si es necesario\n",
    "                    if class_weight:\n",
    "                        model.set_params(class_weight=class_weight)\n",
    "                        \n",
    "                    model.fit(X_train_scaled, y_train)\n",
    "                    \n",
    "                    # Evaluar con métrica relevante (AUC mejor que accuracy para desbalanceados)\n",
    "                    try:\n",
    "                        from sklearn.metrics import roc_auc_score\n",
    "                        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "                        score = roc_auc_score(y_test, y_pred_proba)\n",
    "                    except:\n",
    "                        # Si AUC falla, usar accuracy\n",
    "                        score = model.score(X_test_scaled, y_test)\n",
    "                    \n",
    "                    logging.info(f\"CV split score: {score:.4f}\")\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_model = model\n",
    "                        best_scaler = scaler\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error entrenando en split CV para régimen {regime}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            # Si encontramos un buen modelo en CV, usarlo\n",
    "            if best_score > 0.6:  # Umbral de calidad mínima\n",
    "                self.models[regime] = best_model\n",
    "                self.scalers[regime] = best_scaler\n",
    "                self.is_trained[regime] = True\n",
    "                self.best_validation_scores[regime] = best_score\n",
    "                \n",
    "                # Guardar importancia de features\n",
    "                if hasattr(best_model, 'feature_importances_'):\n",
    "                    self.feature_importance[regime] = pd.Series(\n",
    "                        best_model.feature_importances_,\n",
    "                        index=X.columns\n",
    "                    ).sort_values(ascending=False)\n",
    "                    \n",
    "                    logging.info(f\"Top features para régimen {regime}: {self.feature_importance[regime].head(5).to_dict()}\")\n",
    "                \n",
    "                logging.info(f\"Modelo para régimen {regime} entrenado con score: {best_score:.4f}\")\n",
    "                self.last_training_date = datetime.now()\n",
    "                return True\n",
    "            else:\n",
    "                # Modelo final con todos los datos si no obtuvimos buen modelo en CV\n",
    "                try:\n",
    "                    scaler = StandardScaler()\n",
    "                    X_scaled = scaler.fit_transform(X)\n",
    "                    \n",
    "                    # Usar modelo más simple para datos difíciles\n",
    "                    model = GradientBoostingClassifier(\n",
    "                        learning_rate=0.05, \n",
    "                        max_depth=3, \n",
    "                        n_estimators=100,\n",
    "                        subsample=0.8,\n",
    "                        class_weight='balanced',\n",
    "                        random_state=42\n",
    "                    )\n",
    "                    \n",
    "                    model.fit(X_scaled, y)\n",
    "                    self.models[regime] = model\n",
    "                    self.scalers[regime] = scaler\n",
    "                    self.is_trained[regime] = True\n",
    "                    \n",
    "                    # Calidad estimada mediante validación\n",
    "                    preds = model.predict(X_scaled)\n",
    "                    accuracy = (preds == y).mean()\n",
    "                    self.best_validation_scores[regime] = accuracy\n",
    "                    \n",
    "                    logging.info(f\"Modelo alternativo para régimen {regime} con accuracy: {accuracy:.4f}\")\n",
    "                    \n",
    "                    # Guardar importancia de features\n",
    "                    if hasattr(model, 'feature_importances_'):\n",
    "                        self.feature_importance[regime] = pd.Series(\n",
    "                            model.feature_importances_,\n",
    "                            index=X.columns\n",
    "                        ).sort_values(ascending=False)\n",
    "                    \n",
    "                    self.last_training_date = datetime.now()\n",
    "                    return True\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error entrenando modelo final para régimen {regime}: {str(e)}\")\n",
    "                    logging.error(traceback.format_exc())\n",
    "                    return False\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error general en train para régimen {regime}: {str(e)}\")\n",
    "            logging.error(traceback.format_exc())\n",
    "            return False\n",
    "    \n",
    "    def predict_convergence(self, data, pair_info, regime):\n",
    "        \"\"\"Predice probabilidad de convergencia para un par con manejo robusto de errores\"\"\"\n",
    "        if not self.is_trained[regime]:\n",
    "            logging.info(f\"Modelo para régimen {regime} no entrenado, usando probabilidad neutral\")\n",
    "            return 0.5  # Valor neutro si no está entrenado\n",
    "            \n",
    "        try:\n",
    "            ticker1 = pair_info['ticker1']\n",
    "            ticker2 = pair_info['ticker2']\n",
    "            hedge_ratio = pair_info['hedge_ratio']\n",
    "            half_life = pair_info.get('half_life', 15)  # Valor por defecto si no está\n",
    "            \n",
    "            # Crear features con parámetros adaptados a disponibilidad de datos\n",
    "            pair = (ticker1, ticker2)\n",
    "            features = self.create_features(data, pair, hedge_ratio, regime, lookback=60)\n",
    "            \n",
    "            if features is None or len(features) < 5:\n",
    "                logging.info(f\"Features insuficientes para predicción de {ticker1}-{ticker2}\")\n",
    "                return 0.5\n",
    "                \n",
    "            # Añadir half-life\n",
    "            features['half_life'] = half_life\n",
    "            \n",
    "            # Verificar columnas consistentes con entrenamiento\n",
    "            if hasattr(self.models[regime], 'feature_names_in_'):\n",
    "                model_features = set(self.models[regime].feature_names_in_)\n",
    "                current_features = set(features.columns)\n",
    "                \n",
    "                # Si faltan features en los datos actuales\n",
    "                missing_features = model_features - current_features\n",
    "                if missing_features:\n",
    "                    logging.warning(f\"Faltan features: {missing_features}\")\n",
    "                    # Añadir las que faltan con valores neutrales\n",
    "                    for feat in missing_features:\n",
    "                        features[feat] = 0\n",
    "                        \n",
    "                # Seleccionar solo las features que el modelo conoce\n",
    "                features = features[list(model_features)]\n",
    "            \n",
    "            # Usar solo la última fila\n",
    "            latest_features = features.iloc[-1:].copy()\n",
    "            \n",
    "            # Escalar features\n",
    "            try:\n",
    "                latest_features_scaled = self.scalers[regime].transform(latest_features)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error al escalar features para {ticker1}-{ticker2}: {str(e)}\")\n",
    "                return 0.5\n",
    "            \n",
    "            # Predecir probabilidad\n",
    "            try:\n",
    "                probability = self.models[regime].predict_proba(latest_features_scaled)[0, 1]\n",
    "                logging.info(f\"Predicción para {ticker1}-{ticker2}: {probability:.4f}\")\n",
    "                return probability\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error en predict_proba para {ticker1}-{ticker2}: {str(e)}\")\n",
    "                return 0.5  # En caso de error, devolver valor neutro\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en predict_convergence para {ticker1}_{ticker2}: {str(e)}\")\n",
    "            logging.error(traceback.format_exc())\n",
    "            return 0.5\n",
    "\n",
    "class SignalGenerator:\n",
    "    \"\"\"Genera señales de trading basadas en z-scores y regímenes con mayor robustez\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Umbrales de entrada por régimen (ajustados para mayor prudencia)\n",
    "        self.entry_thresholds = {\n",
    "            1: {'long': -2.0, 'short': 2.0},     # Régimen favorable\n",
    "            2: {'long': -2.2, 'short': 2.2},     # Régimen de transición\n",
    "            3: {'long': -2.5, 'short': 2.5}      # Régimen de crisis\n",
    "        }\n",
    "        \n",
    "        # Umbrales de salida por régimen\n",
    "        self.exit_thresholds = {\n",
    "            1: {'long': -0.5, 'short': 0.5},\n",
    "            2: {'long': -0.7, 'short': 0.7},\n",
    "            3: {'long': -1.0, 'short': 1.0}\n",
    "        }\n",
    "        \n",
    "        # Bandas de no-transacción (ajustables por régimen)\n",
    "        self.no_trade_base_width = {\n",
    "            1: 0.2,  # Menor en regímenes estables\n",
    "            2: 0.3,\n",
    "            3: 0.4   # Mayor en regímenes volátiles\n",
    "        }\n",
    "        \n",
    "        # Seguimiento de señales históricas para análisis\n",
    "        self.signal_history = {}\n",
    "        \n",
    "        # Retrasos mínimos entre operaciones para cada par\n",
    "        self.min_bars_between_trades = 3\n",
    "    \n",
    "    def calculate_z_score(self, data, pair_info, lookback=None, method='ewm'):\n",
    "        \"\"\"Calcula z-score actual para un par con ventanas adaptativas\"\"\"\n",
    "        ticker1 = pair_info['ticker1']\n",
    "        ticker2 = pair_info['ticker2']\n",
    "        hedge_ratio = pair_info['hedge_ratio']\n",
    "        \n",
    "        try:\n",
    "            # Verificación de tickers\n",
    "            if ticker1 not in data['prices'].columns or ticker2 not in data['prices'].columns:\n",
    "                logging.warning(f\"Tickers {ticker1} o {ticker2} no encontrados para cálculo de z-score\")\n",
    "                return None\n",
    "            \n",
    "            # Determinar lookback adaptativo\n",
    "            if lookback is None:\n",
    "                # Usar una ventana adaptativa basada en half-life si está disponible\n",
    "                if 'half_life' in pair_info and pair_info['half_life'] not in (None, np.inf):\n",
    "                    half_life = pair_info['half_life']\n",
    "                    lookback = min(int(half_life * 5), 60)  # 5x half-life o máximo 60 días\n",
    "                else:\n",
    "                    lookback = min(60, len(data['prices']) // 4)  # 25% de datos disponibles o máximo 60 días\n",
    "                lookback = max(lookback, 20)  # Mínimo 20 días para estadística decente\n",
    "            \n",
    "            # Verificar datos suficientes\n",
    "            if len(data['prices']) < lookback:\n",
    "                lookback = max(20, len(data['prices']) - 1)  # Ajustar dinámicamente\n",
    "                \n",
    "            # Obtener precios\n",
    "            prices = data['prices'].iloc[-lookback:][[ticker1, ticker2]]\n",
    "            \n",
    "            # Verificar datos válidos\n",
    "            if len(prices) < lookback/2 or prices[ticker1].isna().any() or prices[ticker2].isna().any():\n",
    "                logging.warning(f\"Datos insuficientes o inválidos para z-score de {ticker1}-{ticker2}\")\n",
    "                return None\n",
    "            \n",
    "            # Calcular spread\n",
    "            spread = prices[ticker1] + hedge_ratio * prices[ticker2]\n",
    "            \n",
    "            # Calcular media y desviación con método seleccionado\n",
    "            if method == 'ewm':\n",
    "                # Media y desviación móvil exponencial\n",
    "                # Usar half-life si está disponible, o ajustar según lookback\n",
    "                half_life = pair_info.get('half_life', lookback/3)\n",
    "                # Limitar half-life a valores razonables\n",
    "                half_life = min(max(5, half_life), lookback/2)\n",
    "                \n",
    "                spread_mean = spread.ewm(halflife=half_life).mean().iloc[-1]\n",
    "                spread_std = spread.ewm(halflife=half_life/3).std().iloc[-1]  # Más sensible a volatilidad reciente\n",
    "            else:\n",
    "                # Media y desviación móvil simple\n",
    "                window = min(lookback, max(20, len(spread) // 3))\n",
    "                spread_mean = spread.rolling(window=window).mean().iloc[-1]\n",
    "                spread_std = spread.rolling(window=window).std().iloc[-1]\n",
    "            \n",
    "            # Verificar valores válidos\n",
    "            if pd.isna(spread_mean) or pd.isna(spread_std) or spread_std <= 0:\n",
    "                logging.warning(f\"Estadísticas inválidas para z-score de {ticker1}-{ticker2}\")\n",
    "                return None\n",
    "            \n",
    "            # Z-score\n",
    "            z_score = (spread.iloc[-1] - spread_mean) / spread_std\n",
    "            \n",
    "            # Verificar valor razonable\n",
    "            if abs(z_score) > 5:\n",
    "                logging.warning(f\"Z-score extremo: {z_score:.2f} para {ticker1}-{ticker2}\")\n",
    "            \n",
    "            logging.info(f\"Z-score calculado para {ticker1}-{ticker2}: {z_score:.2f}\")\n",
    "            return z_score\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error calculando z-score para {ticker1}-{ticker2}: {str(e)}\")\n",
    "            logging.error(traceback.format_exc())\n",
    "            return None\n",
    "    \n",
    "    def adjust_no_trade_band(self, pair_info, regime, vol_increase=0.0, cost_bps=1.0):\n",
    "        \"\"\"Ajusta banda de no-transacción según volatilidad, costos y régimen\"\"\"\n",
    "        try:\n",
    "            # Base ajustada por régimen\n",
    "            base_width = self.no_trade_base_width.get(regime, 0.3)\n",
    "            \n",
    "            # Ajuste por volatilidad (incrementar si volatilidad sube)\n",
    "            vol_adj = base_width + 0.1 * (vol_increase / 25.0)\n",
    "            \n",
    "            # Ajuste por costos de transacción\n",
    "            # Escalar proporcionalmente al spread de entrada/salida\n",
    "            entry_exit_gap = self.entry_thresholds[regime]['short'] - self.exit_thresholds[regime]['short']\n",
    "            cost_factor = (cost_bps / 10) * entry_exit_gap * 0.1  # 1bp = 0.01% costo\n",
    "            cost_adj = vol_adj + cost_factor\n",
    "            \n",
    "            # Ajuste por half-life si está disponible\n",
    "            if 'half_life' in pair_info and pair_info['half_life'] not in (None, np.inf):\n",
    "                half_life = pair_info['half_life']\n",
    "                # Pares con half-life más corto (más rápido) necesitan bandas más estrechas\n",
    "                if half_life < 10:\n",
    "                    half_life_adj = cost_adj * 0.9\n",
    "                elif half_life > 20:\n",
    "                    half_life_adj = cost_adj * 1.1\n",
    "                else:\n",
    "                    half_life_adj = cost_adj\n",
    "                    \n",
    "                return half_life_adj\n",
    "            else:\n",
    "                return cost_adj\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en adjust_no_trade_band: {str(e)}\")\n",
    "            # Valor por defecto seguro\n",
    "            return 0.3\n",
    "    \n",
    "    def check_recent_signals(self, pair_id, current_date):\n",
    "        \"\"\"Verifica si ha pasado suficiente tiempo desde la última señal\"\"\"\n",
    "        try:\n",
    "            if pair_id not in self.signal_history:\n",
    "                return True  # No hay historial previo\n",
    "                \n",
    "            last_signal_date = self.signal_history[pair_id]['date']\n",
    "            min_bars = self.min_bars_between_trades\n",
    "            \n",
    "            # Calcular días de trading (aproximado)\n",
    "            trading_days = 0\n",
    "            date_diff = (current_date - last_signal_date).days\n",
    "            \n",
    "            if date_diff < 1:\n",
    "                return False  # Misma fecha, demasiado pronto\n",
    "            elif date_diff < min_bars:\n",
    "                # Contar días de trading (aproximado, ignora fines de semana)\n",
    "                if last_signal_date.weekday() < 5 and current_date.weekday() < 5:\n",
    "                    trading_days = date_diff\n",
    "                else:\n",
    "                    # Ajustar por posibles fines de semana\n",
    "                    trading_days = max(1, date_diff - 2)\n",
    "                    \n",
    "                return trading_days >= min_bars\n",
    "            else:\n",
    "                return True  # Ha pasado suficiente tiempo\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en check_recent_signals: {str(e)}\")\n",
    "            return True  # En caso de duda, permitir la señal\n",
    "    \n",
    "    def generate_signal(self, data, pair_info, regime, current_position=0, conv_probability=0.5, \n",
    "                        current_date=None, market_conditions=None):\n",
    "        \"\"\"Genera señal de trading para un par con mayor robustez y factores adicionales\"\"\"\n",
    "        try:\n",
    "            # Importar linregress para el análisis de tendencia\n",
    "            from scipy.stats import linregress\n",
    "            \n",
    "            ticker1 = pair_info['ticker1']\n",
    "            ticker2 = pair_info['ticker2']\n",
    "            hedge_ratio = pair_info['hedge_ratio']\n",
    "            pair_id = f\"{ticker1}_{ticker2}\"\n",
    "            \n",
    "            # Usar fecha actual del conjunto de datos\n",
    "            if current_date is None and not data['prices'].empty:\n",
    "                current_date = data['prices'].index[-1]\n",
    "            elif current_date is None:\n",
    "                current_date = datetime.now()\n",
    "            \n",
    "            # Verificar tiempo desde última señal\n",
    "            if not self.check_recent_signals(pair_id, current_date):\n",
    "                logging.info(f\"Tiempo insuficiente desde última señal para {pair_id}\")\n",
    "                return {'signal': 0, 'z_score': None, 'strength': 0}\n",
    "            \n",
    "            # Calcular z-score usando método adecuado\n",
    "            z_score = self.calculate_z_score(data, pair_info)\n",
    "            \n",
    "            if z_score is None:\n",
    "                return {'signal': 0, 'z_score': None, 'strength': 0}\n",
    "            \n",
    "            # Ajustar umbrales basados en probabilidad de convergencia\n",
    "            # Más agresivo si probabilidad es alta, más conservador si es baja\n",
    "            prob_adj = (conv_probability - 0.5) * 0.5\n",
    "            \n",
    "            # Obtener umbrales base para el régimen\n",
    "            entry_long_base = self.entry_thresholds[regime]['long']\n",
    "            entry_short_base = self.entry_thresholds[regime]['short']\n",
    "            exit_long_base = self.exit_thresholds[regime]['long']\n",
    "            exit_short_base = self.exit_thresholds[regime]['short']\n",
    "            \n",
    "            # Aplicar ajuste por probabilidad\n",
    "            entry_long = entry_long_base + prob_adj\n",
    "            entry_short = entry_short_base - prob_adj\n",
    "            exit_long = exit_long_base - prob_adj\n",
    "            exit_short = exit_short_base + prob_adj\n",
    "            \n",
    "            # Ajustar en mercados turbulentos si tenemos información\n",
    "            if market_conditions and 'high_volatility' in market_conditions and market_conditions['high_volatility']:\n",
    "                # Más conservador en entrada, más rápido en salida\n",
    "                entry_long *= 1.1  # Requiere mayor desviación para entrar largo (-2.0 -> -2.2)\n",
    "                entry_short *= 1.1  # Requiere mayor desviación para entrar corto (2.0 -> 2.2)\n",
    "                exit_long *= 0.8   # Salir antes en posiciones largas (-0.5 -> -0.4)\n",
    "                exit_short *= 0.8  # Salir antes en posiciones cortas (0.5 -> 0.4)\n",
    "                \n",
    "                logging.info(f\"Umbrales ajustados por alta volatilidad para {pair_id}\")\n",
    "            \n",
    "            # Ajustar bandas de no-transacción\n",
    "            vol_increase = market_conditions.get('volatility_increase', 0.0) if market_conditions else 0.0\n",
    "            cost_bps = market_conditions.get('cost_bps', 1.0) if market_conditions else 1.0\n",
    "            no_trade_band = self.adjust_no_trade_band(pair_info, regime, vol_increase, cost_bps)\n",
    "            exit_no_trade_band = no_trade_band * 1.2\n",
    "            \n",
    "            # Determinar señal\n",
    "            signal = 0\n",
    "            strength = 0\n",
    "            reason = \"Sin señal\"\n",
    "            \n",
    "            if current_position == 0:  # Sin posición\n",
    "                if z_score < entry_long:\n",
    "                    signal = 1  # Comprar\n",
    "                    strength = min(1.0, (entry_long - z_score) / abs(entry_long * 0.5))\n",
    "                    reason = \"Entrada larga\"\n",
    "                elif z_score > entry_short:\n",
    "                    signal = -1  # Vender\n",
    "                    strength = min(1.0, (z_score - entry_short) / abs(entry_short * 0.5))\n",
    "                    reason = \"Entrada corta\"\n",
    "            elif current_position == 1:  # Posición larga\n",
    "                if z_score > exit_long:\n",
    "                    signal = -1  # Cerrar\n",
    "                    strength = min(1.0, (z_score - exit_long) / abs(exit_long * 0.5))\n",
    "                    reason = \"Cierre de larga\"\n",
    "            elif current_position == -1:  # Posición corta\n",
    "                if z_score < exit_short:\n",
    "                    signal = 1  # Cerrar\n",
    "                    strength = min(1.0, (exit_short - z_score) / abs(exit_short * 0.5))\n",
    "                    reason = \"Cierre de corta\"\n",
    "            \n",
    "            # Aplicar bandas de no-transacción para evitar ruido\n",
    "            if abs(signal) > 0 and abs(z_score) < no_trade_band and current_position == 0:\n",
    "                signal = 0\n",
    "                strength = 0\n",
    "                reason = f\"Dentro de banda de no-transacción ({no_trade_band:.2f})\"\n",
    "            elif abs(signal) > 0 and abs(z_score) < exit_no_trade_band and current_position != 0:\n",
    "                signal = 0\n",
    "                strength = 0\n",
    "                reason = f\"Dentro de banda de no-transacción para salida ({exit_no_trade_band:.2f})\"\n",
    "            \n",
    "            # Verificar si la señal es contraria a la tendencia reciente\n",
    "            if signal != 0:\n",
    "                try:\n",
    "                    # Calcular tendencia de z-score en últimos N períodos\n",
    "                    lookback = min(10, len(data['prices']) // 10)\n",
    "                    prices_subset = data['prices'].iloc[-lookback:][[ticker1, ticker2]]\n",
    "                    spread_subset = prices_subset[ticker1] + hedge_ratio * prices_subset[ticker2]\n",
    "                    \n",
    "                    if len(spread_subset) >= 5:\n",
    "                        # Tendencia: positiva (1) o negativa (-1)\n",
    "                        # Usar simple regresión lineal de z-score\n",
    "                        x = np.arange(len(spread_subset))\n",
    "                        y = spread_subset.values\n",
    "                        slope, _, _, _, _ = linregress(x, y)\n",
    "                        trend = 1 if slope > 0 else -1 if slope < 0 else 0\n",
    "                        \n",
    "                        # Si la señal es contraria a tendencia fuerte, aumentar fuerza\n",
    "                        # Esto porque una reversión contra tendencia puede ser significativa\n",
    "                        if signal * trend < 0 and abs(slope) > 0.02:  # Tendencia contraria significativa\n",
    "                            strength = min(1.0, strength * 1.2)\n",
    "                            reason += \" - Contra tendencia (señal más fuerte)\"\n",
    "                        elif signal * trend > 0 and abs(slope) > 0.02:  # A favor de tendencia\n",
    "                            # Puede ser continuación o sobreextensión - usar fuerza normal\n",
    "                            reason += \" - Con tendencia\"\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"No se pudo analizar tendencia: {str(e)}\")\n",
    "            \n",
    "            # Registrar en historial si hay señal\n",
    "            if signal != 0:\n",
    "                self.signal_history[pair_id] = {\n",
    "                    'date': current_date,\n",
    "                    'z_score': z_score,\n",
    "                    'signal': signal,\n",
    "                    'strength': strength\n",
    "                }\n",
    "                \n",
    "                logging.info(f\"Señal generada para {pair_id}: {signal} (fuerza={strength:.2f}, z-score={z_score:.2f}, razón={reason})\")\n",
    "            \n",
    "            return {\n",
    "                'signal': signal,\n",
    "                'z_score': z_score,\n",
    "                'strength': strength,\n",
    "                'reason': reason,\n",
    "                'thresholds': {\n",
    "                    'entry_long': entry_long,\n",
    "                    'entry_short': entry_short,\n",
    "                    'exit_long': exit_long,\n",
    "                    'exit_short': exit_short,\n",
    "                    'no_trade_band': no_trade_band\n",
    "                }\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en generate_signal para {pair_info.get('ticker1', '')}-{pair_info.get('ticker2', '')}: {str(e)}\")\n",
    "            logging.error(traceback.format_exc())\n",
    "            return {'signal': 0, 'z_score': None, 'strength': 0}\n",
    "        \n",
    "# Gestión de Posiciones y Riesgo\n",
    "class PositionManager:\n",
    "    \"\"\"Gestiona posiciones y riesgo para la estrategia con parámetros realistas\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Límites de posición por régimen (reducidos para mayor prudencia)\n",
    "        self.position_limits = {\n",
    "            1: 0.025,  # 2.5% máximo por par en Régimen 1 (favorable)\n",
    "            2: 0.02,   # 2% en Régimen 2 (transición)\n",
    "            3: 0.015   # 1.5% en Régimen 3 (crisis)\n",
    "        }\n",
    "        \n",
    "        # Factores ATR para stop-loss (aumentados para mayor margen)\n",
    "        self.atr_factors = {\n",
    "            1: 3.5,  # Régimen favorable - stops más amplios\n",
    "            2: 3.0,  # Régimen de transición\n",
    "            3: 2.5   # Régimen de crisis - stops más ajustados\n",
    "        }\n",
    "        \n",
    "        # Límites de concentración por sector (reducidos)\n",
    "        self.sector_limits = {\n",
    "            1: 0.18,  # 18% por sector en Régimen 1\n",
    "            2: 0.15,  # 15% en Régimen 2\n",
    "            3: 0.12   # 12% en Régimen 3\n",
    "        }\n",
    "        \n",
    "        # Volatilidad objetivo por régimen (reducida para mejor control de riesgo)\n",
    "        self.vol_targets = {\n",
    "            1: 0.08,  # 8% anualizada en régimen favorable\n",
    "            2: 0.07,  # 7% anualizada en régimen de transición\n",
    "            3: 0.05   # 5% anualizada en régimen de crisis\n",
    "        }\n",
    "        \n",
    "        # Circuit breakers reajustados\n",
    "        self.circuit_breakers = {\n",
    "            'level1': {'reduction': 0.30, 'vix_percentile': 75, 'vix_increase': 0.08},\n",
    "            'level2': {'reduction': 0.55, 'vix_percentile': 85, 'vix_increase': 0.15, 'correlation': 0.65},\n",
    "            'level3': {'reduction': 0.80, 'vix_percentile': 90, 'vix_increase': 0.20, 'correlation': 0.75}\n",
    "        }\n",
    "        \n",
    "        # Parámetros de costos de transacción\n",
    "        self.transaction_costs = {\n",
    "            'commission_pct': 0.0005,  # 0.05% comisión por operación (5bps)\n",
    "            'slippage_factor': 0.0003, # 0.03% slippage base (3bps)\n",
    "            'market_impact_factor': 0.1 # Factor de impacto de mercado (% de ADV)\n",
    "        }\n",
    "        \n",
    "        # Parámetros de liquidez\n",
    "        self.liquidity_params = {\n",
    "            'min_adv_usd': 5000000,   # $5M ADV mínimo\n",
    "            'max_position_pct_adv': 0.03,  # Máximo 3% del ADV diario\n",
    "            'illiquid_slippage_multiplier': 2.0  # Doble slippage para baja liquidez\n",
    "        }\n",
    "        \n",
    "        self.current_positions = {}\n",
    "        self.position_history = []\n",
    "        self.last_risk_check = None\n",
    "    \n",
    "    def calculate_transaction_costs(self, pair_info, position_size, data):\n",
    "        \"\"\"Calcula costos de transacción realistas incluyendo comisiones, slippage e impacto de mercado\"\"\"\n",
    "        try:\n",
    "            ticker1 = pair_info['ticker1']\n",
    "            ticker2 = pair_info['ticker2']\n",
    "            \n",
    "            # 1. Comisiones fijas\n",
    "            commission_cost = abs(position_size) * self.transaction_costs['commission_pct']\n",
    "            \n",
    "            # 2. Slippage basado en liquidez\n",
    "            base_slippage = self.transaction_costs['slippage_factor']\n",
    "            \n",
    "            # Obtener liquidez reciente\n",
    "            ticker1_adv = data['adv'].iloc[-5:][ticker1].mean() if ticker1 in data['adv'].columns else 0\n",
    "            ticker2_adv = data['adv'].iloc[-5:][ticker2].mean() if ticker2 in data['adv'].columns else 0\n",
    "            \n",
    "            # Si datos no disponibles, estimar con volumen * precio\n",
    "            if ticker1_adv == 0 and 'volumes' in data and 'prices' in data:\n",
    "                if ticker1 in data['volumes'].columns and ticker1 in data['prices'].columns:\n",
    "                    vol = data['volumes'].iloc[-5:][ticker1].mean()\n",
    "                    price = data['prices'].iloc[-1][ticker1]\n",
    "                    ticker1_adv = vol * price\n",
    "            \n",
    "            if ticker2_adv == 0 and 'volumes' in data and 'prices' in data:\n",
    "                if ticker2 in data['volumes'].columns and ticker2 in data['prices'].columns:\n",
    "                    vol = data['volumes'].iloc[-5:][ticker2].mean()\n",
    "                    price = data['prices'].iloc[-1][ticker2]\n",
    "                    ticker2_adv = vol * price\n",
    "            \n",
    "            # Determinar el activo menos líquido\n",
    "            min_adv = min(ticker1_adv, ticker2_adv)\n",
    "            min_adv = max(min_adv, 1000000)  # Asegurar valor mínimo para evitar división por cero\n",
    "            \n",
    "            # Calcular slippage ajustado por liquidez\n",
    "            if min_adv < self.liquidity_params['min_adv_usd']:\n",
    "                slippage_multiplier = self.liquidity_params['illiquid_slippage_multiplier']\n",
    "                logging.info(f\"Aumentando slippage para {ticker1}-{ticker2} por baja liquidez: ${min_adv/1e6:.1f}M < ${self.liquidity_params['min_adv_usd']/1e6:.1f}M\")\n",
    "            else:\n",
    "                slippage_multiplier = 1.0\n",
    "                \n",
    "            adjusted_slippage = base_slippage * slippage_multiplier\n",
    "            \n",
    "            # 3. Impacto de mercado (aumenta con el tamaño relativo a ADV)\n",
    "            # Estimar valor nominal de la posición\n",
    "            notional_value = abs(position_size) * 1e6  # Asumiendo $1M como capital base para simplificar\n",
    "            \n",
    "            # Calcular % de ADV\n",
    "            pct_of_adv = notional_value / min_adv if min_adv > 0 else 0.05\n",
    "            \n",
    "            # Ajustar por tamaño (modelo simplificado: impacto ~ raíz cuadrada del tamaño relativo)\n",
    "            market_impact = self.transaction_costs['market_impact_factor'] * np.sqrt(pct_of_adv)\n",
    "            \n",
    "            # Total de costos como % del valor de la posición\n",
    "            total_cost_pct = commission_cost + adjusted_slippage + market_impact\n",
    "            \n",
    "            logging.info(f\"Costos para {ticker1}-{ticker2}: comisión={commission_cost*100:.3f}%, slippage={adjusted_slippage*100:.3f}%, impacto={market_impact*100:.3f}%, total={total_cost_pct*100:.3f}%\")\n",
    "            \n",
    "            return {\n",
    "                'total_pct': total_cost_pct,\n",
    "                'commission': commission_cost,\n",
    "                'slippage': adjusted_slippage,\n",
    "                'market_impact': market_impact,\n",
    "                'min_adv': min_adv,\n",
    "                'pct_of_adv': pct_of_adv\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en calculate_transaction_costs: {str(e)}\")\n",
    "            logging.error(traceback.format_exc())\n",
    "            # Retornar valor por defecto en caso de error\n",
    "            return {\n",
    "                'total_pct': 0.001,  # 0.1% por defecto\n",
    "                'commission': 0.0005,\n",
    "                'slippage': 0.0005,\n",
    "                'market_impact': 0,\n",
    "                'min_adv': 0,\n",
    "                'pct_of_adv': 0\n",
    "            }\n",
    "    \n",
    "    def calculate_position_size(self, pair_info, signal_strength, regime, \n",
    "                               conv_probability, current_volatility, data):\n",
    "        \"\"\"Calcula tamaño óptimo de posición considerando todos los factores relevantes\"\"\"\n",
    "        try:\n",
    "            ticker1 = pair_info['ticker1']\n",
    "            ticker2 = pair_info['ticker2']\n",
    "            \n",
    "            # Verificar volatilidad válida\n",
    "            if current_volatility <= 0:\n",
    "                logging.warning(f\"Volatilidad inválida para {ticker1}-{ticker2}\")\n",
    "                return 0\n",
    "                \n",
    "            # 1. Volatilidad inversa normalizada con límites realistas\n",
    "            vol_sizing = min(0.2 / current_volatility, 3.0) / 3.0\n",
    "            \n",
    "            # 2. Ajuste por convicción (probabilidad de convergencia)\n",
    "            # Más convicción = posición más grande\n",
    "            conv_multiplier = 0.7 + 0.6 * conv_probability\n",
    "            \n",
    "            # 3. Liquidez y capacidad\n",
    "            liquidity = self.check_liquidity_constraint(pair_info, data)\n",
    "            liquidity_factor = min(1.0, liquidity / 50e6)\n",
    "            \n",
    "            # 4. Señal y fuerza de la señal\n",
    "            signal_factor = signal_strength\n",
    "            \n",
    "            # Calcular tamaño base\n",
    "            base_size = vol_sizing * conv_multiplier * liquidity_factor * signal_factor\n",
    "            \n",
    "            # Aplicar límite por régimen\n",
    "            max_size = self.position_limits[regime]\n",
    "            position_size = min(base_size * max_size, max_size)\n",
    "            \n",
    "            # Verificar costos de transacción\n",
    "            costs = self.calculate_transaction_costs(pair_info, position_size, data)\n",
    "            \n",
    "            # Si los costos son muy altos relativos al retorno esperado, reducir posición\n",
    "            expected_return = 0.01  # Retorno esperado por default (1%)\n",
    "            \n",
    "            # Estimar retorno esperado basado en z-score y half-life\n",
    "            if 'z_score' in pair_info and 'half_life' in pair_info:\n",
    "                z_score = pair_info.get('z_score', 2.0)\n",
    "                half_life = pair_info.get('half_life', 15)\n",
    "                \n",
    "                # Modelo simple: mayor z-score y menor half-life = mayor retorno esperado\n",
    "                expected_return = min(abs(z_score) * 0.01, 0.03) * (10 / max(half_life, 5))\n",
    "            \n",
    "            # Ajustar por probabilidad de convergencia\n",
    "            expected_return *= conv_probability\n",
    "            \n",
    "            # Si los costos son más del 25% del retorno esperado, reducir proporcionalmente\n",
    "            if costs['total_pct'] > 0.25 * expected_return:\n",
    "                cost_adjustment = (expected_return / (4 * costs['total_pct']))\n",
    "                position_size *= cost_adjustment\n",
    "                logging.info(f\"Posición reducida por costos altos: {cost_adjustment:.2f} x tamaño\")\n",
    "            \n",
    "            # Verificar restricción de ADV\n",
    "            if 'min_adv' in costs and costs['min_adv'] > 0:\n",
    "                max_by_adv = self.liquidity_params['max_position_pct_adv'] * costs['min_adv'] / 1e6\n",
    "                if position_size > max_by_adv:\n",
    "                    position_size = max_by_adv\n",
    "                    logging.info(f\"Posición limitada por ADV a {position_size:.4f}\")\n",
    "            \n",
    "            logging.info(f\"Tamaño calculado para {ticker1}-{ticker2}: {position_size:.4f}\")\n",
    "            return position_size\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en calculate_position_size: {str(e)}\")\n",
    "            logging.error(traceback.format_exc())\n",
    "            return 0\n",
    "    \n",
    "    def check_liquidity_constraint(self, pair_info, data):\n",
    "        \"\"\"Verifica restricciones de liquidez y capacidad para un par\"\"\"\n",
    "        try:\n",
    "            ticker1 = pair_info['ticker1']\n",
    "            ticker2 = pair_info['ticker2']\n",
    "            \n",
    "            # Obtener ADV en dólares\n",
    "            ticker1_adv = data['adv'].iloc[-5:][ticker1].mean() if ticker1 in data['adv'].columns else 0\n",
    "            ticker2_adv = data['adv'].iloc[-5:][ticker2].mean() if ticker2 in data['adv'].columns else 0\n",
    "            \n",
    "            # Verificar datos\n",
    "            if ticker1_adv == 0 or ticker2_adv == 0:\n",
    "                # Intentar calcular con volumen * precio\n",
    "                if 'volumes' in data and 'prices' in data:\n",
    "                    if ticker1_adv == 0 and ticker1 in data['volumes'].columns and ticker1 in data['prices'].columns:\n",
    "                        vol = data['volumes'].iloc[-5:][ticker1].mean()\n",
    "                        price = data['prices'].iloc[-1][ticker1]\n",
    "                        ticker1_adv = vol * price\n",
    "                    \n",
    "                    if ticker2_adv == 0 and ticker2 in data['volumes'].columns and ticker2 in data['prices'].columns:\n",
    "                        vol = data['volumes'].iloc[-5:][ticker2].mean()\n",
    "                        price = data['prices'].iloc[-1][ticker2]\n",
    "                        ticker2_adv = vol * price\n",
    "            \n",
    "            # Si aún no hay datos, usar valor por defecto bajo\n",
    "            if ticker1_adv == 0:\n",
    "                ticker1_adv = self.liquidity_params['min_adv_usd'] / 2\n",
    "            if ticker2_adv == 0:\n",
    "                ticker2_adv = self.liquidity_params['min_adv_usd'] / 2\n",
    "            \n",
    "            # Retornar el mínimo (el activo menos líquido determina la capacidad)\n",
    "            return min(ticker1_adv, ticker2_adv)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en check_liquidity_constraint: {str(e)}\")\n",
    "            return self.liquidity_params['min_adv_usd'] / 2  # Valor por defecto conservador\n",
    "    \n",
    "    def calculate_stop_loss(self, data, pair_info, regime, entry_price, position_type):\n",
    "        \"\"\"Calcula nivel de stop-loss con ATR adaptativo\"\"\"\n",
    "        try:\n",
    "            ticker1 = pair_info['ticker1']\n",
    "            ticker2 = pair_info['ticker2']\n",
    "            hedge_ratio = pair_info['hedge_ratio']\n",
    "            \n",
    "            # Verificar datos\n",
    "            if ticker1 not in data['prices'].columns or ticker2 not in data['prices'].columns:\n",
    "                logging.warning(f\"Tickers {ticker1} o {ticker2} no disponibles para stop-loss\")\n",
    "                # Usar valor por defecto\n",
    "                return entry_price * (0.95 if position_type == 1 else 1.05)\n",
    "            \n",
    "            # Calcular spread con ventana adaptativa\n",
    "            lookback = min(60, len(data['prices']) - 1)\n",
    "            lookback = max(lookback, 14)  # Mínimo 14 días para ATR decente\n",
    "            \n",
    "            prices = data['prices'].iloc[-lookback:][[ticker1, ticker2]]\n",
    "            \n",
    "            # Verificar datos suficientes\n",
    "            if len(prices) < 10 or prices[ticker1].isna().any() or prices[ticker2].isna().any():\n",
    "                logging.warning(f\"Datos insuficientes para ATR de {ticker1}-{ticker2}\")\n",
    "                # Usar valor por defecto basado en régimen\n",
    "                default_stop_pct = 0.05 if regime == 1 else 0.04 if regime == 2 else 0.03\n",
    "                return entry_price * (1 - default_stop_pct) if position_type == 1 else entry_price * (1 + default_stop_pct)\n",
    "            \n",
    "            # Calcular spread\n",
    "            spread = prices[ticker1] + hedge_ratio * prices[ticker2]\n",
    "            \n",
    "            # Calcular ATR del spread\n",
    "            spread_high = spread.rolling(window=2).max()\n",
    "            spread_low = spread.rolling(window=2).min()\n",
    "            tr = spread_high - spread_low\n",
    "            \n",
    "            # Usar ventana adaptativa para ATR\n",
    "            atr_window = min(14, len(tr) // 3)\n",
    "            atr = tr.rolling(window=atr_window).mean().iloc[-1]\n",
    "            \n",
    "            # Verificar valor válido\n",
    "            if pd.isna(atr) or atr == 0:\n",
    "                atr = spread.std()\n",
    "                \n",
    "            # Si aún es inválido, usar % del precio\n",
    "            if pd.isna(atr) or atr == 0:\n",
    "                atr = abs(entry_price) * 0.02  # 2% por defecto\n",
    "                \n",
    "            # Aplicar factor según régimen\n",
    "            atr_factor = self.atr_factors[regime]\n",
    "            stop_distance = atr * atr_factor\n",
    "            \n",
    "            # Calcular stop-loss\n",
    "            if position_type == 1:  # Long\n",
    "                stop_loss = entry_price - stop_distance\n",
    "            else:  # Short\n",
    "                stop_loss = entry_price + stop_distance\n",
    "                \n",
    "            # Verificar que el stop no está demasiado cerca\n",
    "            min_distance_pct = 0.01  # Mínimo 1% de distancia\n",
    "            min_distance = abs(entry_price) * min_distance_pct\n",
    "            \n",
    "            if abs(stop_loss - entry_price) < min_distance:\n",
    "                # Ajustar stop para tener distancia mínima\n",
    "                stop_loss = entry_price - min_distance if position_type == 1 else entry_price + min_distance\n",
    "                logging.info(f\"Stop-loss ajustado a distancia mínima para {ticker1}-{ticker2}\")\n",
    "            \n",
    "            logging.info(f\"Stop-loss para {ticker1}-{ticker2}: {stop_loss:.4f}, distancia: {abs(stop_loss-entry_price)/abs(entry_price)*100:.2f}%\")\n",
    "            return stop_loss\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en calculate_stop_loss: {str(e)}\")\n",
    "            logging.error(traceback.format_exc())\n",
    "            # Valor por defecto en caso de error (5% de distancia)\n",
    "            return entry_price * 0.95 if position_type == 1 else entry_price * 1.05\n",
    "    \n",
    "    def calculate_time_stop(self, pair_info):\n",
    "        \"\"\"Calcula stop temporal basado en half-life con límites razonables\"\"\"\n",
    "        try:\n",
    "            # Si no hay half-life disponible, usar valor por defecto\n",
    "            if 'half_life' not in pair_info or pair_info['half_life'] in (None, np.inf, np.nan):\n",
    "                return 15  # Default: 15 días\n",
    "            \n",
    "            half_life = pair_info['half_life']\n",
    "            \n",
    "            # Verificar valor razonable\n",
    "            if half_life <= 0:\n",
    "                logging.warning(f\"Half-life inválido: {half_life}, usando valor por defecto\")\n",
    "                return 15\n",
    "            \n",
    "            # Tiempo objetivo: 2-3x half-life con límites razonables\n",
    "            time_stop = int(2.5 * half_life)\n",
    "            \n",
    "            # Limitar entre 5 y 30 días\n",
    "            time_stop = max(5, min(time_stop, 30))\n",
    "            \n",
    "            return time_stop\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en calculate_time_stop: {str(e)}\")\n",
    "            return 15  # Valor por defecto\n",
    "    \n",
    "    def check_circuit_breakers(self, data, sector_map, current_date=None):\n",
    "        \"\"\"Comprueba activación de circuit breakers con verificaciones mejoradas\"\"\"\n",
    "        try:\n",
    "            # Fecha actual\n",
    "            if current_date is None:\n",
    "                current_date = data['prices'].index[-1] if not data['prices'].empty else datetime.now()\n",
    "            \n",
    "            # Verificar si ya hicimos check hoy\n",
    "            if self.last_risk_check and self.last_risk_check.date() == current_date.date():\n",
    "                logging.info(\"Circuit breakers ya verificados hoy\")\n",
    "                return 1.0  # No re-verificar en la misma fecha\n",
    "            \n",
    "            # Crear proxy de VIX con volatilidad realizada\n",
    "            vix_proxy = None\n",
    "            \n",
    "            # Intentar con datos de VIX si están disponibles\n",
    "            if '^VIX' in data['prices'].columns:\n",
    "                vix_data = data['prices']['^VIX']\n",
    "                if not vix_data.empty:\n",
    "                    vix_proxy = vix_data\n",
    "                    logging.info(\"Usando datos de VIX directos\")\n",
    "            \n",
    "            # Si no hay VIX, usar volatilidad realizada\n",
    "            if vix_proxy is None or vix_proxy.empty:\n",
    "                if 'returns' in data and not data['returns'].empty:\n",
    "                    returns = data['returns'].mean(axis=1)\n",
    "                    # Ventana adaptativa\n",
    "                    vol_window = min(22, len(returns) // 5)\n",
    "                    vix_proxy = returns.rolling(window=vol_window).std() * np.sqrt(252) * 100\n",
    "                    logging.info(f\"Usando proxy de VIX con volatilidad a {vol_window} días\")\n",
    "                else:\n",
    "                    logging.warning(\"Sin datos para cálculo de VIX\")\n",
    "                    return 1.0  # Sin datos suficientes\n",
    "            \n",
    "            # Verificar datos suficientes\n",
    "            if len(vix_proxy) < 60:\n",
    "                logging.warning(f\"Datos insuficientes para VIX: {len(vix_proxy)} < 60\")\n",
    "                # Si hay muy pocos datos, ser conservador\n",
    "                if len(vix_proxy) < 10:\n",
    "                    return 0.8  # Reducción preventiva de 20%\n",
    "            \n",
    "            # Percentiles históricos con ventana adaptativa\n",
    "            lookback = min(252 * 2, len(vix_proxy))\n",
    "            vix_history = vix_proxy.iloc[-lookback:]\n",
    "            \n",
    "            vix_75 = vix_history.quantile(0.75)\n",
    "            vix_85 = vix_history.quantile(0.85)\n",
    "            vix_90 = vix_history.quantile(0.90)\n",
    "            \n",
    "            current_vix = vix_proxy.iloc[-1]\n",
    "            vix_1d_change = 0\n",
    "            \n",
    "            if len(vix_proxy) > 1:\n",
    "                vix_1d_change = current_vix / vix_proxy.iloc[-2] - 1\n",
    "            \n",
    "            # Correlación promedio para medir estrés sistémico\n",
    "            avg_correlation = None\n",
    "            \n",
    "            if 'returns' in data and not data['returns'].empty:\n",
    "                returns_window = min(20, len(data['returns']) // 5)\n",
    "                returns = data['returns'].iloc[-returns_window:]\n",
    "                \n",
    "                if len(returns) > 5 and returns.shape[1] > 5:\n",
    "                    # Calcular matriz de correlación\n",
    "                    corr_matrix = returns.corr()\n",
    "                    \n",
    "                    # Usar solo el triángulo superior sin la diagonal\n",
    "                    mask = np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "                    avg_correlation = corr_matrix.values[mask].mean()\n",
    "                    \n",
    "                    logging.info(f\"Correlación media actual: {avg_correlation:.3f}\")\n",
    "            \n",
    "            # Verificar niveles de circuit breaker\n",
    "            message = \"\"\n",
    "            \n",
    "            # Nivel 3 (el más severo)\n",
    "            if (current_vix > vix_90 and \n",
    "                avg_correlation is not None and avg_correlation > self.circuit_breakers['level3']['correlation']):\n",
    "                circuit_level = 1.0 - self.circuit_breakers['level3']['reduction']\n",
    "                message = f\"CIRCUIT BREAKER NIVEL 3: VIX {current_vix:.1f} > {vix_90:.1f} (p90) y correlación {avg_correlation:.3f} > {self.circuit_breakers['level3']['correlation']:.3f}\"\n",
    "            \n",
    "            # Nivel 2\n",
    "            elif (current_vix > vix_85 or \n",
    "                 (avg_correlation is not None and avg_correlation > self.circuit_breakers['level2']['correlation']) or\n",
    "                 (vix_1d_change > self.circuit_breakers['level2']['vix_increase'])):\n",
    "                circuit_level = 1.0 - self.circuit_breakers['level2']['reduction']\n",
    "                message = f\"CIRCUIT BREAKER NIVEL 2: VIX {current_vix:.1f} > {vix_85:.1f} (p85)\"\n",
    "                if avg_correlation is not None and avg_correlation > self.circuit_breakers['level2']['correlation']:\n",
    "                    message += f\" o correlación {avg_correlation:.3f} > {self.circuit_breakers['level2']['correlation']:.3f}\"\n",
    "                if vix_1d_change > self.circuit_breakers['level2']['vix_increase']:\n",
    "                    message += f\" o aumento de VIX {vix_1d_change*100:.1f}% > {self.circuit_breakers['level2']['vix_increase']*100:.1f}%\"\n",
    "            \n",
    "            # Nivel 1\n",
    "            elif (current_vix > vix_75 and \n",
    "                  vix_1d_change > self.circuit_breakers['level1']['vix_increase']):\n",
    "                circuit_level = 1.0 - self.circuit_breakers['level1']['reduction']\n",
    "                message = f\"CIRCUIT BREAKER NIVEL 1: VIX {current_vix:.1f} > {vix_75:.1f} (p75) y aumento {vix_1d_change*100:.1f}% > {self.circuit_breakers['level1']['vix_increase']*100:.1f}%\"\n",
    "            \n",
    "            else:\n",
    "                circuit_level = 1.0\n",
    "                message = \"Sin circuit breakers activos\"\n",
    "            \n",
    "            logging.info(message)\n",
    "            \n",
    "            # Registrar último check\n",
    "            self.last_risk_check = current_date\n",
    "            \n",
    "            return circuit_level\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en check_circuit_breakers: {str(e)}\")\n",
    "            logging.error(traceback.format_exc())\n",
    "            return 0.7  # Valor conservador en caso de error (30% reducción)\n",
    "    \n",
    "    def check_sector_concentration(self, new_positions, sector_map, regime):\n",
    "        \"\"\"Verifica límites de concentración por sector con mejor manejo de nulos\"\"\"\n",
    "        try:\n",
    "            # Si no hay posiciones, no hay nada que verificar\n",
    "            if not new_positions:\n",
    "                return {}\n",
    "                \n",
    "            # Calcular exposición por sector\n",
    "            sector_exposure = {}\n",
    "            sector_positions = {}  # Para tracking detallado\n",
    "            \n",
    "            for pair_id, position in new_positions.items():\n",
    "                ticker1, ticker2 = pair_id.split('_')\n",
    "                sector1 = sector_map.get(ticker1)\n",
    "                sector2 = sector_map.get(ticker2)\n",
    "                \n",
    "                if not sector1:\n",
    "                    logging.warning(f\"Sector desconocido para {ticker1}, usando 'Unknown'\")\n",
    "                    sector1 = 'Unknown'\n",
    "                    \n",
    "                if not sector2:\n",
    "                    logging.warning(f\"Sector desconocido para {ticker2}, usando 'Unknown'\")\n",
    "                    sector2 = 'Unknown'\n",
    "                \n",
    "                size = abs(position['size'])\n",
    "                \n",
    "                # Asignar 50% de exposición a cada sector\n",
    "                for sector in (sector1, sector2):\n",
    "                    if sector not in sector_exposure:\n",
    "                        sector_exposure[sector] = 0\n",
    "                        sector_positions[sector] = []\n",
    "                        \n",
    "                    sector_exposure[sector] += size / 2\n",
    "                    sector_positions[sector].append({\n",
    "                        'pair_id': pair_id,\n",
    "                        'exposure': size / 2\n",
    "                    })\n",
    "            \n",
    "            # Verificar límites\n",
    "            sector_limit = self.sector_limits[regime]\n",
    "            adjustment_needed = False\n",
    "            \n",
    "            # Identificar sectores que exceden límites\n",
    "            exceeding_sectors = []\n",
    "            for sector, exposure in sector_exposure.items():\n",
    "                if exposure > sector_limit:\n",
    "                    adjustment_needed = True\n",
    "                    exceeding_sectors.append({\n",
    "                        'sector': sector,\n",
    "                        'exposure': exposure,\n",
    "                        'excess': exposure - sector_limit\n",
    "                    })\n",
    "                    logging.warning(f\"Sector {sector} excede límite: {exposure:.3f} > {sector_limit:.3f}\")\n",
    "            \n",
    "            if not adjustment_needed:\n",
    "                logging.info(\"Concentración sectorial dentro de límites\")\n",
    "                return new_positions\n",
    "            \n",
    "            # Ordenar por exceso para ajustar primero los más excedidos\n",
    "            exceeding_sectors.sort(key=lambda x: x['excess'], reverse=True)\n",
    "            \n",
    "            # Calcular factores de escala por sector\n",
    "            sector_scales = {}\n",
    "            for sector, exposure in sector_exposure.items():\n",
    "                if exposure > sector_limit:\n",
    "                    sector_scales[sector] = sector_limit / exposure\n",
    "                else:\n",
    "                    sector_scales[sector] = 1.0\n",
    "            \n",
    "            # Ajustar posiciones (versión mejorada)\n",
    "            adjusted_positions = {}\n",
    "            \n",
    "            for pair_id, position in new_positions.items():\n",
    "                ticker1, ticker2 = pair_id.split('_')\n",
    "                sector1 = sector_map.get(ticker1, 'Unknown')\n",
    "                sector2 = sector_map.get(ticker2, 'Unknown')\n",
    "                \n",
    "                # Determinar factor de escala basado en el sector más restrictivo\n",
    "                scale1 = sector_scales.get(sector1, 1.0)\n",
    "                scale2 = sector_scales.get(sector2, 1.0)\n",
    "                scale = min(scale1, scale2)\n",
    "                \n",
    "                # Aplicar escala solo a posiciones en sectores excedidos\n",
    "                if scale < 1.0:\n",
    "                    adjusted_position = position.copy()\n",
    "                    adjusted_position['size'] *= scale\n",
    "                    adjusted_position['original_size'] = position['size']  # Guardar original para referencia\n",
    "                    adjusted_position['scale_factor'] = scale\n",
    "                    adjusted_position['scale_reason'] = f\"Límite sectorial: {sector1 if scale1 < scale2 else sector2}\"\n",
    "                    adjusted_positions[pair_id] = adjusted_position\n",
    "                    \n",
    "                    logging.info(f\"Posición {pair_id} ajustada por concentración sectorial: {position['size']:.4f} -> {adjusted_position['size']:.4f}\")\n",
    "                else:\n",
    "                    # Sin ajuste necesario\n",
    "                    adjusted_positions[pair_id] = position\n",
    "            \n",
    "            return adjusted_positions\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en check_sector_concentration: {str(e)}\")\n",
    "            logging.error(traceback.format_exc())\n",
    "            # En caso de error, mejor mantener posiciones originales\n",
    "            return new_positions\n",
    "    \n",
    "    def optimize_portfolio(self, data, pairs_with_signals, regime, sector_map, current_date=None):\n",
    "        \"\"\"Optimiza el portfolio basado en señales y restricciones con mejor control de riesgo\"\"\"\n",
    "        try:\n",
    "            # Fecha actual\n",
    "            if current_date is None:\n",
    "                current_date = data['prices'].index[-1] if not data['prices'].empty else datetime.now()\n",
    "                \n",
    "            # Verificar datos suficientes\n",
    "            if data['prices'].empty:\n",
    "                logging.error(\"Datos insuficientes para optimización\")\n",
    "                return {}\n",
    "                \n",
    "            logging.info(f\"Optimizando portfolio para régimen {regime} con {len(pairs_with_signals)} pares\")\n",
    "            \n",
    "            new_positions = {}\n",
    "            \n",
    "            # Calcular volatilidad de cada par\n",
    "            pair_volatilities = {}\n",
    "            market_conditions = {}\n",
    "            \n",
    "            # Obtener volatilidad de mercado para comparación\n",
    "            if 'realized_vol' in data and not data['realized_vol'].empty:\n",
    "                # Usar índice de mercado si está disponible\n",
    "                if '^GSPC' in data['realized_vol'].columns:\n",
    "                    mkt_vol = data['realized_vol']['^GSPC'].iloc[-20:].mean()\n",
    "                # O promedio de volatilidades\n",
    "                else:\n",
    "                    mkt_vol = data['realized_vol'].iloc[-20:].mean().mean()\n",
    "                    \n",
    "                # Comparar con volatilidad histórica para detectar cambios\n",
    "                if len(data['realized_vol']) > 60:\n",
    "                    prev_mkt_vol = data['realized_vol'].iloc[-60:-20].mean().mean()\n",
    "                    vol_increase = mkt_vol / prev_mkt_vol - 1 if prev_mkt_vol > 0 else 0\n",
    "                    market_conditions['volatility_increase'] = vol_increase\n",
    "                    \n",
    "                    if vol_increase > 0.25:  # 25% aumento\n",
    "                        market_conditions['high_volatility'] = True\n",
    "                        logging.warning(f\"Alta volatilidad detectada: +{vol_increase*100:.1f}%\")\n",
    "                    else:\n",
    "                        market_conditions['high_volatility'] = False\n",
    "            \n",
    "            # Calcular volatilidad de pares con ventana adaptativa\n",
    "            for pair_info in pairs_with_signals:\n",
    "                ticker1 = pair_info['ticker1']\n",
    "                ticker2 = pair_info['ticker2']\n",
    "                hedge_ratio = pair_info['hedge_ratio']\n",
    "                \n",
    "                lookback = min(60, len(data['prices']) - 1)\n",
    "                prices = data['prices'].iloc[-lookback:][[ticker1, ticker2]]\n",
    "                \n",
    "                if len(prices) < 20 or prices[ticker1].isna().any() or prices[ticker2].isna().any():\n",
    "                    logging.warning(f\"Datos insuficientes para volatilidad de {ticker1}-{ticker2}\")\n",
    "                    # Usar valor por defecto alto (conservador)\n",
    "                    pair_volatilities[f\"{ticker1}_{ticker2}\"] = 0.03  # 3% diaria = ~48% anualizada\n",
    "                    continue\n",
    "                    \n",
    "                spread = prices[ticker1] + hedge_ratio * prices[ticker2]\n",
    "                \n",
    "                # Usar retorno logarítmico para mejor precisión\n",
    "                log_returns = np.log(spread / spread.shift(1)).dropna()\n",
    "                \n",
    "                if len(log_returns) < 5:\n",
    "                    volatility = 0.03  # Valor por defecto\n",
    "                else:\n",
    "                    # Anualizar volatilidad (√252 días)\n",
    "                    volatility = log_returns.std() * np.sqrt(252)\n",
    "                    \n",
    "                    # Verificar valor razonable\n",
    "                    if volatility < 0.05:  # Menos de 5% anual es sospechosamente bajo\n",
    "                        volatility = 0.05\n",
    "                    elif volatility > 0.80:  # Más de 80% anual es extremadamente alto\n",
    "                        volatility = 0.80\n",
    "                \n",
    "                pair_id = f\"{ticker1}_{ticker2}\"\n",
    "                pair_volatilities[pair_id] = volatility\n",
    "            \n",
    "            # Ordenar pares por fuerza de señal * probabilidad * (1/volatilidad)\n",
    "            sorted_pairs = sorted(\n",
    "                pairs_with_signals, \n",
    "                key=lambda x: abs(x['signal']['signal']) * x['signal']['strength'] * x['conv_probability'] / (pair_volatilities.get(f\"{x['ticker1']}_{x['ticker2']}\", 0.3) + 0.05),\n",
    "                reverse=True\n",
    "            )\n",
    "            \n",
    "            # Asignar posiciones\n",
    "            total_risk = 0\n",
    "            \n",
    "            for pair_info in sorted_pairs:\n",
    "                ticker1 = pair_info['ticker1']\n",
    "                ticker2 = pair_info['ticker2']\n",
    "                hedge_ratio = pair_info['hedge_ratio']\n",
    "                signal = pair_info['signal']['signal']\n",
    "                strength = pair_info['signal']['strength']\n",
    "                z_score = pair_info['signal']['z_score']\n",
    "                conv_probability = pair_info['conv_probability']\n",
    "                \n",
    "                pair_id = f\"{ticker1}_{ticker2}\"\n",
    "                \n",
    "                if signal == 0 or pair_id not in pair_volatilities:\n",
    "                    logging.info(f\"Sin señal para {pair_id} o volatilidad no disponible\")\n",
    "                    continue\n",
    "                \n",
    "                # Calcular tamaño con todos los factores\n",
    "                volatility = pair_volatilities[pair_id]\n",
    "                position_size = self.calculate_position_size(\n",
    "                    pair_info, strength, regime, conv_probability, volatility, data\n",
    "                )\n",
    "                \n",
    "                # Si el tamaño es demasiado pequeño, ignorar\n",
    "                min_size = 0.003  # 0.3% mínimo para evitar posiciones insignificantes\n",
    "                if position_size < min_size:\n",
    "                    logging.info(f\"Posición demasiado pequeña para {pair_id}: {position_size:.4f} < {min_size:.4f}\")\n",
    "                    continue\n",
    "                \n",
    "                # Ajustar signo\n",
    "                signed_position = position_size * np.sign(signal)\n",
    "                \n",
    "                # Simular precio de entrada\n",
    "                entry_price = 1.0  # Normalizado\n",
    "                \n",
    "                # Calcular stops\n",
    "                stop_loss = self.calculate_stop_loss(\n",
    "                    data, pair_info, regime, entry_price, np.sign(signal)\n",
    "                )\n",
    "                \n",
    "                time_stop = self.calculate_time_stop(pair_info)\n",
    "                \n",
    "                # Guardar posición\n",
    "                new_positions[pair_id] = {\n",
    "                    'ticker1': ticker1,\n",
    "                    'ticker2': ticker2,\n",
    "                    'hedge_ratio': hedge_ratio,\n",
    "                    'size': signed_position,\n",
    "                    'signal': signal,\n",
    "                    'strength': strength,\n",
    "                    'z_score': z_score,\n",
    "                    'entry_price': entry_price,\n",
    "                    'stop_loss': stop_loss,\n",
    "                    'time_stop': time_stop,\n",
    "                    'entry_date': current_date,\n",
    "                    'days_held': 0,\n",
    "                    'half_life': pair_info.get('half_life', 15),\n",
    "                    'volatility': volatility,\n",
    "                    'costs': self.calculate_transaction_costs(pair_info, position_size, data)\n",
    "                }\n",
    "                \n",
    "                # Acumular riesgo (volatilidad * tamaño)\n",
    "                total_risk += abs(signed_position) * volatility\n",
    "            \n",
    "            # Si no hay posiciones, retornar diccionario vacío\n",
    "            if not new_positions:\n",
    "                logging.warning(\"No se generaron posiciones nuevas\")\n",
    "                return {}\n",
    "            \n",
    "            # Verificar concentración sectorial\n",
    "            adjusted_positions = self.check_sector_concentration(new_positions, sector_map, regime)\n",
    "            \n",
    "            # Recalcular riesgo total después de ajuste sectorial\n",
    "            if adjusted_positions:\n",
    "                total_risk = sum(abs(pos['size']) * pair_volatilities.get(pair_id, 0.3) \n",
    "                                for pair_id, pos in adjusted_positions.items())\n",
    "            \n",
    "            # Escalar a volatilidad objetivo\n",
    "            target_vol = self.vol_targets[regime]\n",
    "            \n",
    "            if total_risk > 0 and total_risk != target_vol:\n",
    "                vol_scale = target_vol / total_risk\n",
    "                \n",
    "                # Limitar factor de escala para evitar cambios extremos\n",
    "                vol_scale = max(0.5, min(vol_scale, 2.0))\n",
    "                \n",
    "                logging.info(f\"Escalando portfolio a volatilidad objetivo {target_vol:.1%}: factor {vol_scale:.2f}\")\n",
    "                \n",
    "                for pair_id in adjusted_positions:\n",
    "                    adjusted_positions[pair_id]['size'] *= vol_scale\n",
    "                    adjusted_positions[pair_id]['vol_scale_factor'] = vol_scale\n",
    "            \n",
    "            # Aplicar circuit breakers\n",
    "            circuit_breaker_scale = self.check_circuit_breakers(data, sector_map, current_date)\n",
    "            \n",
    "            if circuit_breaker_scale < 1.0:\n",
    "                logging.warning(f\"Circuit breaker activado: {circuit_breaker_scale:.2f}\")\n",
    "                \n",
    "                for pair_id in adjusted_positions:\n",
    "                    adjusted_positions[pair_id]['size'] *= circuit_breaker_scale\n",
    "                    adjusted_positions[pair_id]['circuit_breaker_factor'] = circuit_breaker_scale\n",
    "            \n",
    "            # Registrar estado final\n",
    "            self.current_positions = adjusted_positions\n",
    "            \n",
    "            # Guardar snapshot para historia\n",
    "            position_snapshot = {\n",
    "                'date': current_date,\n",
    "                'regime': regime,\n",
    "                'positions': {k: v.copy() for k, v in adjusted_positions.items()},\n",
    "                'total_risk': total_risk,\n",
    "                'target_vol': target_vol,\n",
    "                'circuit_breaker': circuit_breaker_scale,\n",
    "                'market_conditions': market_conditions\n",
    "            }\n",
    "            self.position_history.append(position_snapshot)\n",
    "            \n",
    "            total_positions = len(adjusted_positions)\n",
    "            total_exposure = sum(abs(pos['size']) for pos in adjusted_positions.values())\n",
    "            \n",
    "            logging.info(f\"Portfolio optimizado: {total_positions} posiciones, exposición total {total_exposure:.2f}\")\n",
    "            \n",
    "            return adjusted_positions\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en optimize_portfolio: {str(e)}\")\n",
    "            logging.error(traceback.format_exc())\n",
    "            # En caso de error grave, mejor mantener posiciones anteriores\n",
    "            return self.current_positions\n",
    "\n",
    "# Estrategia completa\n",
    "class StatArbStrategy:\n",
    "    \"\"\"Implementación completa y mejorada de la estrategia de arbitraje estadístico\"\"\"\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        # Configuración por defecto\n",
    "        self.default_config = {\n",
    "            'min_data_years': 5,\n",
    "            'max_pairs_per_regime': {1: 25, 2: 20, 3: 15},\n",
    "            'min_liquidity': 10e6,\n",
    "            'recalibration_days': 5,\n",
    "            'transaction_costs': True,\n",
    "            'include_market_impact': True,\n",
    "            'circuit_breakers_enabled': True\n",
    "        }\n",
    "        \n",
    "        # Usar config proporcionada o valores por defecto\n",
    "        self.config = config if config is not None else self.default_config\n",
    "        \n",
    "        # Inicializar componentes con parámetros mejorados\n",
    "        self.regime_detector = RegimeDetector(\n",
    "            min_train_samples=min(252 * self.config.get('min_data_years', 5), 1260)\n",
    "        )\n",
    "        \n",
    "        self.pair_selector = PairSelector(\n",
    "            min_liquidity=self.config.get('min_liquidity', 10e6),\n",
    "            max_pairs_by_regime=self.config.get('max_pairs_per_regime', {1: 25, 2: 20, 3: 15}),\n",
    "            recalibration_days=self.config.get('recalibration_days', 5)\n",
    "        )\n",
    "        \n",
    "        self.convergence_predictor = ConvergencePredictor()\n",
    "        self.signal_generator = SignalGenerator()\n",
    "        self.position_manager = PositionManager()\n",
    "        \n",
    "        # Estado de la estrategia\n",
    "        self.current_regime = None\n",
    "        self.previous_regime = None\n",
    "        self.selected_pairs = []\n",
    "        self.current_positions = {}\n",
    "        self.positions_history = []\n",
    "        self.regime_history = []\n",
    "        self.recalibration_history = []\n",
    "        \n",
    "        # Resultados de backtesting\n",
    "        self.equity_curve = None\n",
    "        self.trades_log = None\n",
    "        \n",
    "        # Métricas de rendimiento\n",
    "        self.metrics = {\n",
    "            'overall': {\n",
    "                'sharpe_ratio': None,\n",
    "                'sortino_ratio': None,\n",
    "                'max_drawdown': None,\n",
    "                'annual_return': None,\n",
    "                'volatility': None,\n",
    "                'win_rate': None,\n",
    "                'avg_trade_duration': None,\n",
    "                'avg_profit_per_trade': None,\n",
    "                'profit_factor': None\n",
    "            },\n",
    "            'by_regime': {\n",
    "                1: {}, 2: {}, 3: {}\n",
    "            },\n",
    "            'by_sector': {},\n",
    "            'by_period': {}\n",
    "        }\n",
    "        \n",
    "        # Parámetros de costos\n",
    "        self.trading_costs = {\n",
    "            'commission': 0.0005,  # 5 bps (0.05%)\n",
    "            'slippage': 0.0003,    # 3 bps (0.03%)\n",
    "            'market_impact': 0.0002  # 2 bps (0.02%) base\n",
    "        }\n",
    "        \n",
    "        # Historial de métricas para monitoreo\n",
    "        self.metrics_history = []\n",
    "        \n",
    "        # Registro de inicialización\n",
    "        logging.info(\"Estrategia de arbitraje estadístico inicializada con configuración:\")\n",
    "        for k, v in self.config.items():\n",
    "            logging.info(f\"  {k}: {v}\")\n",
    "    \n",
    "    def initialize(self, data, sector_map, subsector_map, calibration_date=None):\n",
    "        \"\"\"Inicializa la estrategia con datos históricos y verificaciones de datos\"\"\"\n",
    "        try:\n",
    "            # Verificar datos suficientes\n",
    "            if 'prices' not in data or data['prices'].empty:\n",
    "                raise ValueError(\"Datos de precios insuficientes o no proporcionados\")\n",
    "                \n",
    "            # Verificar fecha de calibración válida\n",
    "            if calibration_date is None:\n",
    "                # Usar último día disponible en datos\n",
    "                calibration_date = data['prices'].index[-1]\n",
    "                logging.info(f\"Usando fecha de calibración por defecto: {calibration_date}\")\n",
    "                \n",
    "            # Asegurar que fecha no es futura\n",
    "            current_date_time = datetime.now()\n",
    "            if calibration_date.date() > current_date_time.date():\n",
    "                logging.warning(f\"Fecha futura detectada: {calibration_date}, ajustando a {current_date_time}\")\n",
    "                calibration_date = current_date_time\n",
    "                \n",
    "            logging.info(f\"Inicializando estrategia en fecha: {calibration_date}\")\n",
    "            \n",
    "            # Verificar datos suficientes\n",
    "            min_days = 252 * self.config.get('min_data_years', 5)\n",
    "            if len(data['prices']) < min_days:\n",
    "                logging.warning(f\"Datos históricos insuficientes: {len(data['prices'])} < {min_days}\")\n",
    "                \n",
    "            # Filtrar datos hasta fecha de calibración\n",
    "            filtered_data = self._filter_data_until_date(data, calibration_date)\n",
    "            \n",
    "            # Detectar régimen inicial\n",
    "            logging.info(\"Detectando régimen inicial...\")\n",
    "            self.current_regime = self.regime_detector.fit_predict(filtered_data, sector_map, calibration_date)\n",
    "            self.previous_regime = self.current_regime\n",
    "            logging.info(f\"Régimen inicial: {self.current_regime}\")\n",
    "            \n",
    "            # Registrar en historial\n",
    "            self.regime_history.append({\n",
    "                'date': calibration_date,\n",
    "                'regime': self.current_regime\n",
    "            })\n",
    "            \n",
    "            # Generar pares candidatos\n",
    "            logging.info(\"Generando pares candidatos...\")\n",
    "            candidate_pairs = self.pair_selector.generate_candidate_pairs(\n",
    "                filtered_data, sector_map, subsector_map)\n",
    "            logging.info(f\"Pares candidatos generados: {len(candidate_pairs)}\")\n",
    "            \n",
    "            # Seleccionar pares óptimos\n",
    "            logging.info(f\"Seleccionando pares óptimos para régimen {self.current_regime}...\")\n",
    "            self.selected_pairs = self.pair_selector.select_pairs(\n",
    "                filtered_data, self.current_regime, candidate_pairs)\n",
    "            logging.info(f\"Pares seleccionados: {len(self.selected_pairs)}\")\n",
    "            \n",
    "            # Entrenar modelos predictivos\n",
    "            logging.info(\"Entrenando modelos predictivos...\")\n",
    "            training_success = self.convergence_predictor.train(\n",
    "                filtered_data, self.selected_pairs, self.current_regime)\n",
    "            logging.info(f\"Entrenamiento completado, éxito: {training_success}\")\n",
    "            \n",
    "            # Registro de inicialización\n",
    "            self.recalibration_history.append({\n",
    "                'date': calibration_date,\n",
    "                'regime': self.current_regime,\n",
    "                'pairs_count': len(self.selected_pairs),\n",
    "                'training_success': training_success,\n",
    "                'type': 'initialization'\n",
    "            })\n",
    "            \n",
    "            logging.info(\"Estrategia inicializada exitosamente\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en inicialización: {str(e)}\")\n",
    "            logging.error(traceback.format_exc())\n",
    "            return False\n",
    "    \n",
    "    def _filter_data_until_date(self, data, end_date):\n",
    "        \"\"\"Filtra los datos hasta una fecha específica para evitar look-ahead bias\"\"\"\n",
    "        filtered_data = {}\n",
    "        \n",
    "        for key, df in data.items():\n",
    "            if isinstance(df, pd.DataFrame) or isinstance(df, pd.Series):\n",
    "                # Verificar si el índice es de tipo datetime\n",
    "                if isinstance(df.index, pd.DatetimeIndex):\n",
    "                    filtered_data[key] = df[df.index <= end_date].copy()\n",
    "                else:\n",
    "                    # Si no es índice datetime, copiar igual\n",
    "                    filtered_data[key] = df.copy()\n",
    "            else:\n",
    "                # Si no es DataFrame ni Series, copiar igual\n",
    "                filtered_data[key] = df\n",
    "        \n",
    "        return filtered_data\n",
    "    \n",
    "    def update(self, data, sector_map, subsector_map, current_date=None):\n",
    "        \"\"\"Actualiza la estrategia con nuevos datos sin look-ahead bias\"\"\"\n",
    "        try:\n",
    "            # Verificar fecha actual\n",
    "            if current_date is None:\n",
    "                # Usar último día disponible en datos\n",
    "                if 'prices' in data and not data['prices'].empty:\n",
    "                    current_date = data['prices'].index[-1]\n",
    "                else:\n",
    "                    current_date = datetime.now()\n",
    "            \n",
    "            # Verificar datos hasta fecha actual\n",
    "            filtered_data = self._filter_data_until_date(data, current_date)\n",
    "            \n",
    "            # Verificar si tenemos estado inicial\n",
    "            if self.current_regime is None:\n",
    "                logging.warning(\"Estrategia no inicializada, ejecutando inicialización\")\n",
    "                self.initialize(filtered_data, sector_map, subsector_map, current_date)\n",
    "            \n",
    "            logging.info(f\"Actualizando estrategia para fecha: {current_date}\")\n",
    "            \n",
    "            # Guardar régimen anterior\n",
    "            self.previous_regime = self.current_regime\n",
    "            \n",
    "            # Actualizar régimen\n",
    "            self.current_regime = self.regime_detector.fit_predict(\n",
    "                filtered_data, sector_map, current_date)\n",
    "            \n",
    "            # Registrar historia\n",
    "            self.regime_history.append({\n",
    "                'date': current_date,\n",
    "                'regime': self.current_regime\n",
    "            })\n",
    "            \n",
    "            # Verificar recalibración\n",
    "            market_conditions = self._analyze_market_conditions(filtered_data)\n",
    "            needs_recalibration = self.pair_selector.check_recalibration_needed(\n",
    "                current_date, \n",
    "                self.current_regime, \n",
    "                self.previous_regime, \n",
    "                market_conditions\n",
    "            )\n",
    "            \n",
    "            # Recalibrar si es necesario\n",
    "            if needs_recalibration:\n",
    "                logging.info(f\"Recalibración necesaria en fecha {current_date}\")\n",
    "                \n",
    "                candidate_pairs = self.pair_selector.generate_candidate_pairs(\n",
    "                    filtered_data, sector_map, subsector_map)\n",
    "                \n",
    "                self.selected_pairs = self.pair_selector.select_pairs(\n",
    "                    filtered_data, self.current_regime, candidate_pairs, market_conditions)\n",
    "                \n",
    "                training_success = self.convergence_predictor.train(\n",
    "                    filtered_data, self.selected_pairs, self.current_regime)\n",
    "                \n",
    "                # Registrar recalibración\n",
    "                self.recalibration_history.append({\n",
    "                    'date': current_date,\n",
    "                    'regime': self.current_regime,\n",
    "                    'pairs_count': len(self.selected_pairs),\n",
    "                    'training_success': training_success,\n",
    "                    'type': 'regular',\n",
    "                    'market_conditions': market_conditions\n",
    "                })\n",
    "                \n",
    "                logging.info(f\"Recalibración completada: {len(self.selected_pairs)} pares seleccionados\")\n",
    "            \n",
    "            # Generar señales para los pares seleccionados\n",
    "            pairs_with_signals = []\n",
    "            \n",
    "            for pair_info in self.selected_pairs:\n",
    "                ticker1 = pair_info['ticker1']\n",
    "                ticker2 = pair_info['ticker2']\n",
    "                pair_id = f\"{ticker1}_{ticker2}\"\n",
    "                \n",
    "                # Determinar posición actual\n",
    "                current_position = 0\n",
    "                if pair_id in self.current_positions:\n",
    "                    current_position = np.sign(self.current_positions[pair_id]['size'])\n",
    "                \n",
    "                # Predecir convergencia\n",
    "                conv_probability = self.convergence_predictor.predict_convergence(\n",
    "                    filtered_data, pair_info, self.current_regime)\n",
    "                \n",
    "                # Generar señal\n",
    "                signal = self.signal_generator.generate_signal(\n",
    "                    filtered_data, pair_info, self.current_regime, \n",
    "                    current_position, conv_probability, current_date, market_conditions)\n",
    "                \n",
    "                if signal['z_score'] is not None:\n",
    "                    # Añadir información a pair_info para uso posterior\n",
    "                    pair_info_with_signal = pair_info.copy()\n",
    "                    pair_info_with_signal['signal'] = signal\n",
    "                    pair_info_with_signal['conv_probability'] = conv_probability\n",
    "                    pairs_with_signals.append(pair_info_with_signal)\n",
    "            \n",
    "            logging.info(f\"Señales generadas para {len(pairs_with_signals)} pares\")\n",
    "            \n",
    "            # Optimizar portfolio\n",
    "            updated_positions = self.position_manager.optimize_portfolio(\n",
    "                filtered_data, pairs_with_signals, self.current_regime, sector_map, current_date)\n",
    "            \n",
    "            # Actualizar posiciones\n",
    "            self.current_positions = updated_positions\n",
    "            \n",
    "            # Resumir cambios\n",
    "            changed_positions = self._summarize_position_changes(current_date)\n",
    "            \n",
    "            # Registrar estado\n",
    "            position_snapshot = {\n",
    "                'date': current_date,\n",
    "                'regime': self.current_regime,\n",
    "                'positions': {k: v.copy() for k, v in updated_positions.items()},\n",
    "                'market_conditions': market_conditions,\n",
    "                'changes': changed_positions\n",
    "            }\n",
    "            self.positions_history.append(position_snapshot)\n",
    "            \n",
    "            return updated_positions\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en update: {str(e)}\")\n",
    "            logging.error(traceback.format_exc())\n",
    "            # En caso de error, mantener posiciones actuales\n",
    "            return self.current_positions\n",
    "    \n",
    "    def _analyze_market_conditions(self, data):\n",
    "        \"\"\"Analiza condiciones actuales de mercado para toma de decisiones\"\"\"\n",
    "        conditions = {}\n",
    "        \n",
    "        try:\n",
    "            # Volatilidad de mercado\n",
    "            if 'realized_vol' in data and not data['realized_vol'].empty:\n",
    "                # Usar índice si disponible\n",
    "                if '^GSPC' in data['realized_vol'].columns:\n",
    "                    current_vol = data['realized_vol']['^GSPC'].iloc[-5:].mean()\n",
    "                    \n",
    "                    # Comparar con histórico\n",
    "                    if len(data['realized_vol']) > 60:\n",
    "                        hist_vol = data['realized_vol']['^GSPC'].iloc[-60:-5].mean()\n",
    "                        vol_change = current_vol / hist_vol - 1 if hist_vol > 0 else 0\n",
    "                        conditions['volatility_increase'] = vol_change\n",
    "                        conditions['high_volatility'] = vol_change > 0.2\n",
    "                        \n",
    "                # Usar promedio de volatilidades\n",
    "                else:\n",
    "                    current_vol = data['realized_vol'].iloc[-5:].mean().mean()\n",
    "                    \n",
    "                    # Comparar con histórico\n",
    "                    if len(data['realized_vol']) > 60:\n",
    "                        hist_vol = data['realized_vol'].iloc[-60:-5].mean().mean()\n",
    "                        vol_change = current_vol / hist_vol - 1 if hist_vol > 0 else 0\n",
    "                        conditions['volatility_increase'] = vol_change\n",
    "                        conditions['high_volatility'] = vol_change > 0.2\n",
    "            \n",
    "            # Correlación entre activos\n",
    "            if 'returns' in data and not data['returns'].empty:\n",
    "                # Usar últimos 20 días o menos si hay menos datos\n",
    "                window = min(20, len(data['returns']))\n",
    "                recent_returns = data['returns'].iloc[-window:]\n",
    "                \n",
    "                if recent_returns.shape[1] > 5:  # Al menos 5 activos\n",
    "                    corr_matrix = recent_returns.corr()\n",
    "                    # Usar triángulo superior sin diagonal\n",
    "                    mask = np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "                    avg_corr = corr_matrix.values[mask].mean()\n",
    "                    \n",
    "                    conditions['avg_correlation'] = avg_corr\n",
    "                    conditions['high_correlation'] = avg_corr > 0.6\n",
    "            \n",
    "            # Tendencia de mercado\n",
    "            if 'returns' in data and not data['returns'].empty:\n",
    "                if '^GSPC' in data['returns'].columns:\n",
    "                    mkt_returns = data['returns']['^GSPC']\n",
    "                    # Si no hay SPX, usar promedio\n",
    "                else:\n",
    "                    mkt_returns = data['returns'].mean(axis=1)\n",
    "                \n",
    "                # Calcular tendencia reciente (20 días)\n",
    "                window = min(20, len(mkt_returns))\n",
    "                recent_returns = mkt_returns.iloc[-window:]\n",
    "                cumulative_return = (1 + recent_returns).prod() - 1\n",
    "                \n",
    "                conditions['market_trend'] = cumulative_return\n",
    "                conditions['bull_market'] = cumulative_return > 0.03  # +3% en 20 días\n",
    "                conditions['bear_market'] = cumulative_return < -0.03  # -3% en 20 días\n",
    "            \n",
    "            # Condiciones de liquidez\n",
    "            if 'relative_volume' in data and not data['relative_volume'].empty:\n",
    "                avg_rel_vol = data['relative_volume'].iloc[-5:].mean().mean()\n",
    "                conditions['relative_volume'] = avg_rel_vol\n",
    "                conditions['high_volume'] = avg_rel_vol > 1.2\n",
    "                conditions['low_volume'] = avg_rel_vol < 0.8\n",
    "            \n",
    "            # Costos de transacción basados en condiciones\n",
    "            base_cost_bps = 5  # 5 bps base\n",
    "            \n",
    "            # Aumentar costos en alta volatilidad o correlación\n",
    "            if conditions.get('high_volatility', False):\n",
    "                cost_bps = base_cost_bps * 1.3  # +30%\n",
    "            elif conditions.get('high_correlation', False):\n",
    "                cost_bps = base_cost_bps * 1.2  # +20%\n",
    "            else:\n",
    "                cost_bps = base_cost_bps\n",
    "                \n",
    "            conditions['cost_bps'] = cost_bps\n",
    "            \n",
    "            return conditions\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en _analyze_market_conditions: {str(e)}\")\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def _summarize_position_changes(self, current_date):\n",
    "        \"\"\"Resume cambios en posiciones para tracking y análisis\"\"\"\n",
    "        changes = {\n",
    "            'opened': [],\n",
    "            'closed': [],\n",
    "            'modified': [],\n",
    "            'unchanged': []\n",
    "        }\n",
    "        \n",
    "        # Última posición histórica (anterior a la actual)\n",
    "        previous_positions = {}\n",
    "        if len(self.positions_history) > 1:\n",
    "            previous_snapshot = self.positions_history[-2]\n",
    "            previous_positions = previous_snapshot['positions']\n",
    "        \n",
    "        # Nuevas posiciones\n",
    "        for pair_id, position in self.current_positions.items():\n",
    "            if pair_id not in previous_positions:\n",
    "                changes['opened'].append({\n",
    "                    'pair_id': pair_id,\n",
    "                    'ticker1': position['ticker1'],\n",
    "                    'ticker2': position['ticker2'],\n",
    "                    'size': position['size'],\n",
    "                    'signal': position['signal'],\n",
    "                    'date': current_date\n",
    "                })\n",
    "            else:\n",
    "                # Verificar si cambió el tamaño\n",
    "                prev_size = previous_positions[pair_id]['size']\n",
    "                curr_size = position['size']\n",
    "                \n",
    "                if abs(prev_size - curr_size) > 0.001:  # Cambio significativo\n",
    "                    changes['modified'].append({\n",
    "                        'pair_id': pair_id,\n",
    "                        'ticker1': position['ticker1'],\n",
    "                        'ticker2': position['ticker2'],\n",
    "                        'prev_size': prev_size,\n",
    "                        'new_size': curr_size,\n",
    "                        'change_pct': (curr_size - prev_size) / prev_size if prev_size != 0 else float('inf'),\n",
    "                        'date': current_date\n",
    "                    })\n",
    "                else:\n",
    "                    changes['unchanged'].append(pair_id)\n",
    "        \n",
    "        # Posiciones cerradas\n",
    "        for pair_id, position in previous_positions.items():\n",
    "            if pair_id not in self.current_positions:\n",
    "                changes['closed'].append({\n",
    "                    'pair_id': pair_id,\n",
    "                    'ticker1': position['ticker1'],\n",
    "                    'ticker2': position['ticker2'],\n",
    "                    'prev_size': position['size'],\n",
    "                    'entry_date': position['entry_date'],\n",
    "                    'days_held': (current_date - position['entry_date']).days,\n",
    "                    'date': current_date\n",
    "                })\n",
    "        \n",
    "        # Resumen\n",
    "        summary = {\n",
    "            'total_positions': len(self.current_positions),\n",
    "            'new_positions': len(changes['opened']),\n",
    "            'closed_positions': len(changes['closed']),\n",
    "            'modified_positions': len(changes['modified']),\n",
    "            'unchanged_positions': len(changes['unchanged']),\n",
    "            'details': changes\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def backtest(self, data, sector_map, subsector_map, start_date=None, end_date=None):\n",
    "        \"\"\"Realiza backtest de la estrategia con separación estricta de datos\"\"\"\n",
    "        try:\n",
    "            logging.info(\"Iniciando backtest de la estrategia...\")\n",
    "            \n",
    "            if 'prices' not in data or data['prices'].empty:\n",
    "                raise ValueError(\"Datos de precios insuficientes o no proporcionados\")\n",
    "            \n",
    "            prices = data['prices']\n",
    "            \n",
    "            # Definir fechas por defecto si no se proporcionan\n",
    "            if start_date is None:\n",
    "                # Usar suficientes datos para entrenamiento inicial\n",
    "                min_train_days = 252 * self.config.get('min_data_years', 5)\n",
    "                if len(prices) > min_train_days + 20:  # Al menos 20 días para test\n",
    "                    start_date = prices.index[min_train_days]\n",
    "                else:\n",
    "                    # O dividir datos disponibles\n",
    "                    start_date = prices.index[len(prices) // 2]\n",
    "                \n",
    "                logging.info(f\"Usando fecha de inicio por defecto: {start_date}\")\n",
    "            elif isinstance(start_date, str):\n",
    "                # Convertir string a timestamp si es necesario\n",
    "                start_date = pd.Timestamp(start_date)\n",
    "                logging.info(f\"Fecha de inicio convertida a Timestamp: {start_date}\")\n",
    "            \n",
    "            if end_date is None:\n",
    "                # Usar último día disponible\n",
    "                end_date = prices.index[-1]\n",
    "                logging.info(f\"Usando fecha de fin por defecto: {end_date}\")\n",
    "            elif isinstance(end_date, str):\n",
    "                # Convertir string a timestamp si es necesario\n",
    "                end_date = pd.Timestamp(end_date)\n",
    "                logging.info(f\"Fecha de fin convertida a Timestamp: {end_date}\")\n",
    "                \n",
    "            # Verificar que las fechas son válidas\n",
    "            if start_date > end_date:\n",
    "                raise ValueError(f\"Fecha de inicio {start_date} posterior a fecha de fin {end_date}\")\n",
    "                \n",
    "            # Verificar que no son fechas futuras\n",
    "            current_date = datetime.now().date()\n",
    "            if end_date.date() > current_date:\n",
    "                logging.warning(f\"Fecha de fin {end_date} es posterior a fecha actual {current_date}\")\n",
    "                # Encontrar el último índice válido que no sea futuro\n",
    "                valid_indices = prices.index[prices.index.date <= current_date]\n",
    "                if len(valid_indices) > 0:\n",
    "                    end_date = valid_indices[-1]\n",
    "                    logging.info(f\"Ajustando fecha de fin a: {end_date}\")\n",
    "                else:\n",
    "                    raise ValueError(f\"No hay fechas válidas antes de la fecha actual\")\n",
    "            \n",
    "            # Filtrar fechas para backtesting\n",
    "            backtest_mask = (prices.index >= start_date) & (prices.index <= end_date)\n",
    "            backtest_dates = prices.index[backtest_mask]\n",
    "            \n",
    "            if len(backtest_dates) < 5:\n",
    "                raise ValueError(f\"Período de backtest demasiado corto: {len(backtest_dates)} días\")\n",
    "                \n",
    "            logging.info(f\"Período de backtest: {start_date} a {end_date} ({len(backtest_dates)} días)\")\n",
    "            \n",
    "            # Inicializar equity curve\n",
    "            equity_curve = pd.DataFrame(index=backtest_dates, columns=[\n",
    "                'equity', 'returns', 'costs', 'net_returns', 'drawdown', 'regime'\n",
    "            ])\n",
    "            equity_curve['equity'] = 1.0\n",
    "            equity_curve['costs'] = 0.0\n",
    "            \n",
    "            # Inicializar con datos hasta start_date para entrenamiento inicial\n",
    "            training_data = self._filter_data_until_date(data, start_date)\n",
    "            \n",
    "            # Reiniciar estrategia\n",
    "            self._reset_for_backtest()\n",
    "            \n",
    "            # Inicializar con datos de entrenamiento\n",
    "            self.initialize(training_data, sector_map, subsector_map, start_date)\n",
    "            \n",
    "            # Simular trading\n",
    "            logging.info(f\"Ejecutando backtesting día a día...\")\n",
    "            \n",
    "            previous_positions = {}\n",
    "            trades_log = []\n",
    "            \n",
    "            # Iterar por cada fecha\n",
    "            for i, current_date in enumerate(tqdm(backtest_dates)):\n",
    "                # Datos hasta fecha actual (sin look-ahead bias)\n",
    "                current_data = self._filter_data_until_date(data, current_date)\n",
    "                \n",
    "                # Actualizar estrategia\n",
    "                current_positions = self.update(current_data, sector_map, subsector_map, current_date)\n",
    "                \n",
    "                # Registrar régimen\n",
    "                equity_curve.loc[current_date, 'regime'] = self.current_regime\n",
    "                \n",
    "                # Calcular P&L diario\n",
    "                daily_pnl = 0\n",
    "                daily_costs = 0\n",
    "                \n",
    "                # P&L de posiciones cerradas\n",
    "                closed_positions = set(previous_positions.keys()) - set(current_positions.keys())\n",
    "                \n",
    "                for pair_id in closed_positions:\n",
    "                    old_position = previous_positions[pair_id]\n",
    "                    ticker1 = old_position['ticker1']\n",
    "                    ticker2 = old_position['ticker2']\n",
    "                    hedge_ratio = old_position['hedge_ratio']\n",
    "                    \n",
    "                    # Calcular retorno del spread\n",
    "                    if ticker1 in prices.columns and ticker2 in prices.columns:\n",
    "                        try:\n",
    "                            # Calcular spread en fechas de entrada y salida\n",
    "                            entry_date = old_position['entry_date']\n",
    "                            \n",
    "                            if entry_date in prices.index and ticker1 in prices.loc[entry_date] and ticker2 in prices.loc[entry_date]:\n",
    "                                old_spread = prices.loc[entry_date, ticker1] + hedge_ratio * prices.loc[entry_date, ticker2]\n",
    "                                new_spread = prices.loc[current_date, ticker1] + hedge_ratio * prices.loc[current_date, ticker2]\n",
    "                                \n",
    "                                # Retorno según dirección\n",
    "                                if old_position['signal'] > 0:  # Long\n",
    "                                    trade_return = (new_spread - old_spread) / abs(old_spread)\n",
    "                                else:  # Short\n",
    "                                    trade_return = (old_spread - new_spread) / abs(old_spread)\n",
    "                                \n",
    "                                # Posición y P&L bruto\n",
    "                                position_pnl = old_position['size'] * trade_return\n",
    "                                \n",
    "                                # Calcular costos si están habilitados\n",
    "                                trade_costs = 0\n",
    "                                if self.config.get('transaction_costs', True):\n",
    "                                    # Costos de entrada (ya incurridos)\n",
    "                                    entry_costs = old_position.get('costs', {}).get('total_pct', 0.001)\n",
    "                                    \n",
    "                                    # Costos de salida (comisión + slippage)\n",
    "                                    exit_costs = self.trading_costs['commission'] + self.trading_costs['slippage']\n",
    "                                    \n",
    "                                    # Si tenemos información de liquidez para impacto de mercado\n",
    "                                    if self.config.get('include_market_impact', True) and 'min_adv' in old_position.get('costs', {}):\n",
    "                                        position_value = abs(old_position['size']) * 1e6  # Asumiendo $1M base\n",
    "                                        pct_of_adv = position_value / old_position['costs']['min_adv']\n",
    "                                        market_impact = self.trading_costs['market_impact'] * np.sqrt(pct_of_adv)\n",
    "                                        exit_costs += market_impact\n",
    "                                    \n",
    "                                    # Total de costos de salida como % de la posición\n",
    "                                    trade_costs = abs(old_position['size']) * exit_costs\n",
    "                                \n",
    "                                # P&L neto\n",
    "                                net_pnl = position_pnl - trade_costs\n",
    "                                daily_pnl += position_pnl\n",
    "                                daily_costs += trade_costs\n",
    "                                \n",
    "                                # Registrar trade\n",
    "                                trades_log.append({\n",
    "                                    'pair_id': pair_id,\n",
    "                                    'ticker1': ticker1,\n",
    "                                    'ticker2': ticker2,\n",
    "                                    'entry_date': old_position['entry_date'],\n",
    "                                    'exit_date': current_date,\n",
    "                                    'days_held': (current_date - old_position['entry_date']).days,\n",
    "                                    'entry_signal': old_position['signal'],\n",
    "                                    'entry_z_score': old_position['z_score'],\n",
    "                                    'position_size': old_position['size'],\n",
    "                                    'gross_pnl': position_pnl,\n",
    "                                    'costs': trade_costs,\n",
    "                                    'net_pnl': net_pnl,\n",
    "                                    'return': trade_return,\n",
    "                                    'regime': old_position.get('regime', self.current_regime)\n",
    "                                })\n",
    "                            else:\n",
    "                                logging.warning(f\"No se encontraron datos para {ticker1}-{ticker2} en fecha de entrada {entry_date}\")\n",
    "                        except Exception as e:\n",
    "                            logging.error(f\"Error calculando P&L para {pair_id}: {str(e)}\")\n",
    "                \n",
    "                # P&L de posiciones actualizadas\n",
    "                common_positions = set(previous_positions.keys()) & set(current_positions.keys())\n",
    "                \n",
    "                for pair_id in common_positions:\n",
    "                    old_position = previous_positions[pair_id]\n",
    "                    new_position = current_positions[pair_id]\n",
    "                    \n",
    "                    # Si el tamaño cambió, calcular P&L para la parte cerrada\n",
    "                    if abs(old_position['size'] - new_position['size']) > 0.001:\n",
    "                        ticker1 = old_position['ticker1']\n",
    "                        ticker2 = old_position['ticker2']\n",
    "                        hedge_ratio = old_position['hedge_ratio']\n",
    "                        \n",
    "                        try:\n",
    "                            # Tamaño ajustado\n",
    "                            size_diff = old_position['size'] - new_position['size']\n",
    "                            \n",
    "                            # Calcular retorno\n",
    "                            entry_date = old_position['entry_date']\n",
    "                            \n",
    "                            if entry_date in prices.index and ticker1 in prices.loc[entry_date] and ticker2 in prices.loc[entry_date]:\n",
    "                                old_spread = prices.loc[entry_date, ticker1] + hedge_ratio * prices.loc[entry_date, ticker2]\n",
    "                                new_spread = prices.loc[current_date, ticker1] + hedge_ratio * prices.loc[current_date, ticker2]\n",
    "                                \n",
    "                                # Retorno según dirección\n",
    "                                if old_position['signal'] > 0:  # Long\n",
    "                                    trade_return = (new_spread - old_spread) / abs(old_spread)\n",
    "                                else:  # Short\n",
    "                                    trade_return = (old_spread - new_spread) / abs(old_spread)\n",
    "                                \n",
    "                                # P&L bruto\n",
    "                                position_pnl = size_diff * trade_return\n",
    "                                \n",
    "                                # Costos de ajuste de posición\n",
    "                                trade_costs = 0\n",
    "                                if self.config.get('transaction_costs', True):\n",
    "                                    # Costos solo para la parte ajustada\n",
    "                                    costs_pct = self.trading_costs['commission'] + self.trading_costs['slippage']\n",
    "                                    \n",
    "                                    # Impacto de mercado si aplica\n",
    "                                    if self.config.get('include_market_impact', True) and 'costs' in new_position:\n",
    "                                        costs_pct += new_position['costs'].get('market_impact', 0)\n",
    "                                        \n",
    "                                    trade_costs = abs(size_diff) * costs_pct\n",
    "                                \n",
    "                                # P&L neto\n",
    "                                net_pnl = position_pnl - trade_costs\n",
    "                                daily_pnl += position_pnl\n",
    "                                daily_costs += trade_costs\n",
    "                                \n",
    "                                # Registrar trade parcial\n",
    "                                trades_log.append({\n",
    "                                    'pair_id': pair_id,\n",
    "                                    'ticker1': ticker1,\n",
    "                                    'ticker2': ticker2,\n",
    "                                    'entry_date': old_position['entry_date'],\n",
    "                                    'exit_date': current_date,\n",
    "                                    'days_held': (current_date - old_position['entry_date']).days,\n",
    "                                    'entry_signal': old_position['signal'],\n",
    "                                    'entry_z_score': old_position['z_score'],\n",
    "                                    'position_size': size_diff,\n",
    "                                    'gross_pnl': position_pnl,\n",
    "                                    'costs': trade_costs,\n",
    "                                    'net_pnl': net_pnl,\n",
    "                                    'return': trade_return,\n",
    "                                    'regime': old_position.get('regime', self.current_regime),\n",
    "                                    'partial': True\n",
    "                                })\n",
    "                            else:\n",
    "                                logging.warning(f\"No se encontraron datos para {ticker1}-{ticker2} en fecha de entrada {entry_date}\")\n",
    "                        except Exception as e:\n",
    "                            logging.error(f\"Error calculando P&L para ajuste de {pair_id}: {str(e)}\")\n",
    "                \n",
    "                # Añadir costos de nuevas posiciones\n",
    "                new_positions = set(current_positions.keys()) - set(previous_positions.keys())\n",
    "                for pair_id in new_positions:\n",
    "                    if self.config.get('transaction_costs', True):\n",
    "                        position = current_positions[pair_id]\n",
    "                        # Aplicar costos de entrada\n",
    "                        entry_costs = position.get('costs', {}).get('total_pct', 0.001)\n",
    "                        trade_costs = abs(position['size']) * entry_costs\n",
    "                        daily_costs += trade_costs\n",
    "                \n",
    "                # Actualizar equity curve\n",
    "                if i > 0:\n",
    "                    equity_curve.loc[current_date, 'returns'] = daily_pnl\n",
    "                    equity_curve.loc[current_date, 'costs'] = daily_costs\n",
    "                    equity_curve.loc[current_date, 'net_returns'] = daily_pnl - daily_costs\n",
    "                    \n",
    "                    # Equity acumulada con costos\n",
    "                    previous_equity = equity_curve.iloc[i-1]['equity']\n",
    "                    equity_curve.loc[current_date, 'equity'] = previous_equity * (1 + daily_pnl - daily_costs)\n",
    "                else:\n",
    "                    equity_curve.loc[current_date, 'returns'] = 0\n",
    "                    equity_curve.loc[current_date, 'costs'] = 0\n",
    "                    equity_curve.loc[current_date, 'net_returns'] = 0\n",
    "                \n",
    "                # Actualizar posiciones anteriores\n",
    "                previous_positions = {k: v.copy() for k, v in current_positions.items()}\n",
    "            \n",
    "            # Calcular drawdown\n",
    "            equity = equity_curve['equity']\n",
    "            high_water_mark = equity.cummax()\n",
    "            drawdown = 1 - equity / high_water_mark\n",
    "            equity_curve['drawdown'] = drawdown\n",
    "            \n",
    "            # Calcular métricas de rendimiento\n",
    "            self.trades_log = pd.DataFrame(trades_log)\n",
    "            self.calculate_performance_metrics(equity_curve, self.trades_log)\n",
    "            \n",
    "            # Guardar equity curve\n",
    "            self.equity_curve = equity_curve\n",
    "            \n",
    "            logging.info(f\"Backtest completado. Retorno final: {(equity.iloc[-1]/equity.iloc[0]-1)*100:.2f}%\")\n",
    "            \n",
    "            return equity_curve\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en backtest: {str(e)}\")\n",
    "            logging.error(traceback.format_exc())\n",
    "            return None\n",
    "    \n",
    "    def _reset_for_backtest(self):\n",
    "        \"\"\"Reinicia el estado de la estrategia para backtest\"\"\"\n",
    "        # Reiniciar componentes\n",
    "        self.regime_detector = RegimeDetector(\n",
    "            min_train_samples=min(252 * self.config.get('min_data_years', 5), 1260)\n",
    "        )\n",
    "        \n",
    "        self.pair_selector = PairSelector(\n",
    "            min_liquidity=self.config.get('min_liquidity', 10e6),\n",
    "            max_pairs_by_regime=self.config.get('max_pairs_per_regime', {1: 25, 2: 20, 3: 15}),\n",
    "            recalibration_days=self.config.get('recalibration_days', 5)\n",
    "        )\n",
    "        \n",
    "        self.convergence_predictor = ConvergencePredictor()\n",
    "        self.signal_generator = SignalGenerator()\n",
    "        self.position_manager = PositionManager()\n",
    "        \n",
    "        # Reiniciar estado\n",
    "        self.current_regime = None\n",
    "        self.previous_regime = None\n",
    "        self.selected_pairs = []\n",
    "        self.current_positions = {}\n",
    "        self.positions_history = []\n",
    "        self.regime_history = []\n",
    "        self.recalibration_history = []\n",
    "        self.metrics_history = []\n",
    "        \n",
    "        logging.info(\"Estado de la estrategia reiniciado para backtest\")\n",
    "    \n",
    "    def calculate_performance_metrics(self, equity_curve, trades_log):\n",
    "        \"\"\"Calcula métricas de rendimiento completas incluyendo análisis por régimen\"\"\"\n",
    "        try:\n",
    "            logging.info(\"Calculando métricas de rendimiento...\")\n",
    "            \n",
    "            # Retornos diarios (netos después de costos)\n",
    "            daily_returns = equity_curve['net_returns'].dropna()\n",
    "            \n",
    "            if len(daily_returns) == 0:\n",
    "                logging.warning(\"No hay retornos para calcular métricas\")\n",
    "                return self.metrics\n",
    "            \n",
    "            # Métricas generales\n",
    "            \n",
    "            # Sharpe Ratio (anualizado)\n",
    "            sharpe = np.sqrt(252) * daily_returns.mean() / daily_returns.std() if daily_returns.std() > 0 else 0\n",
    "            \n",
    "            # Sortino Ratio (anualizado)\n",
    "            negative_returns = daily_returns[daily_returns < 0]\n",
    "            sortino = np.sqrt(252) * daily_returns.mean() / negative_returns.std() if len(negative_returns) > 0 and negative_returns.std() > 0 else 0\n",
    "            \n",
    "            # Maximum Drawdown\n",
    "            max_drawdown = equity_curve['drawdown'].max()\n",
    "            \n",
    "            # Annualized Return\n",
    "            days = (equity_curve.index[-1] - equity_curve.index[0]).days\n",
    "            annual_return = (equity_curve['equity'].iloc[-1] / equity_curve['equity'].iloc[0]) ** (365 / max(days, 1)) - 1\n",
    "            \n",
    "            # Annualized Volatility\n",
    "            annual_vol = daily_returns.std() * np.sqrt(252)\n",
    "            \n",
    "            # Trades específicos\n",
    "            if len(trades_log) > 0:\n",
    "                win_rate = (trades_log['net_pnl'] > 0).mean()\n",
    "                avg_trade_duration = trades_log['days_held'].mean()\n",
    "                avg_profit_per_trade = trades_log['net_pnl'].mean()\n",
    "                \n",
    "                # Profit factor (gross profits / gross losses)\n",
    "                gross_profits = trades_log[trades_log['gross_pnl'] > 0]['gross_pnl'].sum()\n",
    "                gross_losses = abs(trades_log[trades_log['gross_pnl'] < 0]['gross_pnl'].sum())\n",
    "                profit_factor = gross_profits / gross_losses if gross_losses != 0 else float('inf')\n",
    "                \n",
    "                # Impacto de costos\n",
    "                total_gross_pnl = trades_log['gross_pnl'].sum()\n",
    "                total_costs = trades_log['costs'].sum()\n",
    "                cost_impact = total_costs / total_gross_pnl if total_gross_pnl != 0 else float('inf')\n",
    "            else:\n",
    "                win_rate = 0\n",
    "                avg_trade_duration = 0\n",
    "                avg_profit_per_trade = 0\n",
    "                profit_factor = 0\n",
    "                cost_impact = 0\n",
    "            \n",
    "            # Guardar métricas generales\n",
    "            self.metrics['overall'] = {\n",
    "                'sharpe_ratio': sharpe,\n",
    "                'sortino_ratio': sortino,\n",
    "                'max_drawdown': max_drawdown,\n",
    "                'annual_return': annual_return,\n",
    "                'volatility': annual_vol,\n",
    "                'win_rate': win_rate,\n",
    "                'avg_trade_duration': avg_trade_duration,\n",
    "                'avg_profit_per_trade': avg_profit_per_trade,\n",
    "                'profit_factor': profit_factor,\n",
    "                'cost_impact': cost_impact\n",
    "            }\n",
    "            \n",
    "            # Análisis por régimen\n",
    "            if 'regime' in equity_curve.columns:\n",
    "                for regime in [1, 2, 3]:\n",
    "                    regime_mask = equity_curve['regime'] == regime\n",
    "                    regime_days = regime_mask.sum()\n",
    "                    \n",
    "                    # Si hay suficientes días en este régimen\n",
    "                    if regime_days > 0:\n",
    "                        regime_returns = equity_curve.loc[regime_mask, 'net_returns']\n",
    "                        regime_equity = equity_curve.loc[regime_mask, 'equity']\n",
    "                        \n",
    "                        # Calcular métricas si hay datos suficientes\n",
    "                        if len(regime_returns) > 1:\n",
    "                            regime_sharpe = np.sqrt(252) * regime_returns.mean() / regime_returns.std() if regime_returns.std() > 0 else 0\n",
    "                            \n",
    "                            # Rendimiento en período\n",
    "                            if len(regime_equity) > 1:\n",
    "                                regime_return = regime_equity.iloc[-1] / regime_equity.iloc[0] - 1\n",
    "                            else:\n",
    "                                regime_return = 0\n",
    "                            \n",
    "                            # Drawdown en régimen\n",
    "                            regime_drawdown = equity_curve.loc[regime_mask, 'drawdown'].max()\n",
    "                            \n",
    "                            # Volatilidad anualizada\n",
    "                            regime_vol = regime_returns.std() * np.sqrt(252) if len(regime_returns) > 5 else 0\n",
    "                            \n",
    "                            # Guardar\n",
    "                            self.metrics['by_regime'][regime] = {\n",
    "                                'days': regime_days,\n",
    "                                'pct_time': regime_days / len(equity_curve),\n",
    "                                'return': regime_return,\n",
    "                                'sharpe': regime_sharpe,\n",
    "                                'volatility': regime_vol,\n",
    "                                'max_drawdown': regime_drawdown\n",
    "                            }\n",
    "                    \n",
    "                    # Análisis de trades por régimen\n",
    "                    if len(trades_log) > 0 and 'regime' in trades_log.columns:\n",
    "                        regime_trades = trades_log[trades_log['regime'] == regime]\n",
    "                        \n",
    "                        if len(regime_trades) > 0:\n",
    "                            regime_win_rate = (regime_trades['net_pnl'] > 0).mean()\n",
    "                            regime_avg_profit = regime_trades['net_pnl'].mean()\n",
    "                            \n",
    "                            # Añadir a métricas\n",
    "                            if regime in self.metrics['by_regime']:\n",
    "                                self.metrics['by_regime'][regime].update({\n",
    "                                    'trade_count': len(regime_trades),\n",
    "                                    'win_rate': regime_win_rate,\n",
    "                                    'avg_profit': regime_avg_profit\n",
    "                                })\n",
    "            \n",
    "            # Análisis sectorial si hay información\n",
    "            if len(trades_log) > 0 and 'ticker1' in trades_log.columns:\n",
    "                # Obtener sectores de los tickers (requiere sector_map)\n",
    "                if hasattr(self, 'sector_map'):\n",
    "                    trades_log['sector1'] = trades_log['ticker1'].map(self.sector_map)\n",
    "                    trades_log['sector2'] = trades_log['ticker2'].map(self.sector_map)\n",
    "                    \n",
    "                    # Agrupar por sector y calcular métricas\n",
    "                    for sector in set(trades_log['sector1'].tolist() + trades_log['sector2'].tolist()):\n",
    "                        if pd.isna(sector):\n",
    "                            continue\n",
    "                            \n",
    "                        # Filtrar trades de este sector (en cualquier lado del par)\n",
    "                        sector_trades = trades_log[(trades_log['sector1'] == sector) | (trades_log['sector2'] == sector)]\n",
    "                        \n",
    "                        if len(sector_trades) > 0:\n",
    "                            sector_win_rate = (sector_trades['net_pnl'] > 0).mean()\n",
    "                            sector_avg_profit = sector_trades['net_pnl'].mean()\n",
    "                            sector_profit_factor = sector_trades[sector_trades['gross_pnl'] > 0]['gross_pnl'].sum() / abs(sector_trades[sector_trades['gross_pnl'] < 0]['gross_pnl'].sum()) if abs(sector_trades[sector_trades['gross_pnl'] < 0]['gross_pnl'].sum()) > 0 else float('inf')\n",
    "                            \n",
    "                            self.metrics['by_sector'][sector] = {\n",
    "                                'trade_count': len(sector_trades),\n",
    "                                'win_rate': sector_win_rate,\n",
    "                                'avg_profit': sector_avg_profit,\n",
    "                                'profit_factor': sector_profit_factor\n",
    "                            }\n",
    "            \n",
    "            # Análisis por período (trimestral)\n",
    "            if len(equity_curve) > 20:\n",
    "                # Añadir información de trimestre\n",
    "                equity_curve['quarter'] = equity_curve.index.to_period('Q')\n",
    "                \n",
    "                # Agrupar por trimestre\n",
    "                for quarter in equity_curve['quarter'].unique():\n",
    "                    quarter_mask = equity_curve['quarter'] == quarter\n",
    "                    quarter_returns = equity_curve.loc[quarter_mask, 'net_returns']\n",
    "                    quarter_equity = equity_curve.loc[quarter_mask, 'equity']\n",
    "                    \n",
    "                    if len(quarter_returns) > 5:\n",
    "                        quarter_return = quarter_equity.iloc[-1] / quarter_equity.iloc[0] - 1\n",
    "                        quarter_sharpe = np.sqrt(252) * quarter_returns.mean() / quarter_returns.std() if quarter_returns.std() > 0 else 0\n",
    "                        \n",
    "                        self.metrics['by_period'][str(quarter)] = {\n",
    "                            'days': len(quarter_returns),\n",
    "                            'return': quarter_return,\n",
    "                            'sharpe': quarter_sharpe\n",
    "                        }\n",
    "            \n",
    "            # Guardar en historial\n",
    "            snapshot = {\n",
    "                'date': datetime.now(),\n",
    "                'metrics': self.metrics.copy()\n",
    "            }\n",
    "            self.metrics_history.append(snapshot)\n",
    "            \n",
    "            logging.info(\"Métricas calculadas correctamente\")\n",
    "            \n",
    "            return self.metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error calculando métricas: {str(e)}\")\n",
    "            logging.error(traceback.format_exc())\n",
    "            return self.metrics\n",
    "    \n",
    "    def walk_forward_test(self, data, sector_map, subsector_map, \n",
    "                         training_years=5, validation_months=6, test_months=6, \n",
    "                         num_windows=4, strict_separation=True):\n",
    "        \"\"\"Realiza validación walk-forward con separación estricta de datos para evitar look-ahead bias\"\"\"\n",
    "        try:\n",
    "            prices = data['prices']\n",
    "            dates = prices.index\n",
    "            \n",
    "            # Convertir a días\n",
    "            training_days = training_years * 252\n",
    "            validation_days = validation_months * 21\n",
    "            test_days = test_months * 21\n",
    "            \n",
    "            if len(dates) < training_days + validation_days + test_days:\n",
    "                logging.error(\"Datos insuficientes para walk-forward testing\")\n",
    "                return None\n",
    "            \n",
    "            # Inicializar resultados\n",
    "            results = {\n",
    "                'windows': [],\n",
    "                'equity_curves': [],\n",
    "                'metrics': [],\n",
    "                'trades': []\n",
    "            }\n",
    "            \n",
    "            # Verificar número de ventanas\n",
    "            max_windows = (len(dates) - training_days) // test_days\n",
    "            num_windows = min(num_windows, max_windows)\n",
    "            \n",
    "            logging.info(f\"Realizando walk-forward test con {num_windows} ventanas\")\n",
    "            \n",
    "            # Definir ventanas\n",
    "            for window_idx in range(num_windows):\n",
    "                # Definir índices\n",
    "                train_start_idx = window_idx * test_days\n",
    "                train_end_idx = train_start_idx + training_days\n",
    "                validation_end_idx = train_end_idx + validation_days\n",
    "                test_end_idx = validation_end_idx + test_days\n",
    "                \n",
    "                # Ajustar si excede límites\n",
    "                test_end_idx = min(test_end_idx, len(dates) - 1)\n",
    "                \n",
    "                # Definir fechas - asegurar que sean objetos Timestamp\n",
    "                train_start = dates[train_start_idx]\n",
    "                train_end = dates[train_end_idx]\n",
    "                validation_end = dates[validation_end_idx]\n",
    "                test_end = dates[test_end_idx]\n",
    "                \n",
    "                # Asegurar que son objetos Timestamp\n",
    "                if isinstance(train_start, str):\n",
    "                    train_start = pd.Timestamp(train_start)\n",
    "                if isinstance(train_end, str):\n",
    "                    train_end = pd.Timestamp(train_end)\n",
    "                if isinstance(validation_end, str):\n",
    "                    validation_end = pd.Timestamp(validation_end)\n",
    "                if isinstance(test_end, str):\n",
    "                    test_end = pd.Timestamp(test_end)\n",
    "                \n",
    "                logging.info(f\"\\nWalk-forward ventana {window_idx+1}:\")\n",
    "                logging.info(f\"Entrenamiento: {train_start} a {train_end}\")\n",
    "                logging.info(f\"Validación: {train_end} a {validation_end}\")\n",
    "                logging.info(f\"Prueba: {validation_end} a {test_end}\")\n",
    "                \n",
    "                # Nueva instancia de estrategia para cada ventana (reset completo)\n",
    "                strategy = StatArbStrategy(config=self.config)\n",
    "                \n",
    "                # Preparar datos de entrenamiento (hasta train_end)\n",
    "                train_data = self._filter_data_until_date(data, train_end)\n",
    "                \n",
    "                # Inicializar con datos de entrenamiento\n",
    "                strategy.initialize(train_data, sector_map, subsector_map, train_end)\n",
    "                \n",
    "                # Optimizar hiperparámetros en validación si se requiere\n",
    "                if validation_days > 0:\n",
    "                    # Filtrar datos hasta validation_end\n",
    "                    validation_data = self._filter_data_until_date(data, validation_end)\n",
    "                    \n",
    "                    # Ejecutar en período de validación\n",
    "                    validation_dates = [d for d in dates if train_end < d <= validation_end]\n",
    "                    \n",
    "                    for val_date in tqdm(validation_dates, desc=\"Validación\"):\n",
    "                        # Datos hasta esta fecha de validación\n",
    "                        val_date_data = self._filter_data_until_date(validation_data, val_date)\n",
    "                        strategy.update(val_date_data, sector_map, subsector_map, val_date)\n",
    "                    \n",
    "                    logging.info(\"Validación completada\")\n",
    "                \n",
    "                # Ejecutar backtest en período de prueba\n",
    "                test_data = self._filter_data_until_date(data, test_end)\n",
    "                \n",
    "                # Si strict_separation=True, reiniciar estado después de validación\n",
    "                if strict_separation and validation_days > 0:\n",
    "                    logging.info(\"Reiniciando modelos post-validación para separación estricta\")\n",
    "                    # Mantener configuración pero reiniciar estado\n",
    "                    strategy._reset_for_backtest()\n",
    "                    # Reentrenar con datos hasta validation_end\n",
    "                    strategy.initialize(validation_data, sector_map, subsector_map, validation_end)\n",
    "                \n",
    "                # Ejecutar backtest solo en período de prueba\n",
    "                test_dates = [d for d in dates if validation_end < d <= test_end]\n",
    "                \n",
    "                # Inicializar equity curve para este test\n",
    "                test_equity = pd.DataFrame(index=test_dates, columns=['equity', 'returns', 'costs', 'net_returns', 'drawdown', 'regime'])\n",
    "                test_equity['equity'] = 1.0\n",
    "                test_trades = []\n",
    "                \n",
    "                previous_positions = {}\n",
    "                \n",
    "                # Simular día a día\n",
    "                for i, test_date in enumerate(tqdm(test_dates, desc=\"Test\")):\n",
    "                    # Datos hasta esta fecha\n",
    "                    test_date_data = self._filter_data_until_date(test_data, test_date)\n",
    "                    \n",
    "                    # Actualizar estrategia\n",
    "                    current_positions = strategy.update(test_date_data, sector_map, subsector_map, test_date)\n",
    "                    \n",
    "                    # Registrar régimen\n",
    "                    test_equity.loc[test_date, 'regime'] = strategy.current_regime\n",
    "                    \n",
    "                    # Calcular P&L (lógica igual que en backtest)\n",
    "                    daily_pnl, daily_costs, new_trades = self._calculate_daily_pnl(\n",
    "                        previous_positions, current_positions, prices, test_date\n",
    "                    )\n",
    "                    \n",
    "                    # Añadir datos de ventana a trades\n",
    "                    for trade in new_trades:\n",
    "                        trade['window'] = window_idx + 1\n",
    "                        test_trades.append(trade)\n",
    "                    \n",
    "                    # Actualizar equity curve\n",
    "                    if i > 0:\n",
    "                        test_equity.loc[test_date, 'returns'] = daily_pnl\n",
    "                        test_equity.loc[test_date, 'costs'] = daily_costs\n",
    "                        test_equity.loc[test_date, 'net_returns'] = daily_pnl - daily_costs\n",
    "                        \n",
    "                        previous_equity = test_equity.iloc[i-1]['equity']\n",
    "                        test_equity.loc[test_date, 'equity'] = previous_equity * (1 + daily_pnl - daily_costs)\n",
    "                    else:\n",
    "                        test_equity.loc[test_date, 'returns'] = 0\n",
    "                        test_equity.loc[test_date, 'costs'] = 0\n",
    "                        test_equity.loc[test_date, 'net_returns'] = 0\n",
    "                    \n",
    "                    # Actualizar posiciones anteriores\n",
    "                    previous_positions = {k: v.copy() for k, v in current_positions.items()}\n",
    "                \n",
    "                # Calcular drawdown\n",
    "                equity = test_equity['equity']\n",
    "                drawdown = 1 - equity / equity.cummax()\n",
    "                test_equity['drawdown'] = drawdown\n",
    "                \n",
    "                # Calcular métricas\n",
    "                test_trades_df = pd.DataFrame(test_trades) if test_trades else pd.DataFrame()\n",
    "                window_metrics = self._calculate_window_metrics(test_equity, test_trades_df, window_idx + 1)\n",
    "                \n",
    "                # Guardar resultados\n",
    "                results['windows'].append({\n",
    "                    'window': window_idx + 1,\n",
    "                    'train_start': train_start,\n",
    "                    'train_end': train_end,\n",
    "                    'validation_end': validation_end,\n",
    "                    'test_end': test_end\n",
    "                })\n",
    "                results['equity_curves'].append(test_equity)\n",
    "                results['metrics'].append(window_metrics)\n",
    "                results['trades'].append(test_trades_df)\n",
    "                \n",
    "                logging.info(f\"Ventana {window_idx+1} completada. Retorno: {(test_equity['equity'].iloc[-1] - 1) * 100:.2f}%\")\n",
    "            \n",
    "            # Calcular métricas agregadas\n",
    "            if results['metrics']:\n",
    "                avg_metrics = {}\n",
    "                for key in ['sharpe_ratio', 'sortino_ratio', 'max_drawdown', 'annual_return', 'win_rate']:\n",
    "                    values = [m.get(key, 0) for m in results['metrics']]\n",
    "                    avg_metrics[key] = np.mean(values)\n",
    "                    avg_metrics[f'{key}_std'] = np.std(values)\n",
    "                \n",
    "                results['avg_metrics'] = avg_metrics\n",
    "                \n",
    "                logging.info(\"Resultados agregados de walk-forward test:\")\n",
    "                logging.info(f\"Retorno anual promedio: {avg_metrics['annual_return']*100:.2f}% ± {avg_metrics['annual_return_std']*100:.2f}%\")\n",
    "                logging.info(f\"Sharpe promedio: {avg_metrics['sharpe_ratio']:.2f} ± {avg_metrics['sharpe_ratio_std']:.2f}\")\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en walk_forward_test: {str(e)}\")\n",
    "            logging.error(traceback.format_exc())\n",
    "            return None\n",
    "    \n",
    "    def _calculate_daily_pnl(self, previous_positions, current_positions, prices, current_date):\n",
    "        \"\"\"Calcula P&L diario y costos para backtesting\"\"\"\n",
    "        daily_pnl = 0\n",
    "        daily_costs = 0\n",
    "        new_trades = []\n",
    "        \n",
    "        # P&L de posiciones cerradas\n",
    "        closed_positions = set(previous_positions.keys()) - set(current_positions.keys())\n",
    "        \n",
    "        for pair_id in closed_positions:\n",
    "            old_position = previous_positions[pair_id]\n",
    "            ticker1 = old_position['ticker1']\n",
    "            ticker2 = old_position['ticker2']\n",
    "            hedge_ratio = old_position['hedge_ratio']\n",
    "            \n",
    "            # Calcular retorno del spread\n",
    "            if ticker1 in prices.columns and ticker2 in prices.columns:\n",
    "                try:\n",
    "                    # Calcular spread en fechas de entrada y salida\n",
    "                    entry_date = old_position['entry_date']\n",
    "                    \n",
    "                    if entry_date in prices.index and ticker1 in prices.loc[entry_date] and ticker2 in prices.loc[entry_date]:\n",
    "                        old_spread = prices.loc[entry_date, ticker1] + hedge_ratio * prices.loc[entry_date, ticker2]\n",
    "                        new_spread = prices.loc[current_date, ticker1] + hedge_ratio * prices.loc[current_date, ticker2]\n",
    "                        \n",
    "                        # Retorno según dirección\n",
    "                        if old_position['signal'] > 0:  # Long\n",
    "                            trade_return = (new_spread - old_spread) / abs(old_spread)\n",
    "                        else:  # Short\n",
    "                            trade_return = (old_spread - new_spread) / abs(old_spread)\n",
    "                        \n",
    "                        # Posición y P&L bruto\n",
    "                        position_pnl = old_position['size'] * trade_return\n",
    "                        \n",
    "                        # Calcular costos si están habilitados\n",
    "                        trade_costs = 0\n",
    "                        if self.config.get('transaction_costs', True):\n",
    "                            # Costos de entrada (ya incurridos)\n",
    "                            entry_costs = old_position.get('costs', {}).get('total_pct', 0.001)\n",
    "                            \n",
    "                            # Costos de salida (comisión + slippage)\n",
    "                            exit_costs = self.trading_costs['commission'] + self.trading_costs['slippage']\n",
    "                            \n",
    "                            # Si tenemos información de liquidez para impacto de mercado\n",
    "                            if self.config.get('include_market_impact', True) and 'min_adv' in old_position.get('costs', {}):\n",
    "                                position_value = abs(old_position['size']) * 1e6  # Asumiendo $1M base\n",
    "                                pct_of_adv = position_value / old_position['costs']['min_adv']\n",
    "                                market_impact = self.trading_costs['market_impact'] * np.sqrt(pct_of_adv)\n",
    "                                exit_costs += market_impact\n",
    "                            \n",
    "                            # Total de costos de salida como % de la posición\n",
    "                            trade_costs = abs(old_position['size']) * exit_costs\n",
    "                        \n",
    "                        # P&L neto\n",
    "                        net_pnl = position_pnl - trade_costs\n",
    "                        daily_pnl += position_pnl\n",
    "                        daily_costs += trade_costs\n",
    "                        \n",
    "                        # Registrar trade\n",
    "                        new_trades.append({\n",
    "                            'pair_id': pair_id,\n",
    "                            'ticker1': ticker1,\n",
    "                            'ticker2': ticker2,\n",
    "                            'entry_date': old_position['entry_date'],\n",
    "                            'exit_date': current_date,\n",
    "                            'days_held': (current_date - old_position['entry_date']).days,\n",
    "                            'entry_signal': old_position['signal'],\n",
    "                            'entry_z_score': old_position['z_score'],\n",
    "                            'position_size': old_position['size'],\n",
    "                            'gross_pnl': position_pnl,\n",
    "                            'costs': trade_costs,\n",
    "                            'net_pnl': net_pnl,\n",
    "                            'return': trade_return,\n",
    "                            'regime': old_position.get('regime', self.current_regime)\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error calculando P&L para {pair_id}: {str(e)}\")\n",
    "        \n",
    "        # P&L de posiciones actualizadas\n",
    "        common_positions = set(previous_positions.keys()) & set(current_positions.keys())\n",
    "        \n",
    "        for pair_id in common_positions:\n",
    "            old_position = previous_positions[pair_id]\n",
    "            new_position = current_positions[pair_id]\n",
    "            \n",
    "            # Si el tamaño cambió, calcular P&L para la parte cerrada\n",
    "            if abs(old_position['size'] - new_position['size']) > 0.001:\n",
    "                ticker1 = old_position['ticker1']\n",
    "                ticker2 = old_position['ticker2']\n",
    "                hedge_ratio = old_position['hedge_ratio']\n",
    "                \n",
    "                try:\n",
    "                    # Tamaño ajustado\n",
    "                    size_diff = old_position['size'] - new_position['size']\n",
    "                    \n",
    "                    # Calcular retorno\n",
    "                    entry_date = old_position['entry_date']\n",
    "                    \n",
    "                    if entry_date in prices.index and ticker1 in prices.loc[entry_date] and ticker2 in prices.loc[entry_date]:\n",
    "                        old_spread = prices.loc[entry_date, ticker1] + hedge_ratio * prices.loc[entry_date, ticker2]\n",
    "                        new_spread = prices.loc[current_date, ticker1] + hedge_ratio * prices.loc[current_date, ticker2]\n",
    "                        \n",
    "                        # Retorno según dirección\n",
    "                        if old_position['signal'] > 0:  # Long\n",
    "                            trade_return = (new_spread - old_spread) / abs(old_spread)\n",
    "                        else:  # Short\n",
    "                            trade_return = (old_spread - new_spread) / abs(old_spread)\n",
    "                        \n",
    "                        # P&L bruto\n",
    "                        position_pnl = size_diff * trade_return\n",
    "                        \n",
    "                        # Costos de ajuste de posición\n",
    "                        trade_costs = 0\n",
    "                        if self.config.get('transaction_costs', True):\n",
    "                            # Costos solo para la parte ajustada\n",
    "                            costs_pct = self.trading_costs['commission'] + self.trading_costs['slippage']\n",
    "                            \n",
    "                            # Impacto de mercado si aplica\n",
    "                            if self.config.get('include_market_impact', True) and 'costs' in new_position:\n",
    "                                costs_pct += new_position['costs'].get('market_impact', 0)\n",
    "                                \n",
    "                            trade_costs = abs(size_diff) * costs_pct\n",
    "                        \n",
    "                        # P&L neto\n",
    "                        net_pnl = position_pnl - trade_costs\n",
    "                        daily_pnl += position_pnl\n",
    "                        daily_costs += trade_costs\n",
    "                        \n",
    "                        # Registrar trade parcial\n",
    "                        new_trades.append({\n",
    "                            'pair_id': pair_id,\n",
    "                            'ticker1': ticker1,\n",
    "                            'ticker2': ticker2,\n",
    "                            'entry_date': old_position['entry_date'],\n",
    "                            'exit_date': current_date,\n",
    "                            'days_held': (current_date - old_position['entry_date']).days,\n",
    "                            'entry_signal': old_position['signal'],\n",
    "                            'entry_z_score': old_position['z_score'],\n",
    "                            'position_size': size_diff,\n",
    "                            'gross_pnl': position_pnl,\n",
    "                            'costs': trade_costs,\n",
    "                            'net_pnl': net_pnl,\n",
    "                            'return': trade_return,\n",
    "                            'regime': old_position.get('regime', self.current_regime),\n",
    "                            'partial': True\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error calculando P&L para ajuste de {pair_id}: {str(e)}\")\n",
    "        \n",
    "        # Añadir costos de nuevas posiciones\n",
    "        new_positions = set(current_positions.keys()) - set(previous_positions.keys())\n",
    "        for pair_id in new_positions:\n",
    "            if self.config.get('transaction_costs', True):\n",
    "                position = current_positions[pair_id]\n",
    "                # Aplicar costos de entrada\n",
    "                entry_costs = position.get('costs', {}).get('total_pct', 0.001)\n",
    "                trade_costs = abs(position['size']) * entry_costs\n",
    "                daily_costs += trade_costs\n",
    "        \n",
    "        return daily_pnl, daily_costs, new_trades\n",
    "    \n",
    "    def _calculate_window_metrics(self, equity_curve, trades, window_index):\n",
    "        \"\"\"Calcula métricas para una ventana de walk-forward\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        try:\n",
    "            # Retornos diarios (netos después de costos)\n",
    "            daily_returns = equity_curve['net_returns'].dropna()\n",
    "            \n",
    "            if len(daily_returns) > 0:\n",
    "                # Sharpe Ratio (anualizado)\n",
    "                sharpe = np.sqrt(252) * daily_returns.mean() / daily_returns.std() if daily_returns.std() > 0 else 0\n",
    "                \n",
    "                # Sortino Ratio (anualizado)\n",
    "                negative_returns = daily_returns[daily_returns < 0]\n",
    "                sortino = np.sqrt(252) * daily_returns.mean() / negative_returns.std() if len(negative_returns) > 0 and negative_returns.std() > 0 else 0\n",
    "                \n",
    "                # Maximum Drawdown\n",
    "                max_drawdown = equity_curve['drawdown'].max()\n",
    "                \n",
    "                # Annualized Return\n",
    "                days = (equity_curve.index[-1] - equity_curve.index[0]).days\n",
    "                annual_return = (equity_curve['equity'].iloc[-1] / equity_curve['equity'].iloc[0]) ** (365 / max(days, 1)) - 1\n",
    "                \n",
    "                # Annualized Volatility\n",
    "                annual_vol = daily_returns.std() * np.sqrt(252)\n",
    "                \n",
    "                metrics.update({\n",
    "                    'window': window_index,\n",
    "                    'sharpe_ratio': sharpe,\n",
    "                    'sortino_ratio': sortino,\n",
    "                    'max_drawdown': max_drawdown,\n",
    "                    'annual_return': annual_return,\n",
    "                    'volatility': annual_vol,\n",
    "                    'total_return': equity_curve['equity'].iloc[-1] / equity_curve['equity'].iloc[0] - 1\n",
    "                })\n",
    "            \n",
    "            # Métricas de trades\n",
    "            if len(trades) > 0:\n",
    "                win_rate = (trades['net_pnl'] > 0).mean()\n",
    "                avg_profit = trades['net_pnl'].mean()\n",
    "                total_trades = len(trades)\n",
    "                \n",
    "                metrics.update({\n",
    "                    'win_rate': win_rate,\n",
    "                    'avg_profit': avg_profit,\n",
    "                    'total_trades': total_trades\n",
    "                })\n",
    "                \n",
    "                # Análisis por régimen\n",
    "                if 'regime' in trades.columns:\n",
    "                    regimes_data = {}\n",
    "                    for regime in trades['regime'].unique():\n",
    "                        regime_trades = trades[trades['regime'] == regime]\n",
    "                        if len(regime_trades) > 0:\n",
    "                            regimes_data[int(regime)] = {\n",
    "                                'count': len(regime_trades),\n",
    "                                'win_rate': (regime_trades['net_pnl'] > 0).mean(),\n",
    "                                'avg_profit': regime_trades['net_pnl'].mean()\n",
    "                            }\n",
    "                    \n",
    "                    metrics['regimes'] = regimes_data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error calculando métricas para ventana {window_index}: {str(e)}\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def plot_equity_curve(self, filename='equity_curve.png', include_regimes=True):\n",
    "        \"\"\"Genera gráfico mejorado de curva de equity con análisis de regímenes\"\"\"\n",
    "        if self.equity_curve is None:\n",
    "            logging.warning(\"No hay curva de equity disponible.\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            plt.figure(figsize=(14, 10))\n",
    "            \n",
    "            # Panel 1: Equity con retornos acumulados\n",
    "            ax1 = plt.subplot(3, 1, 1)\n",
    "            \n",
    "            # Equity bruta vs neta\n",
    "            if 'costs' in self.equity_curve.columns:\n",
    "                # Calcular equity bruta (sin costos)\n",
    "                gross_equity = self.equity_curve['equity'].copy()\n",
    "                \n",
    "                for i in range(1, len(self.equity_curve)):\n",
    "                    gross_equity.iloc[i] = gross_equity.iloc[i-1] * (1 + self.equity_curve['returns'].iloc[i])\n",
    "                \n",
    "                ax1.plot(self.equity_curve.index, gross_equity, \n",
    "                        alpha=0.5, linestyle='--', color='skyblue', \n",
    "                        label='Equity Bruta (sin costos)')\n",
    "                \n",
    "            # Equity neta\n",
    "            ax1.plot(self.equity_curve.index, self.equity_curve['equity'], \n",
    "                   color='blue', linewidth=1.5, label='Equity Neta')\n",
    "            \n",
    "            # Marcar cambios de régimen si está disponible\n",
    "            if include_regimes and 'regime' in self.equity_curve.columns:\n",
    "                regime_changes = self.equity_curve['regime'].diff().fillna(0) != 0\n",
    "                change_dates = self.equity_curve.index[regime_changes]\n",
    "                change_regimes = self.equity_curve.loc[regime_changes, 'regime']\n",
    "                \n",
    "                for date, regime in zip(change_dates, change_regimes):\n",
    "                    if regime == 1:\n",
    "                        color = 'green'\n",
    "                        label = 'Régimen 1'\n",
    "                    elif regime == 2:\n",
    "                        color = 'orange'\n",
    "                        label = 'Régimen 2'\n",
    "                    else:\n",
    "                        color = 'red'\n",
    "                        label = 'Régimen 3'\n",
    "                    \n",
    "                    # Evitar etiquetas duplicadas\n",
    "                    if date == change_dates[0]:\n",
    "                        ax1.axvline(x=date, color=color, linestyle='-', alpha=0.3, label=label)\n",
    "                    else:\n",
    "                        ax1.axvline(x=date, color=color, linestyle='-', alpha=0.3)\n",
    "            \n",
    "            # Añadir benchmark si está disponible\n",
    "            if hasattr(self, 'benchmark_equity'):\n",
    "                ax1.plot(self.equity_curve.index, self.benchmark_equity, \n",
    "                       color='gray', linestyle='-.', label='Benchmark')\n",
    "            \n",
    "            # Configurar\n",
    "            ax1.set_title('Curva de Equity', fontsize=12, fontweight='bold')\n",
    "            ax1.set_ylabel('Equity', fontsize=10)\n",
    "            ax1.legend(loc='upper left')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            ax1.set_axisbelow(True)\n",
    "            \n",
    "            # Panel 2: Drawdown\n",
    "            ax2 = plt.subplot(3, 1, 2, sharex=ax1)\n",
    "            ax2.fill_between(self.equity_curve.index, self.equity_curve['drawdown'], \n",
    "                            alpha=0.3, color='red')\n",
    "            ax2.plot(self.equity_curve.index, self.equity_curve['drawdown'], \n",
    "                    color='red', label='Drawdown')\n",
    "            \n",
    "            # Líneas de referencia\n",
    "            ax2.axhline(y=0.05, color='orange', linestyle='--', alpha=0.5, label='5%')\n",
    "            ax2.axhline(y=0.1, color='red', linestyle='--', alpha=0.5, label='10%')\n",
    "            \n",
    "            ax2.set_title('Drawdown', fontsize=12, fontweight='bold')\n",
    "            ax2.set_ylabel('Drawdown', fontsize=10)\n",
    "            ax2.set_ylim(0, min(1.0, self.equity_curve['drawdown'].max() * 1.5))\n",
    "            ax2.legend(loc='upper right')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            ax2.set_axisbelow(True)\n",
    "            \n",
    "            # Panel 3: Retornos diarios\n",
    "            ax3 = plt.subplot(3, 1, 3, sharex=ax1)\n",
    "            \n",
    "            # Retornos brutos vs costos\n",
    "            if 'returns' in self.equity_curve.columns and 'costs' in self.equity_curve.columns:\n",
    "                ax3.bar(self.equity_curve.index, self.equity_curve['returns'], \n",
    "                      width=1.0, color='green', alpha=0.5, label='Retornos Brutos')\n",
    "                ax3.bar(self.equity_curve.index, -self.equity_curve['costs'], \n",
    "                      width=1.0, color='red', alpha=0.5, label='Costos')\n",
    "                ax3.bar(self.equity_curve.index, self.equity_curve['net_returns'], \n",
    "                      width=0.5, color='blue', alpha=0.7, label='Retornos Netos')\n",
    "            else:\n",
    "                # Solo retornos netos\n",
    "                positive_returns = self.equity_curve['net_returns'] > 0\n",
    "                negative_returns = self.equity_curve['net_returns'] < 0\n",
    "                \n",
    "                ax3.bar(self.equity_curve.index[positive_returns], \n",
    "                      self.equity_curve.loc[positive_returns, 'net_returns'], \n",
    "                      width=1.0, color='green', alpha=0.7, label='Retornos Positivos')\n",
    "                ax3.bar(self.equity_curve.index[negative_returns], \n",
    "                      self.equity_curve.loc[negative_returns, 'net_returns'], \n",
    "                      width=1.0, color='red', alpha=0.7, label='Retornos Negativos')\n",
    "            \n",
    "            ax3.set_title('Retornos Diarios', fontsize=12, fontweight='bold')\n",
    "            ax3.set_ylabel('Retorno', fontsize=10)\n",
    "            ax3.legend(loc='upper right')\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "            ax3.set_axisbelow(True)\n",
    "            \n",
    "            # Ajustar diseño\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Guardar figura\n",
    "            plt.savefig(f'./artifacts/results/figures/{filename}', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            logging.info(f\"Gráfico de equity guardado como {filename}\")\n",
    "            \n",
    "            # Generar gráfico adicional de regímenes si están disponibles\n",
    "            if include_regimes and 'regime' in self.equity_curve.columns:\n",
    "                self.plot_regime_distribution(f\"regimes_{filename}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generando gráfico: {str(e)}\")\n",
    "            logging.error(traceback.format_exc())\n",
    "            return False\n",
    "    \n",
    "    def plot_regime_distribution(self, filename='regime_distribution.png'):\n",
    "        \"\"\"Grafica distribución y rendimiento por regímenes\"\"\"\n",
    "        try:\n",
    "            if 'regime' not in self.equity_curve.columns:\n",
    "                logging.warning(\"No hay información de regímenes disponible.\")\n",
    "                return False\n",
    "            \n",
    "            plt.figure(figsize=(14, 8))\n",
    "            \n",
    "            # Panel 1: Distribución de regímenes\n",
    "            ax1 = plt.subplot(1, 2, 1)\n",
    "            regime_counts = self.equity_curve['regime'].value_counts()\n",
    "            \n",
    "            # Colores por régimen\n",
    "            colors = ['green', 'orange', 'red']\n",
    "            labels = [f'Régimen {i} ({n} días)' for i, n in regime_counts.items()]\n",
    "            \n",
    "            ax1.pie(regime_counts, labels=labels, autopct='%1.1f%%', \n",
    "                  startangle=90, shadow=False, colors=colors)\n",
    "            ax1.set_title('Distribución de Regímenes', fontsize=12, fontweight='bold')\n",
    "            \n",
    "            # Panel 2: Rendimiento por régimen\n",
    "            ax2 = plt.subplot(1, 2, 2)\n",
    "            \n",
    "            # Calcular retorno por régimen\n",
    "            regime_returns = {}\n",
    "            \n",
    "            for regime in sorted(self.equity_curve['regime'].unique()):\n",
    "                regime_mask = self.equity_curve['regime'] == regime\n",
    "                regime_curve = self.equity_curve.loc[regime_mask]\n",
    "                \n",
    "                if len(regime_curve) > 0:\n",
    "                    # Calcular equity normalizada para este régimen\n",
    "                    normalized_equity = regime_curve['equity'] / regime_curve['equity'].iloc[0]\n",
    "                    \n",
    "                    # Usar días relativos como eje x para comparar regímenes\n",
    "                    days = np.arange(len(normalized_equity))\n",
    "                    \n",
    "                    # Color según régimen\n",
    "                    color = colors[int(regime)-1] if int(regime) <= len(colors) else 'gray'\n",
    "                    \n",
    "                    # Calcular retorno\n",
    "                    total_return = normalized_equity.iloc[-1] - 1\n",
    "                    regime_returns[regime] = total_return\n",
    "                    \n",
    "                    ax2.plot(days, normalized_equity, \n",
    "                           color=color, \n",
    "                           label=f'Régimen {int(regime)}: {total_return*100:.1f}%')\n",
    "            \n",
    "            # Configuración\n",
    "            ax2.set_title('Rendimiento por Régimen (Normalizado)', fontsize=12, fontweight='bold')\n",
    "            ax2.set_xlabel('Días Transcurridos', fontsize=10)\n",
    "            ax2.set_ylabel('Equity Normalizada', fontsize=10)\n",
    "            ax2.legend(loc='upper left')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            ax2.set_axisbelow(True)\n",
    "            \n",
    "            # Añadir texto con métricas detalladas\n",
    "            metrics_text = \"Métricas por Régimen:\\n\"\n",
    "            \n",
    "            for regime in sorted(regime_returns.keys()):\n",
    "                regime_mask = self.equity_curve['regime'] == regime\n",
    "                regime_days = regime_mask.sum()\n",
    "                \n",
    "                if regime_days > 0:\n",
    "                    regime_return = regime_returns[regime]\n",
    "                    annualized = ((1 + regime_return) ** (252 / regime_days) - 1) if regime_days > 0 else 0\n",
    "                    \n",
    "                    # Volatilidad\n",
    "                    if 'net_returns' in self.equity_curve.columns:\n",
    "                        regime_vol = self.equity_curve.loc[regime_mask, 'net_returns'].std() * np.sqrt(252)\n",
    "                        \n",
    "                        # Sharpe simple\n",
    "                        sharpe = annualized / regime_vol if regime_vol > 0 else 0\n",
    "                        \n",
    "                        metrics_text += f\"Régimen {int(regime)}: {regime_days} días, {regime_return*100:.1f}% ({annualized*100:.1f}% anual), Sharpe: {sharpe:.2f}\\n\"\n",
    "                    else:\n",
    "                        metrics_text += f\"Régimen {int(regime)}: {regime_days} días, {regime_return*100:.1f}% ({annualized*100:.1f}% anual)\\n\"\n",
    "            \n",
    "            # Añadir texto al gráfico\n",
    "            plt.figtext(0.5, 0.01, metrics_text, ha='center', fontsize=10, \n",
    "                      bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "            \n",
    "            # Ajustar diseño\n",
    "            plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "            \n",
    "            # Guardar figura\n",
    "            plt.savefig(f'./artifacts/results/figures/{filename}', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            logging.info(f\"Gráfico de regímenes guardado como {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generando gráfico de regímenes: {str(e)}\")\n",
    "            logging.error(traceback.format_exc())\n",
    "            return False\n",
    "    \n",
    "    def save_performance_summary(self, filename='performance_summary.json'):\n",
    "        \"\"\"Guarda métricas en JSON con formato detallado\"\"\"\n",
    "        try:\n",
    "            # Convertir cualquier numpy float a Python float para serialización\n",
    "            def convert_np_floats(obj):\n",
    "                if isinstance(obj, np.float32) or isinstance(obj, np.float64):\n",
    "                    return float(obj)\n",
    "                elif isinstance(obj, dict):\n",
    "                    return {k: convert_np_floats(v) for k, v in obj.items()}\n",
    "                elif isinstance(obj, list):\n",
    "                    return [convert_np_floats(item) for item in obj]\n",
    "                else:\n",
    "                    return obj\n",
    "            \n",
    "            # Convertir métricas\n",
    "            metrics_json = convert_np_floats(self.metrics)\n",
    "            \n",
    "            # Añadir metadatos\n",
    "            summary = {\n",
    "                'metrics': metrics_json,\n",
    "                'metadata': {\n",
    "                    'generated_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'strategy_config': self.config,\n",
    "                    'data_period': {\n",
    "                        'start': self.equity_curve.index[0].strftime('%Y-%m-%d') if self.equity_curve is not None else None,\n",
    "                        'end': self.equity_curve.index[-1].strftime('%Y-%m-%d') if self.equity_curve is not None else None,\n",
    "                        'days': len(self.equity_curve) if self.equity_curve is not None else 0\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Guardar como JSON\n",
    "            with open(f'./artifacts/results/data/{filename}', 'w') as f:\n",
    "                json.dump(summary, f, indent=2)\n",
    "            \n",
    "            logging.info(f\"Resumen de rendimiento guardado como {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error guardando resumen: {str(e)}\")\n",
    "            logging.error(traceback.format_exc())\n",
    "            return False\n",
    "    \n",
    "    def save_trade_log(self, filename='trade_log.csv'):\n",
    "        \"\"\"Guarda registro de operaciones en CSV con información detallada\"\"\"\n",
    "        try:\n",
    "            if not hasattr(self, 'trades_log') or self.trades_log is None or len(self.trades_log) == 0:\n",
    "                logging.warning(\"No hay registro de operaciones disponible.\")\n",
    "                return False\n",
    "            \n",
    "            # Guardar CSV\n",
    "            self.trades_log.to_csv(f'./artifacts/results/data/{filename}', index=False)\n",
    "            \n",
    "            # Generar análisis adicional de trades\n",
    "            self.analyze_trades(f\"trade_analysis_{filename.replace('.csv', '.json')}\")\n",
    "            \n",
    "            logging.info(f\"Registro de operaciones guardado como {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error guardando registro de operaciones: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def analyze_trades(self, filename='trade_analysis.json'):\n",
    "        \"\"\"Analiza operaciones para identificar patrones y factores de éxito\"\"\"\n",
    "        try:\n",
    "            if not hasattr(self, 'trades_log') or self.trades_log is None or len(self.trades_log) == 0:\n",
    "                logging.warning(\"No hay registro de operaciones para análisis.\")\n",
    "                return False\n",
    "            \n",
    "            trades = self.trades_log\n",
    "            \n",
    "            # Análisis básico\n",
    "            analysis = {\n",
    "                'total_trades': len(trades),\n",
    "                'winning_trades': int((trades['net_pnl'] > 0).sum()),\n",
    "                'losing_trades': int((trades['net_pnl'] <= 0).sum()),\n",
    "                'win_rate': float((trades['net_pnl'] > 0).mean()),\n",
    "                'profit_factor': float(trades[trades['gross_pnl'] > 0]['gross_pnl'].sum() / \n",
    "                                    abs(trades[trades['gross_pnl'] < 0]['gross_pnl'].sum())) \n",
    "                                    if abs(trades[trades['gross_pnl'] < 0]['gross_pnl'].sum()) > 0 else float('inf'),\n",
    "                'avg_profit': float(trades['net_pnl'].mean()),\n",
    "                'avg_win': float(trades[trades['net_pnl'] > 0]['net_pnl'].mean()) if (trades['net_pnl'] > 0).any() else 0,\n",
    "                'avg_loss': float(trades[trades['net_pnl'] <= 0]['net_pnl'].mean()) if (trades['net_pnl'] <= 0).any() else 0,\n",
    "                'max_win': float(trades['net_pnl'].max()),\n",
    "                'max_loss': float(trades['net_pnl'].min()),\n",
    "                'avg_holding_period': float(trades['days_held'].mean()),\n",
    "                'total_costs': float(trades['costs'].sum()),\n",
    "                'cost_impact': float(trades['costs'].sum() / trades['gross_pnl'].sum()) \n",
    "                             if trades['gross_pnl'].sum() != 0 else 0\n",
    "            }\n",
    "            \n",
    "            # Análisis por régimen\n",
    "            if 'regime' in trades.columns:\n",
    "                regime_analysis = {}\n",
    "                for regime in trades['regime'].unique():\n",
    "                    regime_trades = trades[trades['regime'] == regime]\n",
    "                    regime_analysis[int(regime)] = {\n",
    "                        'count': len(regime_trades),\n",
    "                        'pct_total': float(len(regime_trades) / len(trades)),\n",
    "                        'win_rate': float((regime_trades['net_pnl'] > 0).mean()),\n",
    "                        'avg_profit': float(regime_trades['net_pnl'].mean()),\n",
    "                        'avg_holding_period': float(regime_trades['days_held'].mean())\n",
    "                    }\n",
    "                analysis['by_regime'] = regime_analysis\n",
    "            \n",
    "            # Análisis por tamaño de posición\n",
    "            trades['position_size_group'] = pd.qcut(abs(trades['position_size']), \n",
    "                                                 4, labels=['Very Small', 'Small', 'Medium', 'Large'])\n",
    "            size_analysis = {}\n",
    "            for size_group in trades['position_size_group'].unique():\n",
    "                group_trades = trades[trades['position_size_group'] == size_group]\n",
    "                size_analysis[str(size_group)] = {\n",
    "                    'count': len(group_trades),\n",
    "                    'win_rate': float((group_trades['net_pnl'] > 0).mean()),\n",
    "                    'avg_profit': float(group_trades['net_pnl'].mean()),\n",
    "                    'cost_impact': float(group_trades['costs'].sum() / group_trades['gross_pnl'].abs().sum())\n",
    "                                  if group_trades['gross_pnl'].abs().sum() > 0 else 0\n",
    "                }\n",
    "            analysis['by_position_size'] = size_analysis\n",
    "            \n",
    "            # Análisis por duración\n",
    "            trades['duration_group'] = pd.qcut(trades['days_held'], \n",
    "                                            3, labels=['Short', 'Medium', 'Long'])\n",
    "            duration_analysis = {}\n",
    "            for duration_group in trades['duration_group'].unique():\n",
    "                group_trades = trades[trades['duration_group'] == duration_group]\n",
    "                duration_analysis[str(duration_group)] = {\n",
    "                    'count': len(group_trades),\n",
    "                    'win_rate': float((group_trades['net_pnl'] > 0).mean()),\n",
    "                    'avg_profit': float(group_trades['net_pnl'].mean()),\n",
    "                    'avg_days': float(group_trades['days_held'].mean())\n",
    "                }\n",
    "            analysis['by_duration'] = duration_analysis\n",
    "            \n",
    "            # Guardar análisis\n",
    "            with open(f'./artifacts/results/data/{filename}', 'w') as f:\n",
    "                json.dump(analysis, f, indent=2)\n",
    "            \n",
    "            logging.info(f\"Análisis de operaciones guardado como {filename}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en análisis de operaciones: {str(e)}\")\n",
    "            logging.error(traceback.format_exc())\n",
    "            return False\n",
    "\n",
    "# Función principal\n",
    "def main():\n",
    "    \"\"\"Función principal mejorada para ejecutar la estrategia de arbitraje estadístico\"\"\"\n",
    "    print(\"Iniciando implementación de estrategia de arbitraje estadístico...\")\n",
    "    \n",
    "    try:\n",
    "\n",
    "        \n",
    "        # Configuración de la estrategia\n",
    "        config = {\n",
    "            'min_data_years': 5,\n",
    "            'max_pairs_per_regime': {1: 25, 2: 20, 3: 15},\n",
    "            'min_liquidity': 10e6,\n",
    "            'recalibration_days': 5,\n",
    "            'transaction_costs': True,\n",
    "            'include_market_impact': True,\n",
    "            'circuit_breakers_enabled': True\n",
    "        }\n",
    "        \n",
    "        # Período para datos - usar ayer como fecha final para evitar datos futuros\n",
    "        end_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        start_date = (datetime.now() - timedelta(days=252*12)).strftime('%Y-%m-%d')  # Reducido a 6 años para demo\n",
    "        \n",
    "        print(f\"Obteniendo tickers del S&P 500 para período {start_date} - {end_date}\")\n",
    "        tickers, sector_map, subsector_map = get_sp500_tickers()\n",
    "        \n",
    "        # Para pruebas, usar subconjunto de tickers\n",
    "        #tickers = tickers[:100]  # Reducido a 50 para más rápida ejecución\n",
    "        \n",
    "        # Descargar datos (sin fechas futuras)\n",
    "        print(f\"Descargando datos históricos...\")\n",
    "        data_dict = get_historical_data(tickers, start_date, end_date)\n",
    "        \n",
    "        # Verificar datos suficientes\n",
    "        if not data_dict or len(data_dict) < 10:\n",
    "            raise ValueError(f\"Datos insuficientes: solo {len(data_dict)} tickers disponibles\")\n",
    "            \n",
    "        # Preprocesar datos\n",
    "        print(\"Procesando datos...\")\n",
    "        processed_data = preprocess_data(data_dict)\n",
    "        \n",
    "        # Crear estrategia con configuración optimizada\n",
    "        strategy = StatArbStrategy(config=config)\n",
    "        \n",
    "        # Inicializar\n",
    "        print(\"Inicializando estrategia...\")\n",
    "        strategy.initialize(processed_data, sector_map, subsector_map)\n",
    "        \n",
    "        # Ejecutar backtest\n",
    "        print(\"Ejecutando backtest...\")\n",
    "        # Usar 2 años para testing para demostración\n",
    "        # Convertir a objeto Timestamp para evitar error de comparación\n",
    "        backtest_start = pd.Timestamp((datetime.now() - timedelta(days=252*2)).strftime('%Y-%m-%d'))\n",
    "        equity_curve = strategy.backtest(processed_data, sector_map, subsector_map, backtest_start)\n",
    "        \n",
    "        if equity_curve is None:\n",
    "            print(\"Advertencia: El backtest no generó resultados, verifique los logs para más detalles\")\n",
    "            # Seguir con otras funcionalidades en lugar de terminar con error\n",
    "        else:\n",
    "            # Ejecutar walk-forward test\n",
    "            print(\"Ejecutando validación walk-forward...\")\n",
    "            wf_results = strategy.walk_forward_test(\n",
    "                processed_data, sector_map, subsector_map,\n",
    "                training_years=3, validation_months=3, test_months=3,  # Parámetros reducidos\n",
    "                num_windows=2, strict_separation=True  # Reducido a 2 ventanas\n",
    "            )\n",
    "            \n",
    "            # Generar visualizaciones\n",
    "            print(\"Generando visualizaciones...\")\n",
    "            strategy.plot_equity_curve('equity_curve_total.png', include_regimes=True)\n",
    "            \n",
    "            # Visualizar pares actuales\n",
    "            if strategy.current_positions:\n",
    "                print(\"Graficando z-scores de pares seleccionados...\")\n",
    "                # Crear función de visualización de pares\n",
    "                for i, pair_id in enumerate(list(strategy.current_positions.keys())[:3]):  # Solo primeros 3\n",
    "                    plot_pair_zscore(strategy, pair_id, processed_data, \n",
    "                                   f\"pair_{pair_id.replace('_', '-')}.png\")\n",
    "            \n",
    "            # Guardar métricas y resultados\n",
    "            print(\"Guardando métricas de rendimiento...\")\n",
    "            strategy.save_performance_summary('performance_summary.json')\n",
    "            strategy.save_trade_log('trade_log.csv')\n",
    "            \n",
    "            # Guardar resultados de walk-forward\n",
    "            if wf_results:\n",
    "                print(\"Guardando resultados de walk-forward...\")\n",
    "                save_walkforward_results(wf_results, 'walkforward_results.json')\n",
    "                \n",
    "                # Crear visualización de walk-forward\n",
    "                plot_walkforward_results(wf_results, 'walkforward_comparison.png')\n",
    "            \n",
    "            # Mostrar resumen de performance\n",
    "            print_performance_summary(strategy.metrics)\n",
    "        \n",
    "        print(\"Implementación completada con éxito.\")\n",
    "        return strategy\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en ejecución principal: {str(e)}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Funciones auxiliares para análisis y visualización\n",
    "def plot_pair_zscore(strategy, pair_id, data, filename=None):\n",
    "    \"\"\"Visualiza z-score histórico de un par específico\"\"\"\n",
    "    # Asegurar importaciones necesarias\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    if pair_id not in strategy.current_positions:\n",
    "        logging.warning(f\"Par {pair_id} no encontrado en posiciones actuales\")\n",
    "        return False\n",
    "        \n",
    "    try:\n",
    "        position = strategy.current_positions[pair_id]\n",
    "        ticker1 = position['ticker1']\n",
    "        ticker2 = position['ticker2']\n",
    "        hedge_ratio = position['hedge_ratio']\n",
    "        \n",
    "        # Calcular spread y z-score con ventana adaptativa\n",
    "        lookback = min(252, len(data['prices']))\n",
    "        prices = data['prices'].iloc[-lookback:][[ticker1, ticker2]]\n",
    "        \n",
    "        if len(prices) < 30 or prices[ticker1].isna().any() or prices[ticker2].isna().any():\n",
    "            logging.warning(f\"Datos insuficientes para visualizar {pair_id}\")\n",
    "            return False\n",
    "            \n",
    "        spread = prices[ticker1] + hedge_ratio * prices[ticker2]\n",
    "        \n",
    "        # Calcular media y desviación móvil exponencial\n",
    "        half_life = position.get('half_life', lookback/4)\n",
    "        half_life = min(max(5, half_life), lookback/2)\n",
    "        \n",
    "        spread_mean = spread.ewm(halflife=half_life).mean()\n",
    "        spread_std = spread.ewm(halflife=half_life/3).std()\n",
    "        \n",
    "        z_score = (spread - spread_mean) / spread_std\n",
    "        \n",
    "        # Crear gráfico avanzado\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # Panel 1: Spread con bandas\n",
    "        ax1 = plt.subplot(2, 1, 1)\n",
    "        ax1.plot(spread.index, spread, label='Spread', color='blue')\n",
    "        ax1.plot(spread_mean.index, spread_mean, label='Media Móvil', \n",
    "               linestyle='--', color='red')\n",
    "        \n",
    "        # Bandas de volatilidad\n",
    "        ax1.fill_between(\n",
    "            spread.index, \n",
    "            spread_mean + 1*spread_std, \n",
    "            spread_mean - 1*spread_std, \n",
    "            alpha=0.2, color='gray', label='±1σ'\n",
    "        )\n",
    "        ax1.fill_between(\n",
    "            spread.index, \n",
    "            spread_mean + 2*spread_std, \n",
    "            spread_mean - 2*spread_std, \n",
    "            alpha=0.1, color='red', label='±2σ'\n",
    "        )\n",
    "        \n",
    "        # Añadir información del par\n",
    "        ax1.set_title(f'Spread: {ticker1} - {ticker2}  (HR: {hedge_ratio:.4f}, Half-life: {position.get(\"half_life\", \"N/A\")} días)', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "        ax1.set_ylabel('Spread', fontsize=10)\n",
    "        ax1.legend(loc='upper left')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Panel 2: Z-Score\n",
    "        ax2 = plt.subplot(2, 1, 2, sharex=ax1)\n",
    "        ax2.plot(z_score.index, z_score, label='Z-Score', color='blue')\n",
    "        \n",
    "        # Líneas de referencia para los umbrales de trading\n",
    "        ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        ax2.axhline(y=1, color='orange', linestyle='--', alpha=0.5, label='±1')\n",
    "        ax2.axhline(y=-1, color='orange', linestyle='--', alpha=0.5)\n",
    "        ax2.axhline(y=2, color='red', linestyle='--', alpha=0.5, label='±2')\n",
    "        ax2.axhline(y=-2, color='red', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Añadir posición actual\n",
    "        current_z = z_score.iloc[-1]\n",
    "        current_color = 'green' if current_z < -1 else 'red' if current_z > 1 else 'gray'\n",
    "        ax2.plot(z_score.index[-1], current_z, 'o', color=current_color, \n",
    "               markersize=8, label=f'Actual: {current_z:.2f}')\n",
    "        \n",
    "        # Añadir señal actual\n",
    "        signal_label = \"Compra\" if position['signal'] > 0 else \"Venta\" if position['signal'] < 0 else \"Neutral\"\n",
    "        ax2.text(0.02, 0.95, f\"Señal: {signal_label} (Fuerza: {position['strength']:.2f})\", \n",
    "               transform=ax2.transAxes, fontsize=10, verticalalignment='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "        \n",
    "        ax2.set_title('Z-Score del Spread', fontsize=12, fontweight='bold')\n",
    "        ax2.set_ylabel('Z-Score', fontsize=10)\n",
    "        ax2.set_ylim(-4, 4)\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Guardar figura\n",
    "        if filename:\n",
    "            plt.savefig(f'./artifacts/results/figures/{filename}', dpi=300, bbox_inches='tight')\n",
    "        else:\n",
    "            plt.savefig(f'./artifacts/results/figures/pair_{ticker1}_{ticker2}_zscore.png', \n",
    "                       dpi=300, bbox_inches='tight')\n",
    "            \n",
    "        plt.close()\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error visualizando par {pair_id}: {str(e)}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "        return False\n",
    "\n",
    "def save_walkforward_results(wf_results, filename):\n",
    "    \"\"\"Guarda resultados de walk-forward test en formato JSON\"\"\"\n",
    "    try:\n",
    "        # Preparar datos para serialización (convertir DataFrames)\n",
    "        serializable_results = {\n",
    "            'windows': wf_results['windows'],\n",
    "            'metrics': wf_results['metrics'],\n",
    "            'avg_metrics': wf_results.get('avg_metrics', {})\n",
    "        }\n",
    "        \n",
    "        # Convertir valores numpy a Python nativos\n",
    "        def convert_np_values(obj):\n",
    "            if isinstance(obj, np.float32) or isinstance(obj, np.float64):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.int32) or isinstance(obj, np.int64):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, dict):\n",
    "                return {k: convert_np_values(v) for k, v in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [convert_np_values(item) for item in obj]\n",
    "            else:\n",
    "                return obj\n",
    "                \n",
    "        serializable_results = convert_np_values(serializable_results)\n",
    "        \n",
    "        # Guardar como JSON\n",
    "        with open(f'./artifacts/results/data/{filename}', 'w') as f:\n",
    "            json.dump(serializable_results, f, indent=2)\n",
    "            \n",
    "        # Guardar equity curves como CSV\n",
    "        for i, equity_df in enumerate(wf_results['equity_curves']):\n",
    "            equity_df.to_csv(f'./artifacts/results/data/wf_window_{i+1}_equity.csv')\n",
    "            \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error guardando resultados walk-forward: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def plot_walkforward_results(wf_results, filename):\n",
    "    \"\"\"Visualiza resultados comparativos de walk-forward test\"\"\"\n",
    "    try:\n",
    "        if not wf_results or 'equity_curves' not in wf_results or not wf_results['equity_curves']:\n",
    "            logging.warning(\"Sin datos de equity curve para visualización walk-forward\")\n",
    "            return False\n",
    "            \n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # Panel 1: Comparación de equity curves\n",
    "        ax1 = plt.subplot(2, 1, 1)\n",
    "        \n",
    "        colors = ['blue', 'green', 'red', 'purple', 'orange', 'brown']\n",
    "        \n",
    "        for i, equity_df in enumerate(wf_results['equity_curves']):\n",
    "            color = colors[i % len(colors)]\n",
    "            window_info = wf_results['windows'][i]\n",
    "            label = f\"Ventana {i+1}: {window_info['test_end'].strftime('%Y-%m-%d')}\"\n",
    "            \n",
    "            # Normalizar a 1.0 para comparación\n",
    "            normalized = equity_df['equity'] / equity_df['equity'].iloc[0]\n",
    "            ax1.plot(range(len(normalized)), normalized, color=color, label=label)\n",
    "            \n",
    "            # Añadir retorno final\n",
    "            final_return = normalized.iloc[-1] - 1\n",
    "            ax1.annotate(f\"{final_return*100:.1f}%\", \n",
    "                       xy=(len(normalized)-1, normalized.iloc[-1]),\n",
    "                       xytext=(len(normalized)-1, normalized.iloc[-1]),\n",
    "                       color=color, fontweight='bold')\n",
    "        \n",
    "        ax1.set_title('Comparación de Rendimiento por Ventana', fontsize=12, fontweight='bold')\n",
    "        ax1.set_ylabel('Equity Normalizada', fontsize=10)\n",
    "        ax1.set_xlabel('Días', fontsize=10)\n",
    "        ax1.legend(loc='upper left')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Panel 2: Métricas comparativas\n",
    "        ax2 = plt.subplot(2, 1, 2)\n",
    "        \n",
    "        metrics = ['sharpe_ratio', 'sortino_ratio', 'annual_return', 'max_drawdown', 'win_rate']\n",
    "        metric_labels = ['Sharpe', 'Sortino', 'Retorno Anual', 'Drawdown Máx', 'Win Rate']\n",
    "        \n",
    "        # Extraer valores por ventana\n",
    "        metric_values = {}\n",
    "        for metric in metrics:\n",
    "            metric_values[metric] = [m.get(metric, 0) for m in wf_results['metrics']]\n",
    "        \n",
    "        # Convertir a array para manipulación\n",
    "        metric_arrays = {}\n",
    "        for metric in metrics:\n",
    "            if metric == 'max_drawdown' or metric == 'win_rate':\n",
    "                # Convertir a porcentaje\n",
    "                metric_arrays[metric] = np.array(metric_values[metric]) * 100\n",
    "            else:\n",
    "                metric_arrays[metric] = np.array(metric_values[metric])\n",
    "        \n",
    "        # Agrupar datos para gráfico de barras\n",
    "        num_windows = len(wf_results['metrics'])\n",
    "        width = 0.15  # ancho de barras\n",
    "        indices = np.arange(len(metrics))\n",
    "        \n",
    "        for i in range(num_windows):\n",
    "            window_values = [metric_arrays[metric][i] for metric in metrics]\n",
    "            ax2.bar(indices + i*width, window_values, width, label=f'Ventana {i+1}',\n",
    "                  color=colors[i % len(colors)])\n",
    "        \n",
    "        # Configuración\n",
    "        ax2.set_ylabel('Valor', fontsize=10)\n",
    "        ax2.set_xticks(indices + width * (num_windows-1)/2)\n",
    "        ax2.set_xticklabels(metric_labels)\n",
    "        ax2.legend(loc='upper right')\n",
    "        ax2.set_title('Métricas por Ventana', fontsize=12, fontweight='bold')\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'./artifacts/results/figures/{filename}', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error visualizando resultados walk-forward: {str(e)}\")\n",
    "        logging.error(traceback.format_exc())\n",
    "        return False\n",
    "\n",
    "def print_performance_summary(metrics):\n",
    "    \"\"\"Imprime un resumen de performance en la consola\"\"\"\n",
    "    try:\n",
    "        overall = metrics.get('overall', {})\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RESUMEN DE RENDIMIENTO DE LA ESTRATEGIA\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nRENDIMIENTO GENERAL:\")\n",
    "        print(f\"  Retorno Anual:      {overall.get('annual_return', 0)*100:6.2f}%\")\n",
    "        print(f\"  Sharpe Ratio:       {overall.get('sharpe_ratio', 0):6.2f}\")\n",
    "        print(f\"  Sortino Ratio:      {overall.get('sortino_ratio', 0):6.2f}\")\n",
    "        print(f\"  Volatilidad Anual:  {overall.get('volatility', 0)*100:6.2f}%\")\n",
    "        print(f\"  Drawdown Máximo:    {overall.get('max_drawdown', 0)*100:6.2f}%\")\n",
    "        print(f\"  Win Rate:           {overall.get('win_rate', 0)*100:6.2f}%\")\n",
    "        print(f\"  Profit Factor:      {overall.get('profit_factor', 0):6.2f}\")\n",
    "        print(f\"  Impacto de Costos:  {overall.get('cost_impact', 0)*100:6.2f}%\")\n",
    "        \n",
    "        print(\"\\nRENDIMIENTO POR RÉGIMEN:\")\n",
    "        regime_metrics = metrics.get('by_regime', {})\n",
    "        for regime, regime_data in regime_metrics.items():\n",
    "            if regime_data:\n",
    "                print(f\"  Régimen {regime}:\")\n",
    "                print(f\"    Tiempo en régimen: {regime_data.get('pct_time', 0)*100:6.2f}%\")\n",
    "                print(f\"    Retorno:           {regime_data.get('return', 0)*100:6.2f}%\")\n",
    "                print(f\"    Sharpe:            {regime_data.get('sharpe', 0):6.2f}\")\n",
    "                print(f\"    Win Rate:          {regime_data.get('win_rate', 0)*100:6.2f}%\")\n",
    "        \n",
    "        print(\"\\nMEJORES SECTORES:\")\n",
    "        sector_metrics = metrics.get('by_sector', {})\n",
    "        \n",
    "        # Ordenar sectores por profit factor\n",
    "        if sector_metrics:\n",
    "            sorted_sectors = sorted(\n",
    "                sector_metrics.items(), \n",
    "                key=lambda x: x[1].get('profit_factor', 0), \n",
    "                reverse=True\n",
    "            )[:3]  # Top 3\n",
    "            \n",
    "            for sector, sector_data in sorted_sectors:\n",
    "                print(f\"  {sector}:\")\n",
    "                print(f\"    Trades:       {sector_data.get('trade_count', 0):6d}\")\n",
    "                print(f\"    Win Rate:     {sector_data.get('win_rate', 0)*100:6.2f}%\")\n",
    "                print(f\"    Profit Factor:{sector_data.get('profit_factor', 0):6.2f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error imprimiendo resumen: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Iniciar temporizador\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Ejecutar estrategia\n",
    "        strategy = main()\n",
    "        \n",
    "        # Tiempo total\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"\\nEjecución completada en {elapsed_time/60:.2f} minutos\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en ejecución principal: {str(e)}\")\n",
    "        with open('./artifacts/errors.txt', 'a') as f:\n",
    "            f.write(f\"\\n[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ERROR EN EJECUCIÓN PRINCIPAL:\\n\")\n",
    "            traceback.print_exc(file=f)\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f85cbcb-b9b7-40f5-b416-1a336c5a1b63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
